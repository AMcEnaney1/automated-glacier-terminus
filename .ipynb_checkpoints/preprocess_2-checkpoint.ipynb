{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process images over the glaciers\n",
    "\n",
    "_Last modified 2022-05-11._\n",
    "\n",
    "This script processes downloaded images prior to the Wavelet Transform Modulus Maxima (WTMM) segmentation analysis that produces the calving front delineations. The images will be:\n",
    "\n",
    "     1) Grab image acquisition dates from the images\n",
    "     2) Reproject all downloaded images\n",
    "     3) Calculate terminus box dimensions and rasterize\n",
    "     4) Rotate using weighted average glacier flow direction\n",
    "     5) Crop rotated images to the same minimum dimensions and convert to pgm format\n",
    "\n",
    "### The rotation step (4) requires an input ice velocity raster (vx and vy files).\n",
    "\n",
    "For Greenland, we use the [MEASURES 2000-2015](https://nsidc.org/data/NSIDC-0670/versions/1) ice velocity mosaic.\n",
    "\n",
    "### The image conversion step (5) requires the installation of ImageMagick.\n",
    "\n",
    "Download [ImageMagick](https://imagemagick.org/script/download.php) command line software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import rasterio as rio\n",
    "import fiona\n",
    "import datetime\n",
    "from automated_terminus_functions import resize_pngs, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008']\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# Enter folder containing the terminus box and RGI glacier outline shapefile(s):\n",
    "basepath = '/home/jukes/Documents/Sample_glaciers/' \n",
    "downloadpath ='/media/jukes/jukes1/LS8aws/' # folder containing downloaded Landsat images\n",
    "csvext = '_updated_test_Box008.csv' # same as in image download script, a file suffix\n",
    "csvpath = '/home/jukes/Documents/Sample_glaciers/' # folder to store CSV files\n",
    "# that describes the analysis (e.g., glacier or group of glaciers) \n",
    "\n",
    "# Input your velocity input file info\n",
    "vpath = basepath # path to folder containing velocity files\n",
    "vx_name = 'greenland_vel_mosaic250_vx_v1.tif' # MEaSUREs product\n",
    "vy_name = 'greenland_vel_mosaic250_vy_v1.tif' # MEaSUREs product\n",
    "no_data_val = -2000000000.0 # no data value for the velocity maps\n",
    "vt = 'day' # time unit for velocity (e.g., day for m/day, year for m/year)\n",
    "\n",
    "# Enter glacier IDs\n",
    "BoxIDs = []\n",
    "boxes = list(map(str, np.arange(8, 9, 1))) #1, 642, 1\n",
    "for BoxID in boxes: # convert integers to 3-digit strings with leading zeros\n",
    "    BoxID = BoxID.zfill(3)\n",
    "    BoxIDs.append(BoxID)\n",
    "# or read from a file\n",
    "print(BoxIDs) # show the final BoxIDs\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files that will be produced:\n",
      "\n",
      "Glacier_vel_updated_test_Box008.csv\n",
      "Buffdist_updated_test_Box008.csv\n",
      "imgdates_updated_test_Box008.csv\n"
     ]
    }
   ],
   "source": [
    "# filenames that will be written in this script\n",
    "# all with common extension\n",
    "print(\"CSV files that will be produced:\"); print()\n",
    "VEL_FILENAME = 'Glacier_vel'+csvext; print(VEL_FILENAME)\n",
    "BOX_FILENAME = 'Buffdist'+csvext; print(BOX_FILENAME)\n",
    "DATES_FILENAME = 'imgdates'+csvext; print(DATES_FILENAME) # acquisition dates for all downloaded Landsat images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Grab image acquisition dates from downloaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created in the download step:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Scene</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189</td>\n",
       "      <td>LE07_L1TP_027006_19990630_20200918_02_T1</td>\n",
       "      <td>1999-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>LE07_L1TP_030006_19990705_20200918_02_T1</td>\n",
       "      <td>1999-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>LE07_L1TP_027006_19990716_20200918_02_T1</td>\n",
       "      <td>1999-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188</td>\n",
       "      <td>LE07_L1TP_028006_19990723_20200918_02_T1</td>\n",
       "      <td>1999-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>LE07_L1TP_031006_19990829_20200918_02_T1</td>\n",
       "      <td>1999-08-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     Scene    datetime\n",
       "0         189  LE07_L1TP_027006_19990630_20200918_02_T1  1999-06-30\n",
       "1          51  LE07_L1TP_030006_19990705_20200918_02_T1  1999-07-05\n",
       "2          93  LE07_L1TP_027006_19990716_20200918_02_T1  1999-07-16\n",
       "3         188  LE07_L1TP_028006_19990723_20200918_02_T1  1999-07-23\n",
       "4         133  LE07_L1TP_031006_19990829_20200918_02_T1  1999-08-29"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the acquisition date datafile wasn't already created in the downloading step,\n",
    "if not os.path.exists(csvpath+DATES_FILENAME): \n",
    "    # create it now\n",
    "    datetimes = []; scenenames = []\n",
    "    for BoxID in BoxIDs:\n",
    "        for img in os.listdir(downloadpath+'Box'+BoxID+'/'):\n",
    "            if img.startswith('L') and img.endswith('TIF'):\n",
    "                datestr = img[17:25] # grab acquisition date from filename\n",
    "                date = datetime.datetime.strptime(datestr, '%Y%m%d') # save as datetime object\n",
    "                datetimes.append(date); scenenames.append(img[:40])\n",
    "                \n",
    "    # write to csv\n",
    "    dates_df.to_csv(path_or_buf = basepath+DATES_FILENAME, sep=',')  \n",
    "    \n",
    "else: # if it exists already, don't overwrite\n",
    "    print('Already created in the download step:')\n",
    "    dates_df = pd.read_csv(csvpath+DATES_FILENAME)\n",
    "dates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Reproject the images\n",
    "\n",
    "When images are downloaded across many different Landsat path rows, it's possible that they will be in several UTM projections. As a result, you will need to reproject them into a single projection (e.g., Greenland Polar Stereographic). \n",
    "\n",
    "#### If all your images are in the same projection, then you can skip this step. Change SKIP to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008 Reprojected folder exists already.\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "SKIP = False # if you want to skip this step, change to True\n",
    "desired_proj = '3413' # EPSG code for desired projection\n",
    "suffix = '_PS' # suffix for reprojected images - something that indicates the projection\n",
    "######################################################################################\n",
    "\n",
    "for BoxID in BoxIDs:\n",
    "    bp_out = downloadpath+'Box'+BoxID+'/' # path to downloaded files\n",
    "\n",
    "    # create output reprojected folder if does not exist\n",
    "    if os.path.exists(bp_out+'reprojected/'):\n",
    "        print(\"Box\"+BoxID, \"Reprojected folder exists already.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out+'reprojected/')\n",
    "        print(\"Box\"+BoxID+\" Reprojected directory made\")\n",
    "    \n",
    "    for image in os.listdir(bp_out): # all downloaded images\n",
    "        if image.startswith('L') and image.endswith('.TIF'):\n",
    "            \n",
    "            # if reprojection skipped, just move the image into the reprojected fodler\n",
    "            if SKIP:\n",
    "                shutil.copyfile(bp_out+image, bp_out+'reprojected/'+image)\n",
    "            \n",
    "            # if not skipped, reproject it\n",
    "            else:\n",
    "                imagename = image[:-4] # remove suffix\n",
    "                rp_PS = \"gdalwarp -t_srs EPSG:\"+desired_proj+' '+bp_out+image+\" \"\n",
    "                rp_PS += bp_out+'reprojected/'+imagename+suffix+\".TIF\"\n",
    "                subprocess.run(rp_PS, shell=True,check=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Calculate terminus box dimensions and rasterize - figure out how to crop to any image dimensions\n",
    "\n",
    "In this section, the minimum dimension of the terminus box is calculated which will be used in the generation of glacier flowlines in the analyze_wtmm_results.ipynb script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(left=528232.5, bottom=8443357.5, right=557677.5, top=8469367.5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_reader.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image spatial resolution of 15.0  meters calculated from  LC08_L1TP_032005_2015_B8_Buffer008.TIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jukes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FionaDeprecationWarning: Collection.__next__() is buggy and will be removed in Fiona 2.0. Switch to `next(iter(collection))`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Buff_dist_m</th>\n",
       "      <th>min_dim_px</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>008</td>\n",
       "      <td>9438</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 BoxID Buff_dist_m  min_dim_px\n",
       "0          0   008        9438         298"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the minimum dimensions of the terminus boxes\n",
    "mindimensions = [] # initialize empty list\n",
    "for BoxID in BoxIDs:\n",
    "    for file in os.listdir(basepath+'Box'+BoxID+'/'):\n",
    "        if 'UTM' in file and '.shp' in file and \"Box\" in file: # identify UTM projected box\n",
    "            boxpath = basepath+\"Box\"+BoxID+\"/\"+file  \n",
    "            termbox = fiona.open(boxpath)\n",
    "    \n",
    "    # automatically calculate spatial resolution of images and grab the extent\n",
    "    for img in os.listdir(downloadpath+'Box'+BoxID+'/'):\n",
    "        if img.startswith('L') and img.endswith('TIF'):\n",
    "            img_reader = rio.open(downloadpath+'Box'+BoxID+'/'+img)\n",
    "            PIXRES = img_reader.transform[0]\n",
    "            print('Image spatial resolution of',PIXRES,' meters calculated from ',img)\n",
    "            break # stop after the first image\n",
    "            \n",
    "    # grab the box coordinates:\n",
    "    box = termbox.next(); box_geom= box.get('geometry'); box_coords = box_geom.get('coordinates')[0]\n",
    "    points = []\n",
    "    for coord_pair in box_coords:\n",
    "        lat = coord_pair[0]; lon = coord_pair[1]; points.append([lat, lon])\n",
    "            \n",
    "    # Calculate distance between coord 1 and 2 and between 2 and 3\n",
    "    coord1 = points[0]; coord2 = points[1]; coord3 = points[2]   \n",
    "    dist1 = distance(coord1[0], coord1[1], coord2[0], coord2[1]);\n",
    "    dist2 = distance(coord2[0], coord2[1], coord3[0], coord3[1]) \n",
    "    mindim = int(np.min([dist1, dist2]))/PIXRES # calculate minimum dimensions, converted into pixels\n",
    "    mindimensions.append(int(mindim))\n",
    "\n",
    "if os.path.exists(csvpath+BOX_FILENAME): # if the buffer distance was calculated in the previous step\n",
    "    buff_df = pd.read_csv(csvpath+BOX_FILENAME, dtype=str)\n",
    "    buff_df['min_dim_px'] = mindimensions # add the minimum dimensions to the existing data\n",
    "else: # if not, create a new data file\n",
    "    buff_df = pd.DataFrame(list(zip(BoxIDs, mindimensions)), columns=['BoxID','min_dim_px'])\n",
    "buff_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "buff_df.to_csv(basepath+BOX_FILENAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the terminus boxes are rasterized using the **gdal_rasterize** command with the following syntax:\n",
    "\n",
    "    gdal_rasterize -burn 1.0 -tr x_resolution y_resolution -a_nodata 0.0 path_to_terminusbox.shp path_to_terminusbox_raster.TIF\n",
    "\n",
    "and subset to the image dimensions using **gdal_warp**:\n",
    "\n",
    "    gdalwarp -cutline path_to_Buffer###.shp -crop_to_cutline path_to_terminusbox_raster.TIF path_to_subset_raster_cut.TIF\n",
    "\n",
    "These rasterized terminus boxes will be used as a mask during the 2D WTMM analysis.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterize: gdal_rasterize -burn 1.0 -tr 15.0 15.0 -a_nodata 0.0 /home/jukes/Documents/Sample_glaciers/Box008/Box008.shp /home/jukes/Documents/Sample_glaciers/Box008/Box008.TIF\n",
      "Subset: gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Buffer008.shp -crop_to_cutline /home/jukes/Documents/Sample_glaciers/Box008/Box008.TIF /home/jukes/Documents/Sample_glaciers/Box008/Box008_raster_cut.TIF\n"
     ]
    }
   ],
   "source": [
    "# Rasterize images\n",
    "for index, row in buff_df.iterrows():\n",
    "    BoxID = row['BoxID']\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # path to box\n",
    "    buffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID # path to buffer\n",
    "    \n",
    "    terminusraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".TIF\" # path to rasterized box\n",
    "    cutraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\"_raster_cut.TIF\" # name for cropped file\n",
    "    \n",
    "    # Set rasterization command:\n",
    "    rasterize_cmd = 'gdal_rasterize -burn 1.0 -tr '+str(PIXRES)+' '+str(PIXRES) # PIXRES from image\n",
    "    rasterize_cmd += ' -a_nodata 0.0 '+terminusbox_path+'.shp '+terminusraster_path\n",
    "    print(\"Rasterize:\", rasterize_cmd); print()\n",
    "    \n",
    "    # Set subsetting command:\n",
    "    subset_cmd = 'gdalwarp -overwrite -cutline '+buffer_path+'.shp -crop_to_cutline '\n",
    "    subset_cmd += terminusraster_path+' '+cutraster_path\n",
    "    print('Subset:', subset_cmd)\n",
    "    \n",
    "#     subprocess.run(rasterize_cmd, shell=True,check=True) # rasterize with command terminal\n",
    "#     subprocess.run(subset_cmd, shell=True,check=True) # subset to buffer with command terminal\n",
    "    \n",
    "    \n",
    "# Crop to a shapefile or extent from the image - can be grabbed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Rotate all images so glacier flow is to the right\n",
    "\n",
    "## 4A) Calculate weighted average flow direction \n",
    "\n",
    "The following code processes ice velocity (vx, vy) rasters to determine each glacier of interest's weighted average flow direction. These files should be placed in the base directory (basepath). The rasters are subset using the terminus box shapefile or the Randolph Glacier Inventory outlines using a GDAL command (**gdalwarp**) with the following syntax:\n",
    "\n",
    "    gdalwarp -cutline path_to_terminusbox.shp -crop_to_cutline path_to_input_velocity.TIF path_to_output_velocity.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008\n",
      "Using terminus box.\n",
      "Box008 done.\n"
     ]
    }
   ],
   "source": [
    "# crop the velocity raster to the RGI shapefile if available, terminus box if not\n",
    "for BoxID in BoxIDs:\n",
    "    print('Box'+BoxID)\n",
    "    terminus_path = basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+\".shp\"  # path to RGI shapefile\n",
    "\n",
    "    if not os.path.exists(terminus_path): # if the RGI shapefile does not exist\n",
    "        terminus_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"  # set the path to the box shapefile instead \n",
    "        print('Using terminus box.')\n",
    "    else:\n",
    "        print('Using RGI glacier outline.')\n",
    "\n",
    "    vx_in = basepath+vx_name; vy_in = basepath+vy_name # input velocity file paths\n",
    "    \n",
    "    # output paths for the cropped velocity data:\n",
    "    vx_out = terminus_path[:-4]+'_'+vx_name; vy_out = terminus_path[:-4]+'_'+vy_name\n",
    "    \n",
    "    # subset x and y velocity files\n",
    "    v_subset1 = 'gdalwarp -cutline '+terminus_path+' -crop_to_cutline '+vx_in+\" \"+vx_out\n",
    "    v_subset2 = 'gdalwarp -cutline '+terminus_path+' -crop_to_cutline '+vy_in+\" \"+vy_out\n",
    "    subprocess.run(v_subset1, shell=True, check=True)\n",
    "    subprocess.run(v_subset2, shell=True, check=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID+' done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, these subset velocity rasters are opened using the **rasterio** package and read into arrays. They are filtered for anomalous values and the velocity magnitudes are converted into weights. Then the **numpy.average()** function is used to calculated the weighted average flow directions where the flow directions of the pixels where the highest velocities are found are weighted higher. The resulting average flow direction will be used to rotate the images of the glaciers so that their flow is due right.\n",
    "\n",
    "#### For the slow-moving Greenland peripheral glaciers, the ice velocities are very uncertain. Below, we use the manual delineations of the Greenland peripheral glaciers in 2000 and 2015 to approximate the flow direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['301', '289', '283', '265', '241', '223', '285', '181', '097', '091', '067', '083', '221', '173', '113', '101', '089', '082', '100', '112', '118', '130', '160', '196', '208', '226', '256', '262', '280', '298', '322', '072', '074', '080', '082', '084', '102', '114', '134', '132', '144', '159', '188', '189', '198', '207', '212', '222', '224', '234', '242', '243', '249', '254', '258', '264', '267', '272', '273', '278', '282', '284', '288', '297', '305', '306', '307', '315', '318', '321', '324', '327', '330', '331', '338', '341', '344', '354', '356', '357', '358', '359', '362', '363', '364', '369', '370', '371', '372', '373', '374', '376', '377', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '404', '405', '406', '407', '408', '409', '410', '414', '415', '416', '417', '418', '419', '420', '421', '422', '427', '430', '431', '434', '436', '438', '440']\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# ONLY APPLIES TO GREENLAND PERIPHERAL GLACIERS\n",
    "gperiph = True # SWITCH TO FALSE IF NOT USING THE PERIPHERAL GLACIER DATA\n",
    "\n",
    "badvelocities = ['301', '289', '283', '265', '241', '223', '285', '181', '097', '091', '067','083',\n",
    "                 '221', '173', '113', '101', '089', '082', '100', '112', '118', '130', '160', '196', \n",
    "                 '208', '226', '256', '262', '280', '298', '322', '072', '074', '080', '082', '084',\n",
    "                '102', '114', '134', '132', '144', '159', '188', '189', '198', '207', '212', '222',\n",
    "                '224', '234', '242', '243', '249', '254', '258', '264', '267', '272', '273', '278', \n",
    "                '282', '284', '288', '297', '305', '306', '307', '315', '318', '321', '324', '327',\n",
    "                '330', '331', '338', '341', '344', '354', '356', '357', '358', '359', '362', '363',\n",
    "                 '364', '369', '370', '371', '372', '373', '374', '376', '377', '379', '380', '381', \n",
    "                 '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393',\n",
    "                 '394', '395', '396', '397', '398', '399', '400', '401' ,'404', '405', '406', '407',\n",
    "                 '408', '409', '410', '414', '415', '416', '417', '418', '419', '420', '421', '422',\n",
    "                 '427', '430', '431', '434', '436', '438', '440']\n",
    "print(badvelocities)\n",
    "\n",
    "# path to the folder with the 2000-2015 manual terminus delineations:\n",
    "path2000_2015 = '/media/jukes/easystore/TEMP_STORE_FROM_JUKES1/2000_2015/'\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Flow_dir</th>\n",
       "      <th>Max_speed</th>\n",
       "      <th>Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008</td>\n",
       "      <td>317.060795</td>\n",
       "      <td>1714.385254</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoxID    Flow_dir    Max_speed  Pixels\n",
       "0   008  317.060795  1714.385254     284"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate glacier flow direction and speed\n",
    "boxes = []; avg_rot = []; max_mag = []; num_cells = [] # to hold the boxIDs, rotation angle, max. glacier speed, and number of pixels\n",
    "\n",
    "for BoxID in BoxIDs:\n",
    "    rot_angles = []; max_magnitudes = [] # store angles and speeds from all pixels\n",
    "    \n",
    "    # determine if RGI outline was used to subset velocities\n",
    "    rgi_exists = 0\n",
    "    for file in os.listdir(basepath+\"Box\"+BoxID):\n",
    "        if file.startswith('RGI'):\n",
    "            rgi_exists = 1\n",
    "            \n",
    "    if rgi_exists == 1: # if yes, open those files    \n",
    "        vx = rio.open(basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+'_'+vx_name, \"r\") \n",
    "        vy = rio.open(basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+'_'+vy_name, \"r\") \n",
    "    else: # if not, they were subset using the boxes. Open those files\n",
    "        vx = rio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vx_name, \"r\") \n",
    "        vy = rio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vy_name, \"r\") \n",
    "    vx_array = vx.read(); vy_array = vy.read() # read as numpy array\n",
    "    \n",
    "    # remove no data values\n",
    "    vx_masked = vx_array[vx_array != no_data_val]\n",
    "    vy_masked = vy_array[vy_array != no_data_val]\n",
    "    \n",
    "    # calculate flow direction\n",
    "    direction = np.arctan2(vy_masked, vx_masked)*180/np.pi \n",
    "    # transform so any negative angles are placed on 0 to 360 scale:\n",
    "    if len(direction[direction < 0]) > 0:\n",
    "        direction[direction < 0] = 360.0+direction[direction < 0]\n",
    "    \n",
    "    # calculate speed (flow magnitude)\n",
    "    magnitude = np.sqrt((vx_masked*vx_masked) + (vy_masked*vy_masked)) \n",
    "    \n",
    "    ncells = len(direction) # number of pixels\n",
    "    if ncells > 0:\n",
    "        # Determine if there are a large number of direction pixels with values > 200.0\n",
    "        # If so, it's probably pointing East\n",
    "        dir_range = direction.max() - direction.min()\n",
    "        if dir_range > 200.0 and len(direction[direction > 200]): # if large range and values above 200\n",
    "            direction[direction > 180] = direction[direction > 180] - 360.0 # transform those values on a negative scale\n",
    "            # calculate weights (0 - 1) from magnitudes\n",
    "            mag_range = magnitude.max() - magnitude.min()\n",
    "            stretch = 1/mag_range; weights = stretch*(magnitude - magnitude.min()) # weights for averaging\n",
    "            avg_dir = np.average(direction, weights=weights) # calculate average flow direction\n",
    "            if avg_dir < 0: # if negative:\n",
    "                avg_dir = avg_dir + 360.0 # transform back to 0 to 360 scale\n",
    "        else:\n",
    "            mag_range = magnitude.max() - magnitude.min(); stretch = 1/mag_range\n",
    "            weights = stretch*(magnitude - magnitude.min())\n",
    "            avg_dir = np.average(direction, weights=weights)\n",
    "                \n",
    "        if vt == 'day':\n",
    "            max_magnitude = magnitude.max()\n",
    "        elif vt == 'year':\n",
    "            yr_day_conv = 0.00273973 # conversion to m/d from m/a\n",
    "            max_magnitude = magnitude.max()*yr_day_conv  \n",
    "    else: # no velocity pixels remaining once cropped\n",
    "        avg_dir = np.NaN ; max_magnitude = np.NaN # no velocities to calculate this with\n",
    "    \n",
    "    if gperiph == True and BoxID in badvelocities: # ONLY APPLIES TO GREENLAND PERIPHERAL GLACIERS\n",
    "        # grab the 2000 and 2015 delineation centroids:\n",
    "        shp2000 = fiona.open(path2000_2015+'GreenlandPeriph_term2000_'+BoxID+'.shp'); feat2000= shp2000.next()\n",
    "        lineshp2000 = LineString(feat2000['geometry']['coordinates'])\n",
    "        cent2000 = np.array(lineshp2000.centroid)\n",
    "\n",
    "        shp2015 = fiona.open(path2000_2015+'GreenlandPeriph_term2015_'+BoxID+'.shp'); feat2015= shp2015.next()\n",
    "        lineshp2015 = LineString(feat2015['geometry']['coordinates'])\n",
    "        cent2015 = np.array(lineshp2015.centroid)\n",
    "\n",
    "        # grab displacements and use to calculate flow direction in degrees\n",
    "        y = cent2000[1] - cent2015[1]; x = cent2000[0] - cent2015[0]\n",
    "        avg_dir = np.arctan2(y,x)*180/np.pi\n",
    "        if avg_dir < 0:\n",
    "            avg_dir = 360.0+avg_dir\n",
    "         \n",
    "        # if max_magnitude cannot be calculated from the velocity raster (pixels == 0)\n",
    "        if ncells == 0:\n",
    "            # use displacements and time to approximate speed in m/d\n",
    "            yrs = 15.0 # 2000 to 2015\n",
    "            max_magnitude = np.sqrt((y*y)+(x*x))/yrs*yr_day_conv\n",
    "        ncells = np.NaN # enter NaN to indicate that velocity raster was not used\n",
    "   \n",
    "    # Append values to lists:\n",
    "    avg_rot.append(avg_dir); max_mag.append(max_magnitude); boxes.append(BoxID); num_cells.append(ncells)  \n",
    "\n",
    "# store the flow direction (rotation angle), maximum magnitude\n",
    "velocities_df = pd.DataFrame(list(zip(boxes,avg_rot, max_mag, num_cells)), \n",
    "                             columns=['BoxID','Flow_dir', 'Max_speed', 'Pixels'])\n",
    "velocities_df = velocities_df.sort_values(by='BoxID')\n",
    "velocities_df # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write velocity data to CSV\n",
    "velocities_df.to_csv(path_or_buf = basepath+VEL_FILENAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B) Rotate images by glacier flow direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow_dir</th>\n",
       "      <th>Max_speed</th>\n",
       "      <th>Pixels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoxID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>317.06079483032227</td>\n",
       "      <td>1714.38525390625</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Flow_dir         Max_speed Pixels\n",
       "BoxID                                             \n",
       "008    317.06079483032227  1714.38525390625    284"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the glacier velocity file as velocities_df if not already loaded:\n",
    "velocities_df = pd.read_csv(basepath+VEL_FILENAME, sep=',', dtype=str, usecols=[1,2,3,4])\n",
    "velocities_df = velocities_df.set_index('BoxID')\n",
    "velocities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotated image folder already exists.\n"
     ]
    }
   ],
   "source": [
    "###### ENTER FOLDER NAME FOR ROTATED IMAGES #############################\n",
    "folder_name = 'rotated_c2' # rotated collection 2 images\n",
    "#########################################################################\n",
    "\n",
    "# make directory for rotated images in BoxID folders if it doesn't already exist\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    if os.path.exists(downloadpath+\"Box\"+BoxID+'/'+folder_name+'/'):\n",
    "        print(\"Rotated image folder already exists.\")\n",
    "    else:\n",
    "        os.mkdir(downloadpath+\"Box\"+BoxID+'/'+folder_name+'/')\n",
    "        print(\"Rotated image folder made for Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move rasterized terminus box into reprojected folder, since it will also need to be rotated:\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    boxfile = 'Box'+BoxID+'_raster_cut.TIF'\n",
    "    boxrasterpath = basepath+'Box'+BoxID+'/'+boxfile\n",
    "    newpath = downloadpath+'Box'+BoxID+'/reprojected/'+boxfile\n",
    "    shutil.copyfile(boxrasterpath, newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all images to png for rotation using image magick\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/reprojected/; '+'magick mogrify -format png *.TIF'\n",
    "    subprocess.run(command, shell=True,check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate the images\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index; print(\"Box\"+BoxID) # keep track of progress\n",
    "    for file in os.listdir(downloadpath+\"Box\"+BoxID+'/reprojected/'):\n",
    "        if file.endswith('.png'):\n",
    "            print(file)\n",
    "            img  = Image.open(downloadpath+\"Box\"+BoxID+'/reprojected/'+file)\n",
    "            rotated = img.rotate(-float(row['Flow_dir']))\n",
    "            rotated.save(downloadpath+\"Box\"+BoxID+'/'+folder_name+'/R_'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Crop all images to the same size and convert to pgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop all images to the same size\n",
    "for BoxID in BoxIDs:\n",
    "    resizepath = downloadpath+\"Box\"+BoxID+'/'+folder_name+'/' # path to rotated images\n",
    "    resize_pngs(resizepath) # crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all final files to pgm\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/'+folder_name+'/; '+'mogrify -depth 16 -format pgm *.png'\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove intermediate pngs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename the rasterized terminus box files if necessary\n",
    "# for BoxID in BoxIDs:\n",
    "#     files = os.listdir(downloadpath+'Box'+BoxID+'/rotated_c2/')\n",
    "#     for file in files:\n",
    "#         if file.startswith('R_Box'+BoxID+'_cut'):\n",
    "#             rpath = downloadpath+'Box'+BoxID+'/rotated_c2/'\n",
    "#             os.rename(rpath+file, rpath+'R_Box'+BoxID+'_raster_cut'+file[-4:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
