{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenland peripheral glacier automated terminus detection image download and pre-processing\n",
    "\n",
    "By Jukes Liu (jukes.liu@boisestate.edu)\n",
    "\n",
    "_Last modified 05-07-2020._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Set-up: import necessary packages, set paths, and glaciers of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imsave\n",
    "\n",
    "#geospatial packages\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely\n",
    "import rasterio\n",
    "\n",
    "# Enable fiona KML file reading driver\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "\n",
    "#Set base paths\n",
    "basepath='/home/jukes/Documents/Sample_glaciers/' # folder containing the box shapefile and info\n",
    "downloadpath = '/media/jukes/jukes1/LS8aws/' # folder to contain the downloaded images\n",
    "\n",
    "# import necessary functions from automated-glacier-terminus.py\n",
    "os.chdir('/home/jukes/automated-glacier-terminus') #import necessary functions:\n",
    "from automated_terminus_functions import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter glaciers of interest by their 3-digit BoxIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101', '106', '111', '116', '121', '126', '131', '136', '141', '146', '151', '156', '161', '166', '171', '176', '181', '186', '191', '196', '201', '206', '211', '216', '221', '226', '231', '236', '241', '246', '251', '256', '261', '266', '271', '276', '281', '286', '291', '296']\n"
     ]
    }
   ],
   "source": [
    "BoxIDs = np.arange(101, 301, 5)\n",
    "BoxIDs = list(map(str, BoxIDs))\n",
    "print(BoxIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new folders corresponding to these glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists already in Sample_glaciers for Box 101\n",
      "Path exists already in LS8aws for Box 101\n",
      "Path exists already in Sample_glaciers for Box 106\n",
      "Path exists already in LS8aws for Box 106\n",
      "Path exists already in Sample_glaciers for Box 111\n",
      "Path exists already in LS8aws for Box 111\n",
      "Path exists already in Sample_glaciers for Box 116\n",
      "Path exists already in LS8aws for Box 116\n",
      "Path exists already in Sample_glaciers for Box 121\n",
      "Path exists already in LS8aws for Box 121\n",
      "Path exists already in Sample_glaciers for Box 126\n",
      "Path exists already in LS8aws for Box 126\n",
      "Path exists already in Sample_glaciers for Box 131\n",
      "Path exists already in LS8aws for Box 131\n",
      "Path exists already in Sample_glaciers for Box 136\n",
      "Path exists already in LS8aws for Box 136\n",
      "Path exists already in Sample_glaciers for Box 141\n",
      "Path exists already in LS8aws for Box 141\n",
      "Path exists already in Sample_glaciers for Box 146\n",
      "Path exists already in LS8aws for Box 146\n",
      "Path exists already in Sample_glaciers for Box 151\n",
      "Path exists already in LS8aws for Box 151\n",
      "Path exists already in Sample_glaciers for Box 156\n",
      "Path exists already in LS8aws for Box 156\n",
      "Path exists already in Sample_glaciers for Box 161\n",
      "Path exists already in LS8aws for Box 161\n",
      "Path exists already in Sample_glaciers for Box 166\n",
      "Path exists already in LS8aws for Box 166\n",
      "Path exists already in Sample_glaciers for Box 171\n",
      "Path exists already in LS8aws for Box 171\n",
      "Path exists already in Sample_glaciers for Box 176\n",
      "Path exists already in LS8aws for Box 176\n",
      "Path exists already in Sample_glaciers for Box 181\n",
      "Path exists already in LS8aws for Box 181\n",
      "Path exists already in Sample_glaciers for Box 186\n",
      "Path exists already in LS8aws for Box 186\n",
      "Path exists already in Sample_glaciers for Box 191\n",
      "Path exists already in LS8aws for Box 191\n",
      "Path exists already in Sample_glaciers for Box 196\n",
      "Path exists already in LS8aws for Box 196\n",
      "Path exists already in Sample_glaciers for Box 201\n",
      "Path exists already in LS8aws for Box 201\n",
      "Path exists already in Sample_glaciers for Box 206\n",
      "Path exists already in LS8aws for Box 206\n",
      "Path exists already in Sample_glaciers for Box 211\n",
      "Path exists already in LS8aws for Box 211\n",
      "Path exists already in Sample_glaciers for Box 216\n",
      "Path exists already in LS8aws for Box 216\n",
      "Path exists already in Sample_glaciers for Box 221\n",
      "Path exists already in LS8aws for Box 221\n",
      "Path exists already in Sample_glaciers for Box 226\n",
      "Path exists already in LS8aws for Box 226\n",
      "Path exists already in Sample_glaciers for Box 231\n",
      "Path exists already in LS8aws for Box 231\n",
      "Path exists already in Sample_glaciers for Box 236\n",
      "Path exists already in LS8aws for Box 236\n",
      "Path exists already in Sample_glaciers for Box 241\n",
      "Path exists already in LS8aws for Box 241\n",
      "Path exists already in Sample_glaciers for Box 246\n",
      "Path exists already in LS8aws for Box 246\n",
      "Path exists already in Sample_glaciers for Box 251\n",
      "Path exists already in LS8aws for Box 251\n",
      "Path exists already in Sample_glaciers for Box 256\n",
      "Path exists already in LS8aws for Box 256\n",
      "Path exists already in Sample_glaciers for Box 261\n",
      "Path exists already in LS8aws for Box 261\n",
      "Path exists already in Sample_glaciers for Box 266\n",
      "Path exists already in LS8aws for Box 266\n",
      "Path exists already in Sample_glaciers for Box 271\n",
      "Path exists already in LS8aws for Box 271\n",
      "Path exists already in Sample_glaciers for Box 276\n",
      "Path exists already in LS8aws for Box 276\n",
      "Path exists already in Sample_glaciers for Box 281\n",
      "Path exists already in LS8aws for Box 281\n",
      "Path exists already in Sample_glaciers for Box 286\n",
      "Path exists already in LS8aws for Box 286\n",
      "Path exists already in Sample_glaciers for Box 291\n",
      "Path exists already in LS8aws for Box 291\n",
      "Path exists already in Sample_glaciers for Box 296\n",
      "Path exists already in LS8aws for Box 296\n"
     ]
    }
   ],
   "source": [
    "# create new BoxID folders \n",
    "for BoxID in BoxIDs:\n",
    "    # create folder in Sample_glaciers\n",
    "    if os.path.exists(basepath+'Box'+BoxID)==True:\n",
    "#         shutil.rmtree(basepath+'Box'+BoxID)\n",
    "        print(\"Path exists already in Sample_glaciers for Box\", BoxID)\n",
    "    else:\n",
    "        os.mkdir(basepath+'Box'+BoxID)\n",
    "        \n",
    "    # move terminus box shapefile into the new folder\n",
    "    ID = int(BoxID) # make into an integer in order to grab the .shp files from Boxes_individual\n",
    "    boxespath = '/media/jukes/jukes1/Boxes_individual/'\n",
    "    for file in os.listdir(boxespath):\n",
    "        if file.startswith(str(ID)):\n",
    "            if len(file) == len(str(ID))+4:\n",
    "#             if file.endswith('.dbf') or file.endswith('.prj') or file.endswith('.qpj') or file.endswith('.shx') or file.endswith('.shp'):\n",
    "                shutil.copyfile(boxespath+file, basepath+'Box'+BoxID+'/'+BoxID+file[-4:])\n",
    "    \n",
    "    # make new download path folder\n",
    "    if os.path.exists(downloadpath+'/Box'+BoxID)==True:\n",
    "        print(\"Path exists already in LS8aws for Box\", BoxID)\n",
    "    else:\n",
    "        os.mkdir(downloadpath+'/Box'+BoxID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Find all the Landsat Path Row identifiers that overlap the glaciers\n",
    "\n",
    "This step requires the WRS-2_bound_world.kml file containing the footprints of all the Landsat scene boundaries available through the USGS (https://www.usgs.gov/land-resources/nli/landsat/landsat-shapefiles-and-kml-files). To check if they overlap the glacier terminus box shapefiles, the box shapefiles must be reprojected into WRS84 coordinates (ESPG: 4326) using GDAL command line command:\n",
    "\n",
    "    ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:NEW_EPSG_NUMBER -s_srs EPSG:OLD_EPSG_NUMBER output.shp input.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n",
      "THIS ISNT WORKING - WHY?\n"
     ]
    }
   ],
   "source": [
    "# reproject\n",
    "for BoxID in BoxIDs:\n",
    "    boxpath = basepath+\"Box\"+BoxID+\"/Box\"+BoxID\n",
    "    rp = 'ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:4326 -s_srs EPSG:3413 '+boxpath+'_WRS.shp '+boxpath+'.shp'\n",
    "    subprocess.call(rp, shell=True)\n",
    "    print(\"THIS ISNT WORKING - WHY?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the boxes' WRS84 coordinates\n",
    "box_points = {}\n",
    "for BoxID in BoxIDs:\n",
    "    boxpath = basepath+\"Box\"+BoxID+\"/Box\"+BoxID; termbox = fiona.open(boxpath+'_WRS.shp')\n",
    "    box = termbox.next()\n",
    "    box_geom= box.get('geometry'); box_coords = box_geom.get('coordinates')[0]  \n",
    "    points = [] # to hold the vertices\n",
    "    for coord_pair in box_coords:\n",
    "        lat = coord_pair[0]; lon = coord_pair[1]\n",
    "        #create shapely points and append to points list\n",
    "        point = shapely.geometry.Point(lat, lon)\n",
    "#         print(point)\n",
    "        points.append(point)\n",
    "    box_points.update({BoxID: points}); print(\"Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the kml file with the pathrow bounds as WRS\n",
    "WRS = fiona.open(basepath+'WRS-2_bound_world.kml', driver='KML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all the scene footprints and stores those that overlap ALL of your shapefile vertices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to hold the paths and rows and BoxIDs\n",
    "paths = []; rows = []; boxes = []\n",
    "\n",
    "#loop through all features in the WRS .kml:\n",
    "for feature in WRS:\n",
    "    #create shapely polygons with the pathrow bounds\n",
    "    coordinates = feature['geometry']['coordinates'][0]\n",
    "    coords = [xy[0:2] for xy in coordinates]\n",
    "    pathrow_poly = Polygon(coords)\n",
    "    #grab the path and row name\n",
    "    pathrowname = feature['properties']['Name']  \n",
    "    path = pathrowname.split('_')[0]; row = pathrowname.split('_')[1]\n",
    "#     print(path, row)\n",
    "    \n",
    "    #for each feature, loop through each of the box_points\n",
    "    for BoxID in box_points:\n",
    "        #create a counter for number of box_points in the pathrow_geom:\n",
    "        box_points_in = 0\n",
    "        \n",
    "        points = box_points.get(BoxID)\n",
    "        for i in range(0, len(points)):\n",
    "            point = points[i]\n",
    "            #if the pathrow shape contains the point\n",
    "            if point.within(pathrow_poly):\n",
    "                #append the counter\n",
    "                box_points_in = box_points_in+1\n",
    "#         print(box_points_in)\n",
    "        if box_points_in == 5:\n",
    "            #save that path row and boxID\n",
    "            paths.append('%03d' % int(path)); rows.append('%03d' % int(row))\n",
    "            boxes.append(BoxID)\n",
    "\n",
    "boxes_pr_df = pd.DataFrame(list(zip(boxes, paths, rows)), columns=['BoxID','Path', 'Row'])\n",
    "boxes_pr_df = boxes_pr_df.sort_values(by='BoxID')\n",
    "boxes_pr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write it to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_FILENAME = 'LS_pathrows_SE_2.csv'\n",
    "boxes_pr_df.to_csv(path_or_buf = basepath+PR_FILENAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Download metadata (MTL.txt) files for all available images over glaciers\n",
    "\n",
    "Bulk download Landsat-8 images and metadata stored on the Amazon Web Services cloud (s3 bucket) over a region of interest using the Amazon Web Services command line interface. Follow instructions at https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html to get aws commands onto your command terminal. The syntax for grabbing an individual metadata file is as follows:\n",
    "\n",
    "     aws --no-sign-request s3 cp s3://landsat-pds/L8/path/row/LC8pathrowyear001LGN00/LC8pathrowyear001LGN00_MTL.txt /path_to/output/\n",
    "\n",
    "Access https://docs.opendata.aws/landsat-pds/readme.html to learn more.\n",
    "\n",
    "#### Download the metadatafiles into Path_Row folders created using:\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/L8/031/005/ Output/path/LS8aws/Path031_Row005/ --recursive --exclude \"*\" --include \"*.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the dataframe with your path row combinations:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    #grab the path row names and set the folder name\n",
    "    path = row['Path']; row = row['Row']; folder_name = 'Path'+path+'_Row'+row\n",
    "\n",
    "    #set path to access the image on the amazon cloud:\n",
    "    bp_in = 's3://landsat-pds/L8/'; totalp_in = bp_in+path+'/'+row+'/'\n",
    "    #set output path for the downloaded files:\n",
    "    bp_out = outputpath+newfoldername+'/'+folder_name+'/'; print(bp_out)\n",
    "    \n",
    "    #create Path_Row folders and do the download if they don't exist already to hold the metadata\n",
    "    if os.path.exists(bp_out):\n",
    "        print(folder_name, \" EXISTS ALREADY. SKIP.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out)\n",
    "        print(folder_name+\" directory made\")\n",
    "\n",
    "        command = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*.txt\"'\n",
    "        print(command)\n",
    "\n",
    "        #call the command line that downloads the metadata files using aws\n",
    "        subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Determine how cloudy the images are using the LS quality band files subset to terminus box - PARALLEL\n",
    "\n",
    "Use gdalwarp commands and the __vsicurl__ link to download subset of the quality band we will use to determine cloud cover:\n",
    "\n",
    "    gdalwarp -cutline path_to_shp.shp -crop_to_cutline /vsicurl/https://landsat-pds.s3.amazonaws.com/L8/031/005/LC80310052013143LGN01/LC80310052013143LGN01_BQA.TIF path_to_subsetBQA.TIF\n",
    "    \n",
    "These will need to go into the newly created BoxID folders because the subset of the same image for different boxes will be different. We will need to reproject the shapefiles into UTM to match the LS8 bands. We must grab the UTM zones from the metadata files downloaded and fill in the following syntax:\n",
    "    \n",
    "    ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326zone -s_srs EPSG:3413 output.shp input.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = {}; zone_df = []\n",
    "\n",
    "#Loop through the dataframe with your path row combinations:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    path = row['Path']; row = row['Row']; BoxID = index; folder_name = 'Path'+path+'_Row'+row\n",
    "    \n",
    "    #set path to path row folders to grab scenes from and output path according to each of the BoxIDs\n",
    "    pr_folderpath = outputpath+newfoldername+'/'+folder_name+'/'\n",
    "    \n",
    "    #set shapefile path for each BoxID\n",
    "    pathtoshp = basepath+\"Box\"+BoxID+\"/Box\"+BoxID\n",
    "    \n",
    "    #if there are files in the folder:\n",
    "    if len(os.listdir(pr_folderpath)) > 0:\n",
    "        #grab UTM Zone from the first metadata file in the pr_folderpath\n",
    "        mtl_scene = os.listdir(pr_folderpath)[0]\n",
    "        mtl = open(pr_folderpath+mtl_scene+'/'+mtl_scene+'_MTL.txt', 'r')\n",
    "        \n",
    "        #loop through each line in metadata to find the UTM ZONE\n",
    "        for line in mtl:\n",
    "            variable = line.split(\"=\")[0]\n",
    "            if (\"UTM_ZONE\" in variable): \n",
    "                #save it:\n",
    "                zone = '%02d' % int(line.split(\"=\")[1][1:-1])\n",
    "                zones.update({folder_name: zone}); zone_df.append(zone)\n",
    "                \n",
    "        #reproject shapefile(s) into UTM\n",
    "        zone = zones[folder_name]\n",
    "        rp_shp = 'ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326'+zone+' -s_srs EPSG:4326 '+pathtoshp+\"_UTM_\"+zone+\".shp \"+pathtoshp+\"_WRS.shp\"\n",
    "        subprocess.call(rp_shp, shell=True)\n",
    "    #if not, append nan to zone and do nothing\n",
    "    else:\n",
    "        zone_df.append(np.nan)\n",
    "        \n",
    "    #grab the names of the LS8 scenes from the downloaded metadatafiles using os.listdir() in the Path_Row folders\n",
    "    scenes = os.listdir(pr_folderpath)\n",
    "\n",
    "    for scene in scenes:\n",
    "        #if the file grabbed does correspond to a Landsat scene (contains LGN as the 5th -3rd from last letters):\n",
    "        if len(scenes) > 0 and scene.endswith(\"LGN\", -5, -2):            \n",
    "            #set path to reprojected box shp\n",
    "            pathtoshp_rp = pathtoshp+'_UTM_'+zones[folder_name]\n",
    "            \n",
    "            #set path to the quality band (BQA.TIF) in the cloud using the scenename\n",
    "            pathtoBQA = '/vsicurl/https://landsat-pds.s3.amazonaws.com/L8/'+path+'/'+row+'/'+scene+\"/\"+scene+\"_BQA.TIF\"\n",
    "            subsetout = pr_folderpath+scene+\"/\"+scene+'_BQA_Box'+BoxID+'.TIF'\n",
    "\n",
    "            #Check download command syntax\n",
    "            BQA_dwnld_cmd = 'gdalwarp -cutline '+pathtoshp_rp+'.shp -crop_to_cutline '+pathtoBQA+' '+subsetout\n",
    "\n",
    "            #When you've checked everything, uncomment the following to run:\n",
    "            subprocess.call(BQA_dwnld_cmd, shell=True)\n",
    "            print(scene+'_BQA_Box'+BoxID+'.TIF subset downloaded')\n",
    "\n",
    "boxes_pr_df['Zone'] = boxes_pr_df\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-save csv file with the zone information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_FILENAME = 'LS_pathrows_SE_2.csv'\n",
    "boxes_pr_df.to_csv(path_or_buf = basepath+PR_FILENAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Create buffer zone around terminus boxes and rasterize/subset terminus boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code pulls the buffer distances around the terminus boxes from an existing .csv file with the exported attributes tables for the peripheral glacier terminus boxes. These buffer distances will be used to create a buffer zone to subset the Landsat scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers = []\n",
    "#Calculate a buffer distance around the terminus box using the UTM projected boxes\n",
    "for BoxID in BoxIDs:\n",
    "    buff_distances = []\n",
    "\n",
    "    for file in os.listdir(basepath+'Box'+BoxID+'/'):\n",
    "        if 'UTM' in file and '.shp' in file and \"Box\" in file:\n",
    "            print(file)\n",
    "            boxpath = basepath+\"Box\"+BoxID+\"/\"+file  \n",
    "            termbox = fiona.open(boxpath)\n",
    "            #grab the box feature:\n",
    "            box = termbox.next(); box_geom= box.get('geometry'); box_coords = box_geom.get('coordinates')[0]\n",
    "            points = []\n",
    "            for coord_pair in box_coords:\n",
    "                lat = coord_pair[0]; lon = coord_pair[1]; points.append([lat, lon])\n",
    "            \n",
    "            #Calculate distance between 1 and 2 and distance between 2 and 3 and pick the longer one\n",
    "            coord1 = points[0]; coord2 = points[1]; coord3 = points[2]   \n",
    "            dist1 = distance(coord1[0], coord1[1], coord2[0], coord2[1]);\n",
    "            dist2 = distance(coord2[0], coord2[1], coord3[0], coord3[1])  \n",
    "            buff_dist = int(np.max([dist1, dist2]))\n",
    "            buff_distances.append(buff_dist) \n",
    "    buffer = buff_distances[0]; buffers.append(buffer)\n",
    "buff_df = pd.DataFrame(list(zip(BOXIDS, buffers)), columns=['BoxID', 'Buff_dist_m'])\n",
    "buff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section creates a buffer zone shapefile using GDAL command **ogr2ogr** with the following syntax:\n",
    "\n",
    "    ogr2ogr Buffer###.shp path_to_terminusbox###.shp  -dialect sqlite -sql \"SELECT ST_Buffer(geometry, buffer_distance) AS geometry,*FROM 'Box###'\" -f \"ESRI Shapefile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in buff_df.iterrows():\n",
    "    BoxID = row['BoxID']\n",
    "    buff_dist = str(row['Buff_dist_m'])\n",
    "    \n",
    "    #SET path to the terminus box shapefiles\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"\n",
    "    outputbuffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\"\n",
    "    \n",
    "    #SET buffer command and print to check it\n",
    "    buffer_cmd = 'ogr2ogr '+outputbuffer_path+\" \"+terminusbox_path+' -dialect sqlite -sql \"SELECT ST_Buffer(geometry, '+buff_dist+\") AS geometry,*FROM 'Box\"+BoxID+\"'\"+'\" -f \"ESRI Shapefile\"'\n",
    "    print(buffer_cmd)\n",
    "    \n",
    "    subprocess.call(buffer_cmd, shell=True) \n",
    "    print(\"Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminus box shapefiles are then rasterized (to be used as a mask during the WTMM filering) using the GDAL **gdal_rasterize** command and subset to the buffer zone using the GDAL **gdalwarp** command using the following syntax:\n",
    "\n",
    "1) Rasterize\n",
    "\n",
    "    gdal_rasterize -burn 1.0 -tr x_resolution y_resolution -a_nodata 0.0 path_to_terminusbox.shp path_to_terminusbox_raster.TIF\n",
    "\n",
    "The x_resolution and y_resolution are set to be 15.0 (meters) to match the Landsat B8 resolution.\n",
    "    \n",
    "2) Subset\n",
    "\n",
    "    gdalwarp -cutline path_to_Buffer###.shp -crop_to_cutline path_to_terminusbox_raster.TIF path_to_subset_raster_cut.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in buff_df.iterrows():\n",
    "    BoxID = row['BoxID']\n",
    "    #SET path to the terminus box shapefiles\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"\n",
    "    buffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\"\n",
    "    \n",
    "    #output raster path:\n",
    "    terminusraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".TIF\"\n",
    "    cutraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\"_raster_cut.TIF\"\n",
    "    \n",
    "    #SET commands and print to check\n",
    "    rasterize_cmd = 'gdal_rasterize -burn 1.0 -tr 15.0 15.0 -a_nodata 0.0 '+terminusbox_path+' '+terminusraster_path\n",
    "    subsetbuffer_cmd = 'gdalwarp -cutline '+buffer_path+' -crop_to_cutline '+terminusraster_path+\" \"+cutraster_path\n",
    "    \n",
    "    #RASTERIZE & SUBSET\n",
    "    subprocess.call(rasterize_cmd, shell=True)\n",
    "    subprocess.call(subsetbuffer_cmd, shell=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Download non-cloudy images - PARALLEL\n",
    "\n",
    "To threshold out cloudy images, we will find the number of pixels in our shapefile region that correspond to a cumulative pixel value of > __25000__. If the number of pixels with values above this threshold is greater than our % threshold, we won't download the image. \n",
    "\n",
    "__EDITED: We'll also filter out those images along the border that cutoff through your shapefile region. These will have a designated fill value of 1.__\n",
    "\n",
    "Set cloudy and fill thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET cloud cover threshold\n",
    "cpercent_thresh = 20.0\n",
    "\n",
    "#SET fill percent threshold\n",
    "fpercent_thresh = 60.0\n",
    "\n",
    "#SET BQA pixel value threshold\n",
    "BQA_thresh = 25000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set LS bands to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET your desired bands here\n",
    "bands = [8]\n",
    "# bands = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images that pass these thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the dataframe with your path row combinations:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    #grab the path row names and set the folder name\n",
    "    path = row['Path']; zone = row['Zone']; row = row['Row']; BoxID = index; folder_name = 'Path'+path+'_Row'+row\n",
    "    \n",
    "    #set path to path row folders to grab BQA files from\n",
    "    pr_folderpath = outputpath+newfoldername+'/'+folder_name+'/'\n",
    "    \n",
    "    #Output BoxID folderpath and create BoxID folders\n",
    "    bp_out = outputpath+newfoldername+'/Box'+BoxID+'/'\n",
    "    \n",
    "    if os.path.exists(bp_out):\n",
    "        print(\"Box\"+BoxID, \" EXISTS ALREADY. SKIP.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out)\n",
    "        print(\"Box\"+BoxID+\" directory made\")\n",
    "                      \n",
    "    #set path to the buffer zones for final subsetting:\n",
    "    pathtobuffer = basepath+'Box'+BoxID+'/Buffer'+BoxID\n",
    "#     pathtoshp_rp = basepath+'Box'+BoxID+'/Box'+BoxID+'_UTM_'+zone+'.shp'\n",
    "    \n",
    "    #REPROJECT BUFFER SHAPE\n",
    "    if len(os.listdir(pr_folderpath)) > 0:\n",
    "#         print(zone)\n",
    "        rp_shp = 'ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326'+zone+' -s_srs EPSG:3413 '+pathtobuffer+\"_UTM_\"+zone+\".shp \"+pathtobuffer+\".shp\"\n",
    "#         print(rp_shp)\n",
    "        subprocess.call(rp_shp, shell=True)\n",
    "\n",
    "    #loop through all the BQA files in the path_row folders:\n",
    "    for scene in os.listdir(pr_folderpath):\n",
    "        if len(os.listdir(pr_folderpath)) > 0 and scene.endswith(\"LGN\", -5, -2):\n",
    "            #grab all the BQA files to calculate the percent cloud\n",
    "            BQApath = pr_folderpath+scene+\"/\"+scene+'_BQA_Box'+BoxID+'.TIF'\n",
    "#             print(BQApath)\n",
    "            \n",
    "            #read in your BQA subset raster as a numpy array:\n",
    "            subsetBQA = mpimg.imread(BQApath)\n",
    "            \n",
    "            #CALCULATE % CLOUD COVER USING YOUR BQA SUBSET\n",
    "            #first, calculate the total number of pixels\n",
    "            totalpixels = subsetBQA.shape[0]*subsetBQA.shape[1]\n",
    "            \n",
    "            #then, calculate the number of cloud pixels:\n",
    "            #mask for pixels that have a value > than your threshold value\n",
    "            cloudBQA = subsetBQA[subsetBQA > BQA_thresh]\n",
    "            \n",
    "            #calculate number of fill pixels (fill value of 1. Will be less than 2.0)\n",
    "            fillBQA = subsetBQA[subsetBQA < 2.0]\n",
    "            \n",
    "            #and count those cloudpixels and fillpixels:\n",
    "            cloudpixels = len(cloudBQA); fillpixels = len(fillBQA)\n",
    "            \n",
    "            #divide this by the totalpixels and multiply by 100\n",
    "            cloudpercent = int(float(cloudpixels)/float(totalpixels)*100)\n",
    "            fillpercent = int(float(fillpixels)/float(totalpixels)*100)\n",
    "            \n",
    "            #check that the values are sensible\n",
    "            print(scene, 'Cloud % ', cloudpercent, 'Fill %', fillpercent)\n",
    "            \n",
    "            #set path to rp buffer:\n",
    "            pathtobuffer_rp = pathtobuffer+\"_UTM_\"+zone+\".shp\"\n",
    "            \n",
    "            #IF % CLOUD COVER IS LESS THAN OR EQUAL TO YOUR THRESHOLD DOWNLOAD THE BANDS FOR THAT SCENE:\n",
    "            if cloudpercent <= cpercent_thresh and fillpercent <= fpercent_thresh:\n",
    "                #download the bands for that scene into your scene folders:\n",
    "                for band in bands:\n",
    "                    #make sure the band is in string format:\n",
    "                    band = str(band)\n",
    "                    #set input path to your bands in AWS:\n",
    "                    pathin = '/vsicurl/https://landsat-pds.s3.amazonaws.com/L8/'+path+\"/\"+row+\"/\"+scene+\"/\"+scene+\"_B\"+band+\".TIF\"\n",
    "                    #set your output path to the overall BoxID folder, not the scene folders\n",
    "                    pathout = outputpath+newfoldername+'/Box'+BoxID+'/'+scene+\"_B\"+band+'_Buffer'+BoxID+'.TIF'\n",
    "                    \n",
    "                    #set download command and check:\n",
    "                    download_cmd = 'gdalwarp -cutline '+pathtobuffer_rp+' -crop_to_cutline '+pathin+' '+pathout\n",
    "#                     download_cmd = 'gdalwarp -cutline '+pathtoshp_rp+' -crop_to_cutline '+pathin+' '+pathout\n",
    "                    print(download_cmd)\n",
    "                    \n",
    "                    #When you're ready, comment out the above print command and\n",
    "                    #uncomment the following to commence the download:                   \n",
    "                    subprocess.call(download_cmd, shell=True)\n",
    "                    print(scene+\"_B\"+band+'_Buffer'+BoxID+\".TIF downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject newly downloaded images into Greenland Polar Stereo projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the dataframe with your path row combinations:\n",
    "for index, row in pr_df_multi.iterrows():\n",
    "    #grab the path row names and set the folder name\n",
    "    path = row['Path']; row = row['Row']; BoxID = index\n",
    "    \n",
    "    #Output BoxID path \n",
    "    bp_out = outputpath+newfoldername+'/Box'+BoxID+'/'\n",
    "    \n",
    "    #create output folder to reproject into\n",
    "    if os.path.exists(bp_out+'reprojected/'):\n",
    "        print(\"Box\"+BoxID, \"Reprojected EXISTS ALREADY. SKIP.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out+'reprojected/')\n",
    "        print(\"Box\"+BoxID+\" Reprojected directory made\")\n",
    "                      \n",
    "    #grab all downloaded images and reproject:\n",
    "    downloadedimages = os.listdir(bp_out)\n",
    "    for image in downloadedimages:\n",
    "        imagename = image[:21]\n",
    "#         print(imagename)\n",
    "        rp_cmd = \"gdalwarp -t_srs '+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m' \"+bp_out+image+\" \"+bp_out+'reprojected/'+imagename+\"_B8_PS_Buffer\"+BoxID+\".TIF\"\n",
    "#         print(rp_cmd)\n",
    "        subprocess.call(rp_cmd, shell=True)   \n",
    "    print(\"Box\"+BoxID+\" reprojected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the image acquisition dates from the metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#create a list of datetime objects for the images:\n",
    "datetimes = []; \n",
    "scenes_dated = []\n",
    "\n",
    "#Loop through the dataframe with your path row combinations:\n",
    "# for index, row in pr_df_multi.iterrows():\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    path = row['Path']; row = row['Row']; BoxID = index; folder_name = 'Path'+path+'_Row'+row; print(folder_name)\n",
    "    \n",
    "    #Output folder paths\n",
    "    folderpath = outputpath+newfoldername+'/'+folder_name+'/'\n",
    "    bp_out = outputpath+newfoldername+'/Box'+BoxID+'/reprojected/'\n",
    "    \n",
    "    #keep track of the number of scenes:\n",
    "    scenecount = 0\n",
    "    \n",
    "    #list of files in directory where you downloaded images\n",
    "    downloaded_scenes = os.listdir(bp_out)\n",
    "    for scene in downloaded_scenes:\n",
    "        scenename = scene[:21]\n",
    "        #if it's a downloaded image and in the folder we are working on:\n",
    "        if scene.endswith('.TIF') and scenename in os.listdir(folderpath):\n",
    "            #go into the folder and open the metadata file:\n",
    "            scenefiles = os.listdir(folderpath+scenename+'/')\n",
    "            \n",
    "            for file in scenefiles:\n",
    "                if (\"MTL.txt\" in file):\n",
    "                    mdata = open(folderpath+scenename+\"/\"+scenename+\"_MTL.txt\", \"r\")\n",
    "                    #loop through each line in metadata to find the date (and time) of acquisition\n",
    "                    for line in mdata:\n",
    "                        variable = line.split(\"=\")[0]\n",
    "                        if (\"DATE_ACQUIRED\" in variable):\n",
    "                            date = line.split(\"=\")[1][1:-1]\n",
    "\n",
    "                    #save the date as a datetime object and append to list\n",
    "                    dates = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                    datetimes.append(dates); scenes_dated.append(scenename)\n",
    "            scenecount = scenecount+1\n",
    "                                       \n",
    "# Store in a dataframe\n",
    "datetime_df = pd.DataFrame(list(zip(scenes_dated, datetimes)), columns=['Scene', 'datetime'])\n",
    "datetime_df = datetime_df.sort_values(by='datetime', ascending=True); datetime_df = datetime_df.drop_duplicates()\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write image dates to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATES_NAME = 'imgdates_SE_2.csv'\n",
    "datetime_df.to_csv(path_or_buf = basepath+DATES_NAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Calculate weighted average flow direction for each glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes ice velocity rasters to determine each glacier of interest's weighted average flow direction. The rasters are subset using the terminus box shapefile using a GDAL command (**gdalwarp**) with the following syntax:\n",
    "\n",
    "    gdalwarp -cutline path_to_terminusbox.shp -crop_to_cutline path_to_input_velocity.TIF path_to_output_velocity_at_term###.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for BoxID in BoxIDs:\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"  #set paths to shapefiles\n",
    "#     for vdate in ['2014_15', '2015_16', '2016_17']:\n",
    "    if True == True:\n",
    "#         vdir = 'measures_velocity_dir_degree.tif'; vmag = 'greenland_vel_mosaic250_mag.tif'\n",
    "        vx = 'greenland_vel_mosaic250_vx_v1.tif';vy = 'greenland_vel_mosaic250_vy_v1.tif'\n",
    "        #set input paths\n",
    "#         vdir_in = basepath+vdir; vmag_in = basepath+vmag\n",
    "        vx_in = basepath+vx; vy_in = basepath+vy\n",
    "    \n",
    "        #SET output paths\n",
    "#         vdir_out = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+'_'+vdir; vmag_out = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+'_'+vmag\n",
    "        vx_out = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vx; vy_out = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vy; \n",
    "    \n",
    "        #SET velocity subset commands and print to check it\n",
    "        v_subset1 = 'gdalwarp -cutline '+terminusbox_path+' -crop_to_cutline '+vx_in+\" \"+vx_out\n",
    "        v_subset2 = 'gdalwarp -cutline '+terminusbox_path+' -crop_to_cutline '+vy_in+\" \"+vy_out\n",
    "#         print(v_subset_dir_cmd); print(v_subset_mag_cmd)\n",
    "\n",
    "        #SUBSET velocity rasters\n",
    "        subprocess.call(v_subset1, shell=True)\n",
    "        subprocess.call(v_subset2, shell=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID+' done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, these subset velocity rasters are opened using the **rasterio** package and read into arrays. They are filtered for anomalous values and the velocity magnitudes are converted into weights. Then the **numpy.average()** function is used to calculated the weighted average flow directions where the flow directions of the pixels where the highest velocities are found are weighted more. \n",
    "\n",
    "The resulting average flow direction will be representative of the glacier's main flow. These directions will be used to rotate the images of the glaciers so that their flow is due right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CREATE list of glacier average flow directions:\n",
    "boxes = []; avg_rot = []; max_mag = []; num_cells = []\n",
    "\n",
    "for BoxID in BOXIDS :\n",
    "    rot_angles = []; max_magnitudes = []\n",
    "    \n",
    "#     for vdate in ['2014_15', '2015_16', '2016_17']:\n",
    "    if True == True:\n",
    "        #READ velocity direction and magnitude data at terminus for each glacier into an array\n",
    "        vx_name = 'greenland_vel_mosaic250_vx_v1.tif';vy_name = 'greenland_vel_mosaic250_vy_v1.tif'\n",
    "        vx = rasterio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vx_name, \"r\")\n",
    "        vy = rasterio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vy_name, \"r\") \n",
    "        vx_array = vx.read(); vy_array = vy.read()\n",
    "        #remove no data values (-2000000000.0)\n",
    "        vx_masked = vx_array[vx_array != -2000000000.0]; vy_masked = vy_array[vy_array != -2000000000.0]\n",
    "        \n",
    "        #CALCULATE FLOW DIRECTION\n",
    "        direction = np.arctan2(vy_masked, vx_masked)*180/np.pi; \n",
    "#         print(BoxID, direction.max(), direction.min())\n",
    "        #transform so any negative angles are placed on 0 to 360 scale:\n",
    "        if len(direction[direction < 0]) > 0:\n",
    "            direction[direction < 0] = 360.0+direction[direction < 0]\n",
    "#             print(BoxID, direction.max(), direction.min())\n",
    "        \n",
    "        #CALCULATE SPEED\n",
    "        magnitude = np.sqrt((vx_masked*vx_masked) + (vy_masked*vy_masked))\n",
    "        \n",
    "        if len(direction) > 0: # if there are velocity values remaining\n",
    "            #CALCULATE the weighted average rotation angle\n",
    "            #calculate weights (0 - 1) from magnitudes\n",
    "            mag_range = magnitude.max() - magnitude.min()\n",
    "            stretch = 1/mag_range\n",
    "            weights = stretch*(magnitude - magnitude.min())\n",
    "    #         print(weights.min(), weights.max()) #should be between 0 and 1\n",
    "    #         print(weights.shape, masked_dir.shape)\n",
    "            avg_dir = np.average(direction, weights=weights)\n",
    "            print(avg_dir)\n",
    "            max_magnitude = magnitude.max()*0.00273973 # conversion to m/d?\n",
    "        else:\n",
    "            avg_dir = np.NaN ; max_magnitude = np.NaN # no velocities to calculate this with\n",
    "\n",
    "        #APPEND to lists:\n",
    "        avg_rot.append(avg_dir); max_mag.append(max_magnitude); boxes.append(BoxID); num_cells.append(len(direction))\n",
    "        \n",
    "velocities_df = pd.DataFrame(list(zip(boxes,avg_rot, max_mag, num_cells)), columns=['BoxID','Flow_dir', 'Max_speed', 'Pixels'])\n",
    "velocities_df = velocities_df.sort_values(by='BoxID')\n",
    "velocities_df = velocities_df.drop_duplicates()\n",
    "velocities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write glacier velocities to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEL_NAME = 'Glacier_vel_measures_SE_2.csv'\n",
    "velocities_df.to_csv(path_or_buf = basepath+VEL_NAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Rotate all images by flow direction\n",
    "\n",
    "Read in the glacier velocity file as velocities_df if not already loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities_df = pd.read_csv(basepath+'Glacier_vel_measures_SE.csv', sep=',', dtype=str)\n",
    "# velocities_df = pd.read_csv(basepath+'Glacier_velocities.csv', sep=',', dtype=str, usecols=[1,2,3])\n",
    "velocities_df = velocities_df.set_index('BoxID')\n",
    "# velocites_df = velocities_df.dropna()\n",
    "velocities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory for rotated images in BoxID folders if it doesn't already exist\n",
    "for BoxID in BOXIDS:\n",
    "    if os.path.exists(downloadpath+\"Box\"+BoxID+'/rotated/'):\n",
    "        print(\"Already exists.\")\n",
    "        #OTHERWISE, create the folder and download into it\n",
    "    else:\n",
    "        os.mkdir(downloadpath+\"Box\"+BoxID+'/rotated/')\n",
    "        print(\"Folder made for Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move rasterized terminus box into reprojected folder:\n",
    "for BoxID in BOXIDS:\n",
    "    boxfile = 'Box'+BoxID+'_raster_cut.TIF'\n",
    "    boxrasterpath = basepath+'Box'+BoxID+'/'+boxfile\n",
    "    newpath = downloadpath+'Box'+BoxID+'/reprojected/'+boxfile\n",
    "    shutil.copyfile(boxrasterpath, newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all files in reprojected folder to png from TIF\n",
    "for BoxID in BOXIDS:\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/reprojected/; '+'mogrify -format png *.TIF'\n",
    "#     print(command)\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROTATE THE IMAGES IF VELOCITIES ARE GOOD\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    print(BoxID) \n",
    "    #for each file in the reprojected folder:\n",
    "    for file in os.listdir(downloadpath+\"Box\"+BoxID+'/reprojected/'):\n",
    "        if file.endswith('.png'):\n",
    "            img  = Image.open(downloadpath+\"Box\"+BoxID+'/reprojected/'+file)\n",
    "            rotated = img.rotate(-float(row['Flow_dir']))\n",
    "            rotated.save(downloadpath+\"Box\"+BoxID+'/rotated/R_'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Resize all images to the same size (the minimum image size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Make sure folders are removed\n",
    "# for BoxID in BOXIDS:\n",
    "#     if os.path.exists(downloadpath+\"Box\"+BoxID+'/rotated/'):\n",
    "#         shutil.rmtree(downloadpath+\"Box\"+BoxID+'/rotated/', ignore_errors=False, onerror=None)\n",
    "#     else:\n",
    "#         print(\"Resized folder already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in velocities_df.iterrows():\n",
    "# for BoxID in ['279']:\n",
    "    BoxID = index\n",
    "    print(BoxID)\n",
    "    dimensions_x = []; dimensions_y = []\n",
    "    images = os.listdir(downloadpath+\"Box\"+BoxID+'/rotated/')\n",
    "    for image in images:\n",
    "        if image.endswith('.png'):\n",
    "            img = mpimg.imread(downloadpath+\"Box\"+BoxID+'/rotated/'+image)\n",
    "            dimensions_x.append(img.shape[1]); dimensions_y.append(img.shape[0])\n",
    "\n",
    "    #find minimum dimensions\n",
    "    min_y = np.min(dimensions_y); min_x = np.min(dimensions_x)\n",
    "    index_y = dimensions_y.index(min_y); index_x = dimensions_x.index(min_x)\n",
    "          \n",
    "    if index_x != index_y:\n",
    "        print('Something is funky with the image dimesions for Box'+BoxID)\n",
    "    else:\n",
    "        crop_y = dimensions_y[index_y]; crop_x = dimensions_x[index_y]\n",
    "\n",
    "        #crop each image if the dimensions are larger than the minimum\n",
    "        for image in images:\n",
    "            if image.endswith('.png'):\n",
    "                img = mpimg.imread(downloadpath+\"Box\"+BoxID+'/rotated/'+image)\n",
    "                if img.shape[1] > crop_x or img.shape[0] > crop_y:\n",
    "                    #calculate difference, and divide by 2 to get amount of rows to remove by\n",
    "                    diffx_half = (img.shape[1] - crop_x)/2; diffy_half = (img.shape[0] - crop_y)/2\n",
    "\n",
    "                    #if the difference is a half pixel, make sure to remove the full value from the first side only\n",
    "                    if int(diffx_half) != diffx_half:\n",
    "                        #remember for image slicing y is the first dimension, x is the second\n",
    "                        img_cropx = img[:, int(diffx_half):-int(diffx_half)-1]\n",
    "                    #otherwise remove it from both sides:\n",
    "                    else:\n",
    "                        img_cropx = img[:, int(diffx_half):-int(diffx_half)]\n",
    "\n",
    "                    #same for y\n",
    "                    if int(diffy_half) != diffy_half:   \n",
    "                        img_cropy = img_cropx[int(diffy_half):-int(diffy_half)-1, :]\n",
    "                    #otherwise remove it from both sides:\n",
    "                    else:\n",
    "                        img_cropy = img_cropx[int(diffy_half):-int(diffy_half), :]\n",
    "\n",
    "                    #save over original images\n",
    "                    scipy.misc.imsave(downloadpath+\"Box\"+BoxID+'/rotated/'+image, img_cropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert these final images to pgm for Xsmurf analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all final files to pgm\n",
    "for index, row in velocities_df.iterrows():\n",
    "# for BoxID in ['279']:\n",
    "    BoxID = index\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/rotated/; '+'mogrify -format pgm *.png'\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the rasterized terminus box files if necessary\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    files = os.listdir(downloadpath+'Box'+BoxID+'/rotated/')\n",
    "    for file in files:\n",
    "        if file.startswith('R_Box'+BoxID+'_cut'):\n",
    "            rpath = downloadpath+'Box'+BoxID+'/rotated/'\n",
    "            os.rename(rpath+file, rpath+'R_Box'+BoxID+'_raster_cut'+file[-4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Grab fraction of total images available that were cloudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxIDs = []; im_tots = []; downloaded = []; fractions = [];\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index # grab the BoxID\n",
    "    pathrows_BoxID = pathrows_df[pathrows_df.index == int(BoxID)].copy() # grab path rows for that BoxID\n",
    "    \n",
    "    im_tot = 0 # count number of total images\n",
    "    for idx, rw in pathrows_BoxID.iterrows():\n",
    "        p = rw['Path']; r = rw['Row']\n",
    "        ims_pr = len(os.listdir(downloadpath+'Path'+p+'_Row'+r)) # grab number of scenes in that pathrow\n",
    "        im_tot = im_tot + ims_pr\n",
    "        \n",
    "    # find number downloaded (not cloudy) from the BoxID folder\n",
    "    counter = 0\n",
    "    for file in os.listdir(downloadpath+'Box'+BoxID):\n",
    "        if file.startswith('LC') and file.endswith('.TIF') and 'B8' in file:\n",
    "            counter = counter + 1\n",
    "            \n",
    "    download_frac = int(counter/im_tot*100) # grab fraction downloaded\n",
    "#     print(BoxID, im_tot, counter, download_frac)\n",
    "    BoxIDs.append(BoxID); im_tots.append(im_tot); downloaded.append(counter); fractions.append(download_frac)\n",
    "    \n",
    "downloaded_df = pd.DataFrame(list(zip(BoxIDs, im_tots, downloaded, fractions)), columns = ['BoxID', 'Total_ims', 'Downloaded', '%'])\n",
    "# downloaded_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write information on number of cloudy images to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUDY_NAME = 'Images_downloaded_SE.csv'\n",
    "downloaded_df.to_csv(basepath+CLOUDY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're ready for 2D WTMM analysis!\n",
    "\n",
    "# 11) Run WTMM analysis in Xsmurf through .tcl scripts\n",
    "\n",
    "Pull in the input BoxIDs from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11A) Run WTMM on 1 CPU with BoxIDs as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downloaded_df = pd.read_csv(basepath+\"Images_downloaded_SE.csv\", usecols=[1,2,3])\n",
    "downloaded_df = downloaded_df.set_index('BoxID')\n",
    "print(downloaded_df.shape)\n",
    "downloaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded_df = downloaded_df[downloaded_df.index >= 279].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXIDS = list(map(str, list(downloaded_df.index)))\n",
    "# inputIDs = \" \".join(BOXIDS)\n",
    "# scr_gaussian = '/home/akhalil/src/xsmurf-2.7/main/xsmurf -nodisplay /home/jukes/Documents/Scripts/scr_gaussian.tcl '+inputIDs\n",
    "# print(scr_gaussian)\n",
    "# subprocess.call(scr_gaussian, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11B) Run WTMM with multiple CPUs\n",
    "\n",
    "Check that this works using Box 214 and 279..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_CPUs = 11\n",
    "\n",
    "for index, row in downloaded_df.iterrows():\n",
    "    BoxID = str(index); print(\"Box\", BoxID)\n",
    "    num_images = row['Downloaded']\n",
    "    num_batches = math.ceil(num_images/num_CPUs)\n",
    "    num_lastbatch = num_CPUs - (num_batches*num_CPUs - num_images)\n",
    "    \n",
    "    imagelist = []\n",
    "    for file in os.listdir(downloadpath+'Box'+BoxID+'/rotated/'):\n",
    "        if \"Buffer\" in file and file.endswith(\".pgm\"):\n",
    "            imagelist.append(file)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(1, num_batches+1):\n",
    "        if i < num_batches:\n",
    "            print(\"Batch\", i)\n",
    "            PIDs = [] # create list to hold the PIDs\n",
    "            for j in range(1, num_CPUs+1):\n",
    "                image = imagelist[counter]\n",
    "                scr = '/home/akhalil/src/xsmurf-2.7/main/xsmurf -nodisplay /home/jukes/Documents/Scripts/scr_gaussian_image.tcl '+BoxID+' '+image+' &'\n",
    "                out = subprocess.Popen(scr, shell=True)\n",
    "                PID = out.pid+1; PIDs.append(PID) # save the PID\n",
    "                print(\"CPU\", j, ':', image)                \n",
    "                counter = counter+1\n",
    "                \n",
    "            for j in range(1, num_CPUs+1):\n",
    "                if psutil.pid_exists(PIDs[j-1]):\n",
    "                    wait = 'wait '+str(PIDs[j-1])\n",
    "                    subprocess.call(wait, shell=True)\n",
    "                    print(wait)\n",
    "        else:\n",
    "            print(\"Batch\", i)\n",
    "            PIDs = [] # create list to hold the PIDs\n",
    "            for k in range(1, num_lastbatch+1):\n",
    "                image = imagelist[counter]\n",
    "                scr = '/home/akhalil/src/xsmurf-2.7/main/xsmurf -nodisplay /home/jukes/Documents/Scripts/scr_gaussian_image.tcl '+BoxID+' '+image+' &'\n",
    "                out = subprocess.Popen(scr, shell=True)\n",
    "                PID = out.pid+1; PIDs.append(PID) # save the PID\n",
    "                print(\"CPU\", k, ':', image)\n",
    "                counter = counter+1\n",
    "            for k in range(1, num_lastbatch+1):\n",
    "                if psutil.pid_exists(PIDs[k-1]):\n",
    "                    wait = 'wait '+str(PIDs[k-1])\n",
    "                    subprocess.call(wait, shell=True)\n",
    "                    print(wait)\n",
    "        print(counter, \"images analyzed\")\n",
    "        \n",
    "# # after it's done, kill all the remaining PIDs to give the computer a break\n",
    "# for PID in PIDs:\n",
    "#     kill = 'kill '+str(PID)\n",
    "#     subprocess.call(kill, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Pick the terminis chains using terminus_pick.tcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOXIDS = list(map(str, list(downloaded_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputIDs = \" \".join(BOXIDS)\n",
    "print(inputIDs)\n",
    "order = '_MSA'\n",
    "size_thresh = 0.71\n",
    "mod_thresh = 0.7\n",
    "arg_thresh = 0.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminus_pick = '/home/akhalil/src/xsmurf-2.7/main/xsmurf -nodisplay /home/jukes/Documents/Scripts/terminus_pick'+str(order)+'.tcl '+str(size_thresh)+' '+str(mod_thresh)+' '+str(arg_thresh)+' '+str(inputIDs)\n",
    "# print(terminus_pick)\n",
    "subprocess.call(terminus_pick, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13) Analyze results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(basepath+'imgdates_SE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE FUNCTION\n",
    "os.chdir('/home/jukes/automated-glacier-terminus')\n",
    "from automated_terminus_functions import results_allglaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results_allglaciers(3, 1, 1) # try 5, 3, 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
