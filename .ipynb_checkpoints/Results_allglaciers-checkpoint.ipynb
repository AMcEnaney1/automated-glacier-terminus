{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process terminus delineation results for all glaciers in one batch of analysis\n",
    "\n",
    "By Jukes Liu. _Last modified 02-15-2020._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd    \n",
    "import scipy.stats\n",
    "import datetime\n",
    "import math\n",
    "import shutil\n",
    "import subprocess\n",
    "os.chdir('/home/jukes/automated-glacier-terminus')\n",
    "from automated_terminus_functions import calc_changerates1, to_datetimes, within, remove_dips, remove_jumps\n",
    "\n",
    "csvpaths = '/home/jukes/Documents/Sample_glaciers/'\n",
    "basepath = '/media/jukes/jukes1/LS8aws/'\n",
    "massorsize = \"mass\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 2)\n"
     ]
    }
   ],
   "source": [
    "#IMAGE DATES\n",
    "datetime_df = pd.read_csv(csvpaths+'imgdates.csv', sep=',', dtype=str, header=0, names=['Scene', 'datetimes'])\n",
    "print(datetime_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "# analysis_date = str(datetime.datetime.now())[0:10]\n",
    "# analysis_date.replace('-', '_'); print(analysis_date)\n",
    "analysis_date = '2020_04_20'\n",
    "#DELINEATION METRIC AND ORDER \n",
    "for file in os.listdir(csvpaths):\n",
    "    if analysis_date in file and file.endswith('.csv'):\n",
    "        print('found'); thefile = file\n",
    "order_df = pd.read_csv(csvpaths+thefile, sep=',', dtype=str, header=1, usecols=[0,1,2,3,4])\n",
    "order_df = order_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [BoxID, Scene, Scale, Metric, Order]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTERLINE INFO\n",
    "centerline_df = pd.read_csv(csvpaths+'Boxes_coords_pathrows.csv', sep=',', dtype=str, header=0)\n",
    "centerline_df = centerline_df.set_index('BoxID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLACIER VELOCITIES\n",
    "flowspeed_df= pd.read_csv(csvpaths+'Glacier_vel_measures_sample10.csv', sep=',', dtype=str)\n",
    "flowspeed_df = flowspeed_df.set_index('BoxID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab BoxIDs of all glaciers analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxIDs = list(set(order_df.BoxID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run analysis of results for each glacier in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for BOI in BoxIDs:\n",
    "    print(\"Box\"+BOI)\n",
    "    metric = \"Datfiles/\"; imagepath = basepath+\"Box\"+BOI+\"/rotated/\"\n",
    "    \n",
    "    order_box_df = order_df[order_df[\"BoxID\"]==BOI].copy()\n",
    "    order_box_df = order_box_df.drop('BoxID', axis=1)\n",
    "    order_box_df = order_box_df.dropna()\n",
    "    \n",
    "    #GRAB INFO FROM IMAGE FILES\n",
    "    image_arrays = []; dats = []; trimdats = []; imgnames = []; boxids = []; scales = []\n",
    "    imgfiles = os.listdir(imagepath)\n",
    "    for imgfile in imgfiles:\n",
    "        #grab image files and append to images list\n",
    "        if imgfile.endswith(BOI+\".png\"):\n",
    "            image = mpimg.imread(imagepath+imgfile); imgname = imgfile[0:-4]; scenename = imgname[2:23]\n",
    "            pathtodat = imagepath+imgname+\".pgm_max_gaussian/\"+metric\n",
    "            datfiles = os.listdir(pathtodat)\n",
    "            #if there are datfiles, grab the trimmed and non-trimmed files\n",
    "            if len(datfiles) > 1: \n",
    "                #find the trimmed dat file and the original\n",
    "                for dat in datfiles:\n",
    "                    if \"trim\" in dat:\n",
    "                        datfile_trim = dat\n",
    "                        #append to trimmed dats list\n",
    "                        trimdats.append(datfile_trim)\n",
    "                        #grab the scale and append the equivalent original dat\n",
    "                        scale = dat[-7:-4]\n",
    "                        datfile = \"terminus_\"+scale+\".dat\"\n",
    "                        dats.append(datfile)\n",
    "                        #append the image array and the image name to the list\n",
    "                        image_arrays.append(image); imgnames.append(scenename); boxids.append(BOI); scales.append(scale)\n",
    "    images_df = pd.DataFrame(list(zip(imgnames, boxids, image_arrays, dats, trimdats, scales)),\n",
    "                  columns=['Scene','BoxID','Image_array', 'Dat_filename', \"Trimmed_dat_filename\", \"Scale\"])\n",
    "    images_df.sort_values(by='Scene'); datetime_df = datetime_df.sort_values(by='Scene')\n",
    "    \n",
    "    #MERGE IMAGE INFO WITH IMAGEDATES AND MERGE WITH ORDER\n",
    "    new_df = images_df.merge(datetime_df, how= 'inner', on = 'Scene')\n",
    "    dated_images_df = new_df.sort_values(by='datetimes', ascending = True)\n",
    "    final_images_df = dated_images_df.merge(order_box_df, how='inner', on=['Scene', 'Scale'])\n",
    "    final_images_df = final_images_df.sort_values(by=['datetimes','Scene','Order'], ascending=True)\n",
    "    \n",
    "    #CALCULATE TERMINUS POSITIONS\n",
    "    #LOAD IN REFERENCE POINTS to calculate terminus position with respect to\n",
    "    box_midpoint_x = np.float(centerline_df.loc[BOI, 'lmid50_x']); box_midpoint_y = np.float(centerline_df.loc[BOI, 'lmid50_y'])\n",
    "    boxmid_x_25 = np.float(centerline_df.loc[BOI, 'lmid25_x']); boxmid_y_25 = np.float(centerline_df.loc[BOI, 'lmid25_y'])\n",
    "    boxmid_x_75 = np.float(centerline_df.loc[BOI, 'lmid75_x']); boxmid_y_75 = np.float(centerline_df.loc[BOI, 'lmid75_y'])\n",
    "\n",
    "    #GRAB CENTERLINE POINTS\n",
    "    #grab slopes and intercepts from the dataframe\n",
    "    c_slope = float(centerline_df.loc[BOI]['m50']); c_intercept = float(centerline_df.loc[BOI]['b50']) \n",
    "    c25_slope = float(centerline_df.loc[BOI]['m25']); c25_intercept = float(centerline_df.loc[BOI]['b25'])\n",
    "    c75_slope = float(centerline_df.loc[BOI]['m75']); c75_intercept = float(centerline_df.loc[BOI]['b75'])  \n",
    "\n",
    "    #grab range of x-values\n",
    "    xmin50 = float(box_midpoint_x); xmax50 = float(centerline_df.loc[BOI, 'rmid50_x']); ymid50 = float(box_midpoint_y)\n",
    "    xmin25 = float(boxmid_x_25); xmax25 = float(centerline_df.loc[BOI, 'rmid25_x']); ymid25 = float(boxmid_y_25)\n",
    "    xmin75 = float(boxmid_x_75); xmax75 = float(centerline_df.loc[BOI, 'lmid75_x']); ymid75 = float(boxmid_y_75)\n",
    "    xmax = np.max([xmax50, xmax25, xmax75]); xmin = np.min([xmin50, xmin25, xmin75]); c_x = np.linspace(xmin, xmax, int(xmax-xmin)*2)\n",
    "\n",
    "    #calculate y-values using the various centerlines\n",
    "    c_y = c_slope*c_x + c_intercept; c_y_25 = c25_slope*c_x + c25_intercept; c_y_75 = c75_slope*c_x + c75_intercept\n",
    "\n",
    "    #LISTS TO HOLD TERMINUS POSITIONS AND INTERSECTION POINTS\n",
    "    terminus_positions = []; tpositions_25 = []; tpositions_75 = []\n",
    "    intersections = []; X25 = []; X75 = []\n",
    "\n",
    "    #for each scene and scale:\n",
    "    for index, row in final_images_df.iterrows():\n",
    "        trimdat = row['Trimmed_dat_filename']; dat = row['Dat_filename']; scene = row['Scene']    \n",
    "        #CALCULATE TERMINUS POSITION\n",
    "        #load in dat files and calculate intersection points\n",
    "        datpath = imagepath+\"R_\"+scene+\"_B8_PS_Buffer\"+BOI+\".pgm_max_gaussian/\"+metric\n",
    "    #     term_trimdat = np.loadtxt(datpath+trimdat)\n",
    "        term_dat = np.loadtxt(datpath+dat)                          \n",
    "        intersect_xs = []; intersect_xs_25 = []; intersect_xs_75 = []\n",
    "        intersect_ys = []; intersect_ys_25 = []; intersect_ys_75 = []\n",
    "\n",
    "        #loop through all the x,y values for the centerline\n",
    "        for j in range(0, len(c_x)):\n",
    "            x = c_x[j]; y = c_y[j]; y25 = c_y_25[j]; y75 = c_y_75[j]        \n",
    "            interval = 0.6\n",
    "            #where are the intersections with the terminus pick?\n",
    "    #         for dat_x, dat_y in term_trimdat:\n",
    "            for dat_x, dat_y in term_dat:\n",
    "                #midway centerline\n",
    "                if within(dat_x, x, interval) and within (dat_y, y, interval):\n",
    "                    #intersect_x = dat_x; intersect_y = dat_y; intersect_found = True\n",
    "                    intersect_xs.append(dat_x); intersect_ys.append(dat_y)            \n",
    "                #1/4th centerline\n",
    "                if within(dat_x, x, interval) and within (dat_y, y25, interval):\n",
    "                    intersect_xs_25.append(dat_x); intersect_ys_25.append(dat_y)              \n",
    "                #3/4th centerline\n",
    "                if within(dat_x, x, interval) and within (dat_y, y75, interval):\n",
    "                    intersect_xs_75.append(dat_x); intersect_ys_75.append(dat_y)\n",
    "        #for 50 centerline\n",
    "        #if no intersections are found with the terminus line, append Nans\n",
    "        if len(intersect_xs) == 0:\n",
    "            tpos50 = np.NaN; intersect_x = np.NaN; intersect_y = np.NaN\n",
    "        #if at least one is found:\n",
    "        else:\n",
    "            #intersection with the greatest x\n",
    "            #use distance formula to calculate distance between\n",
    "            max_index = intersect_xs.index(np.max(intersect_xs))\n",
    "            intersect_x = intersect_xs[max_index]; intersect_y = intersect_ys[max_index]\n",
    "    #         term_position = distance(xmin50, ymid50, intersect_x, intersect_y)*15.0\n",
    "            tpos50 = (intersect_x-xmin50)*15.0\n",
    "    #         print(tpos50)\n",
    "\n",
    "        #for 25 centerline\n",
    "        if len(intersect_xs_25) == 0:\n",
    "            tpos25 = np.NaN; intersect_x25 = np.NaN; intersect_y25 = np.NaN\n",
    "        else:\n",
    "            max_index_25 = intersect_xs_25.index(np.max(intersect_xs_25))\n",
    "            intersect_x25 = intersect_xs_25[max_index_25]; intersect_y25 = intersect_ys_25[max_index_25]\n",
    "            tpos25 = (intersect_x25-xmin25)*15.0\n",
    "    #         tpos25 = distance(xmin25, ymid25, intersect_x25, intersect_y25)*15.0\n",
    "\n",
    "        #for 75 centerline\n",
    "        if len(intersect_xs_75) == 0:\n",
    "            tpos75 = np.NaN; intersect_x75 = np.NaN; intersect_y75 = np.NaN\n",
    "        else:\n",
    "            max_index_75 = intersect_xs_75.index(np.max(intersect_xs_75))\n",
    "            intersect_x75 = intersect_xs_75[max_index_75]; intersect_y75 = intersect_ys_75[max_index_75]\n",
    "            tpos75 = (intersect_x75-xmin75)*15.0\n",
    "    #         tpos75 = distance(xmin75, ymid75, intersect_x75, intersect_y75)*15.0\n",
    "\n",
    "        #append to lists\n",
    "        terminus_positions.append(tpos50); tpositions_25.append(tpos25); tpositions_75.append(tpos75)\n",
    "        intersections.append([intersect_x, intersect_y]); X25.append([intersect_x25, intersect_y25]); X75.append([intersect_x75, intersect_y75])\n",
    "\n",
    "    # ADD TERMINUS POSITION AND INTERSECTIONS\n",
    "    final_images_df['tpos50'] = terminus_positions; final_images_df['tpos25'] = tpositions_25; final_images_df['tpos75'] = tpositions_75\n",
    "    final_images_df['X50'] = intersections ;final_images_df['X25'] = X25; final_images_df['X75'] = X75\n",
    "    \n",
    "    #SPLIT INTO 3 DATAFRAMES FOR 3 FLOWLINES:\n",
    "    final_images_50 = final_images_df[['Scene', 'BoxID', 'Scale', 'datetimes', 'Metric', 'Order', \n",
    "                                      'tpos50', 'X50',]].copy().reset_index(drop=True)\n",
    "    final_images_50 = final_images_50.rename(columns={\"tpos50\": \"tpos\", \"X50\": \"X\"})\n",
    "    final_images_25 = final_images_df[['Scene', 'BoxID', 'Scale', 'datetimes', 'Metric', 'Order', \n",
    "                                      'tpos25', 'X25']].copy().reset_index(drop=True)\n",
    "    final_images_25 = final_images_25.rename(columns={\"tpos25\": \"tpos\", \"X25\": \"X\"})\n",
    "    final_images_75 = final_images_df[['Scene', 'BoxID', 'Scale', 'datetimes', 'Metric', 'Order', \n",
    "                                      'tpos75', 'X75']].copy().reset_index(drop=True)\n",
    "    final_images_75 = final_images_75.rename(columns={\"tpos75\": \"tpos\", \"X75\": \"X\"})\n",
    "    dfs = [final_images_50, final_images_25, final_images_75]\n",
    "    \n",
    "    #CALCULATE TERMINUS CHANGE RATES\n",
    "    dfs_new = []\n",
    "    for df in dfs: \n",
    "        to_datetimes(df); dfs_new.append(calc_changerates1(df))\n",
    "        \n",
    "    #FILTER USING 5*MAXIMUM FLOW SPEEDS\n",
    "    max_flow = float(flowspeed_df['Max_speed'][BOI])\n",
    "    if max_flow < 1.0:\n",
    "        flow_thresh = 5.0\n",
    "    else:\n",
    "        flow_thresh = 5.0*max_flow\n",
    "    #remove dips\n",
    "    N1 = 3; nodips = []\n",
    "    for df in dfs_new:\n",
    "        nodips.append(remove_dips(df, flow_thresh, N1))\n",
    "    #remove jumps\n",
    "    N2 = 2; nojumps = []\n",
    "    for df in nodips:\n",
    "        nojumps.append(remove_jumps(df, flow_thresh, N2))\n",
    "        \n",
    "    #GRAB HIGHEST ORDER PICK AFTER FILTERING\n",
    "    highestorder_dfs = []\n",
    "    for df in nojumps:\n",
    "        #grab unique dates\n",
    "        unique_dates = set(list(df['datetimes']))\n",
    "        print(len(unique_dates))\n",
    "        #grab highest orders:\n",
    "        order_list = []\n",
    "        for date in unique_dates:\n",
    "            date_df = df[df['datetimes'] == date].copy()\n",
    "            highestorder = np.min(np.array(date_df['Order']))\n",
    "            order_list.append(highestorder)\n",
    "        highestorder_df = pd.DataFrame(list(zip(unique_dates, order_list)), columns=['datetimes', 'Order']).sort_values(by='datetimes', ascending=True)\n",
    "        highestorder_dfs.append(highestorder_df)\n",
    "    \n",
    "    onepick_dfs = []\n",
    "    for i in range(0, len(highestorder_dfs)):\n",
    "        onepick_df = nojumps[i].merge(highestorder_dfs[i], how='inner', on=['datetimes', 'Order'])\n",
    "        onepick_dfs.append(onepick_df)\n",
    "        print(onepick_df.shape[0])\n",
    "    \n",
    "    #PLOT AND SAVE\n",
    "    fig, ax1 = plt.subplots(figsize=(12,4))\n",
    "    markers = ['mo', 'ro', 'bo']\n",
    "    for j in range(0, len(onepick_dfs)):\n",
    "        df = onepick_dfs[j];    print(len(df))\n",
    "        ax1.plot(df['datetimes'], df['tpos'], markers[j], markersize=5, alpha=0.8)\n",
    "    #general plot parameters\n",
    "    ax1.set_ylabel('Terminus position (m)', color='k', fontsize=12)\n",
    "    ax1.set_title(\"Box\"+BOI, fontsize=16); ax1.set_xlabel('Date', fontsize=12)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    #save figure\n",
    "    plt.savefig(csvpaths+\"/Figures/Termposition_LS8_m_Box\"+BOI+\"_\"+analysis_date+\".png\", dpi=200)\n",
    "    plt.legend(['1/2', '1/4', '3/4'])\n",
    "    plt.show()\n",
    "    \n",
    "    flowlines = ['flowline50', 'flowline25', 'flowline75']\n",
    "    for k in range(0, len(onepick_dfs)):\n",
    "        df = onepick_dfs[k];\n",
    "        df.to_csv(path_or_buf = csvpaths+'Tpos_Box'+BOI+'_'+flowlines[k]+'_filtered.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
