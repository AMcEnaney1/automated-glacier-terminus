{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Landsat 8 image bulk download and preprocessing\n",
    "\n",
    "_Last modified 2022-05-06._\n",
    "\n",
    "This script is run to download Landsat images over the glaciers available through the AWS s3 bucket and process the images prior to the Wavelet Transform Modulus Maxima (WTMM) segmentation analysis that produces the calving front delineations.\n",
    "\n",
    "First, set up your AWS profile following these steps:\n",
    "\n",
    "    aws configure --profile name_of_profile\n",
    "\n",
    "The workflow is streamlined to analyze images for hundreds of glaciers, specifically, the marine-terminating glaciers along the periphery of Greenland. Sections of code that may need to be modified are indicated as below:\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    Code that must be modified.\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "Keep a record of the csv file names generated throughout the preprocessing as many of them will be used later for analysis.\n",
    "\n",
    "__Terminal command line software requirements:__\n",
    "\n",
    " - Follow instructions at https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html to get required __aws__ commands onto your command terminal.\n",
    " - Uses GDAL command line functions. Make sure __gdal__ is installed properly.\n",
    " - Uses image magick command line functions. Download instructions available at https://imagemagick.org/script/download.php.\n",
    " \n",
    "\n",
    "__Outline of steps:__\n",
    "    1. Set-up: import packages, set paths, and enter glaciers IDs\n",
    "    2. Find all the Landsat footprints that overlap the glaciers\n",
    "    3. Download Landsat metadata (*MTL.txt) files from AWS for all overlapping scenes\n",
    "    4. Calculate cloud % over terminus box using Landsat quality band\n",
    "    5. Create buffer zone around terminus boxes and rasterize terminus boxes\n",
    "    6. Download non-cloudy Landsat images from AWS\n",
    "    7. Calculate weighted average glacier flow direction using velocity data\n",
    "    8. Rotate all images by flow direction\n",
    "    9. Crop all images to the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Set-up: import packages, set paths, and enter glaciers IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# geospatial packages\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point, LineString\n",
    "import shapely\n",
    "from matplotlib.pyplot import imshow\n",
    "import rasterio as rio\n",
    "\n",
    "# Enable fiona KML file reading driver\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "\n",
    "# import necessary functions from automated-glacier-terminus.py\n",
    "from automated_terminus_functions import distance, resize_pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change display width if desired\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! aws s3 ls --profile terminusmapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS settings\n",
    "from rasterio.session import AWSSession\n",
    "import pickle\n",
    "import boto3\n",
    "import boto3.session\n",
    "\n",
    "# cred = boto3.Session().get_credentials()\n",
    "# ACCESS_KEY = cred.access_key\n",
    "# SECRET_KEY = cred.secret_key\n",
    "# SESSION_TOKEN = cred.token  ## optional\n",
    "ACCESS_KEY = 'AKIA4CNBTBJH2TTAGWEW'\n",
    "SECRET_KEY = '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'\n",
    "\n",
    "s3client = boto3.client('s3', \n",
    "                        aws_access_key_id = ACCESS_KEY, \n",
    "                        aws_secret_access_key = SECRET_KEY, \n",
    "#                         aws_session_token = SESSION_TOKEN\n",
    "                       )\n",
    "\n",
    "# response = s3client.get_object(Bucket='name_of_your_bucket', Key='path/to_your/file.pkl')\n",
    "# body = response['Body'].read()\n",
    "# data = pickle.loads(body)\n",
    " \n",
    "######################################################################################\n",
    "# path to the collection on AWS usgs-landsat s3 bucket:\n",
    "collectionpath = 'collection02/level-1/standard/' # collection 2 level 1 data being used\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths, satellites, geographic projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# ADJUST THESE VARIABLES:\n",
    "basepath = '/home/jukes/Documents/Sample_glaciers/' # folder containing the terminus box and RGI glacier outline shapefile(s)\n",
    "downloadpath ='/media/jukes/jukes1/LS8aws/' # folder to eventually contain downloaded Landsat images'\n",
    "\n",
    "sats = ['L7','L8'] # enter names of landsat satellites to download images from (e.g., Landsat 7 and Landsat 8)\n",
    "L8_yrs = np.arange(2013,2022).astype(str) # set target years for L8: 2013-2021\n",
    "L7_yrs = np.arange(1999,2003).astype(str) # set target years for L7: 1999-2002 (some path rows don't exist in 2003 and are throwing errors)\n",
    "L8_bands = [8] # panchromatic band for L8\n",
    "L7_bands = [8] # panchromatic band for L7\n",
    "\n",
    "repopath = '/home/jukes/automated-glacier-terminus/' # path to this repository\n",
    "os.chdir(repopath) # change directories to this repo\n",
    "\n",
    "source_srs = '3413' # EPSG code for the current projection of the glacier shapefiles (3413 = Greenland polar stereo)\n",
    "\n",
    "csvext = '_updated_test_Box008.csv' # enter a file suffix for the CSV files produced that describes the analysis (e.g., glacier or group of glaciers)\n",
    "\n",
    "RGIpath = '/media/jukes/jukes1/RGI_shps/' # path to folder with all RGI glacier outline shapefiles\n",
    "boxespath = '/media/jukes/jukes1/Boxes_individual/' # path to folder with all the glacier terminus box shapefiles\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter in the glacier BoxIDs:\n",
    "\n",
    "The Greenland peripheral glacier terminus boxes were referenced using their 3 digit BoxID: Box###.\n",
    "For other glaciers, replace this code with a list of IDs corresponding to the glaciers and corresponding shapefiles (e.g. BoxHelheim.shp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008']\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "BoxIDs = []\n",
    "boxes = list(map(str, np.arange(8, 9, 1))) #1, 642, 1\n",
    "for BoxID in boxes: # convert integers to 3-digit strings with leading zeros\n",
    "    BoxID = BoxID.zfill(3)\n",
    "    BoxIDs.append(BoxID)\n",
    "print(BoxIDs) # show the final BoxIDs\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new folders corresponding to these glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists already for Box 008\n",
      "Path exists already in LS8aws for Box 008\n",
      "Box008.shp already in folder\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.prj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.shx moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n",
      "RGI_Box008.qpj moved\n",
      "RGI_Box008.shp moved\n",
      "RGI_Box008.dbf moved\n"
     ]
    }
   ],
   "source": [
    "# create new BoxID folders \n",
    "for BoxID in BoxIDs:\n",
    "    # create folder to hold glacier shapefiles\n",
    "    shapefilepath = basepath+'Box'+BoxID+'/' # path to that folder\n",
    "    if os.path.exists(shapefilepath):\n",
    "#         shutil.rmtree(shapefilepath) # remove the old folder\n",
    "        print(\"Path exists already for Box\", BoxID)\n",
    "    else:\n",
    "        os.mkdir(basepath+'Box'+BoxID)\n",
    "            \n",
    "    # create folder to hold glacier images (inside downloadpath)\n",
    "    if os.path.exists(downloadpath+'Box'+BoxID):\n",
    "        print(\"Path exists already in LS8aws for Box\", BoxID)\n",
    "    else:\n",
    "        os.mkdir(downloadpath+'Box'+BoxID)\n",
    "    \n",
    "    # Now place terminus box shapefile and RGI glacier outline shapefile into the\n",
    "    # boxespath folder. Done automatically below for the Greenland peripheral glaciers:\n",
    "    ######################################################################################\n",
    "    ID = int(BoxID) # make into an integer in order to grab the .shp files\n",
    "    \n",
    "    # if the terminus box shapefile is not in this folder, then move it\n",
    "    if not os.path.exists(shapefilepath+'Box'+BoxID+'.shp'):\n",
    "        for filename in os.listdir(boxespath):\n",
    "            if filename.startswith('BoxID_'+str(ID)):\n",
    "                shutil.copyfile(boxespath+filename, basepath+'Box'+BoxID+'/Box'+BoxID+filename[-4:])\n",
    "                print(\"Box\"+BoxID+filename[-4:], \"moved\")\n",
    "    else:\n",
    "        print(\"Box\"+BoxID+'.shp', \"already in folder\")\n",
    "\n",
    "    if not os.path.exists(shapefilepath+'RGI_Box'+BoxID+'.shp'): # if the RGI shapfile is not in this folder\n",
    "        # move RGI glacier outline into the new folder\n",
    "        for filename in os.listdir(RGIpath):\n",
    "            if filename.startswith('BoxID_'+str(ID)):\n",
    "                shutil.copyfile(RGIpath+filename, basepath+'Box'+BoxID+'/RGI_Box'+BoxID+filename[-4:])\n",
    "                print(\"RGI_Box\"+BoxID+filename[-4:], \"moved\")\n",
    "    else:\n",
    "        print(\"RGI_Box\"+BoxID+'.shp', \"already in folder\")\n",
    "    ######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Find all the Landsat footprints that overlap the glaciers\n",
    "\n",
    "This step requires the WRS-2_bound_world_0.kml file containing the footprints of all the Landsat scene boundaries available through the USGS (https://www.usgs.gov/land-resources/nli/landsat/landsat-shapefiles-and-kml-files). Place this file in your base directory (basepath). \n",
    "\n",
    "To check if they overlap the glacier terminus box shapefiles, the box shapefiles must be in WGS84 coordinates (ESPG: 4326). If they are not yet, we use the following GDAL command to reproject them into WGS84:\n",
    "\n",
    "        ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:NEW_EPSG_NUMBER -s_srs EPSG:OLD_EPSG_NUMBER output.shp input.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: ogr2ogr -f 'ESRI Shapefile' -t_srs EPSG:4326 -s_srs EPSG:3413 /home/jukes/Documents/Sample_glaciers/Box008/Box008_WGS.shp /home/jukes/Documents/Sample_glaciers/Box008/Box008.shp\n"
     ]
    }
   ],
   "source": [
    "# Reproject terminus box shapefiles to WGS84 if in a different projection\n",
    "for BoxID in BoxIDs:\n",
    "    boxespath = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # access the BoxID folders created \n",
    "    # construct the gdal command\n",
    "    rp = \"ogr2ogr -f 'ESRI Shapefile' -t_srs EPSG:4326 -s_srs EPSG:\"+source_srs+\" \"+boxespath+\"_WGS.shp \"+boxespath+\".shp\"\n",
    "    print(\"Command:\", rp) # check command\n",
    "    subprocess.run(rp, shell=True, check=True) # run the command on terminal\n",
    "    \n",
    "    # if an error is produced, check the error output on the terminal window that runs this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008 coordinates recorded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jukes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FionaDeprecationWarning: Collection.__next__() is buggy and will be removed in Fiona 2.0. Switch to `next(iter(collection))`.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Grab the WGS84 coordinates of the boxes\n",
    "box_points = {} # dictionary of points\n",
    "for BoxID in BoxIDs:\n",
    "    boxpath = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # path to the reprojected terminus box\n",
    "    termbox = fiona.open(boxpath+'_WGS.shp') # open reprojected terminus box\n",
    "    box = termbox.next(); box_coords=box['geometry']['coordinates'][0] # grab coords\n",
    "    points = [] # to hold the box vertices\n",
    "    \n",
    "    # read coordinates and convert to a shapely object\n",
    "    for coord_pair in box_coords: \n",
    "        lat = coord_pair[0]; lon = coord_pair[1]        \n",
    "        point = shapely.geometry.Point(lat, lon) # create shapely point \n",
    "        points.append(point) # append to points list\n",
    "        \n",
    "    box_points.update({BoxID: points}) # update dictionary\n",
    "    print(\"Box\"+BoxID+\" coordinates recorded.\") # keep track of progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# open the kml file with the Landsat path, row footprints:\n",
    "WRS = fiona.open(basepath+'WRS-2_bound_world_0.kml', driver='KML') # check the path to the world bounds file\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008</td>\n",
       "      <td>031</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>008</td>\n",
       "      <td>030</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008</td>\n",
       "      <td>029</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>008</td>\n",
       "      <td>028</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>008</td>\n",
       "      <td>027</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>008</td>\n",
       "      <td>032</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>008</td>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoxID Path  Row\n",
       "0   008  031  006\n",
       "1   008  030  006\n",
       "2   008  029  006\n",
       "3   008  028  006\n",
       "4   008  027  006\n",
       "5   008  032  005\n",
       "6   008  031  005"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []; rows = []; boxes = [] # create lists to hold the paths and rows and BoxIDs\n",
    "\n",
    "#loop through all Landsat scenes (path, row footprints)\n",
    "for feature in WRS:\n",
    "    # create shapely polygons from the Landsat footprints\n",
    "    coordinates = feature['geometry']['coordinates'][0]\n",
    "    coords = [xy[0:2] for xy in coordinates]\n",
    "    pathrow_poly = Polygon(coords)\n",
    "    \n",
    "    # grab the path and row name from the WRS kml file:\n",
    "    pathrowname = feature['properties']['Name']  \n",
    "    path = pathrowname.split('_')[0]; row = pathrowname.split('_')[1]\n",
    "#     print(path, row)\n",
    "    \n",
    "    # for each feature, loop through each of the vertices stored in the dictionary\n",
    "    for BoxID in box_points:  \n",
    "        box_points_in = 0 # counter for number of box_points in the pathrow_geom:\n",
    "        points = box_points.get(BoxID) # grab the points corresponding to the ID\n",
    "        for i in range(0, len(points)):\n",
    "            point = points[i]\n",
    "            if point.within(pathrow_poly): # if the pathrow shape contains the point\n",
    "                box_points_in = box_points_in+1 # append the counter\n",
    "        if box_points_in == 5: # if all box vertices are inside the footprint, save the path, row, BoxID\n",
    "            paths.append('%03d' % int(path))\n",
    "            rows.append('%03d' % int(row))\n",
    "            boxes.append(BoxID)\n",
    "\n",
    "# Store in dataframe\n",
    "boxes_pr_df = pd.DataFrame(list(zip(boxes, paths, rows)), columns=['BoxID','Path', 'Row'])\n",
    "boxes_pr_df = boxes_pr_df.sort_values(by='BoxID')\n",
    "boxes_pr_df # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "PR_FILENAME = 'LS_pathrows'+csvext # save name with common extension\n",
    "boxes_pr_df.to_csv(path_or_buf = basepath+PR_FILENAME, sep=',') # write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Download metadata files from AWS s3 for overlapping Landsat scenes\n",
    "The old syntax for grabbing an individual Landsat 8 metadata file from the Collection 1 data is as follows:\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/c1/L8/path/row/LC08_XXXX_pathrow_yyyyMMdd_01_T1/LC08_XXXX_pathrow_yyyyMMdd_01_T1_MTL.txt /path_to/output/\n",
    "     \n",
    "The new syntax for listing the Collection 2 Landsat image files AWS s3 bucket is as follows:\n",
    "\n",
    "    aws s3 ls --request-payer requester s3://usgs-landsat/collection02/level-2/standard/oli-tirs/yyyy/path/row/LC08_LS2R_pathrow_yyyyMMdd_yyyyMMdd_02_T1/ \n",
    "    \n",
    "If you use the old syntax for grabbing a metadata file, you will encounter an AccessDenied return. However, s3 is a valid argument command for the aws CLI. Writing this combined with ls (lists the files within a folder) and --request-payer requester (indicates the requester will be charged for data download) will allow you to access files within the USGS Landsat image bucket.\n",
    "\n",
    "We can use the paths and rows in the dataframe to access the full Landsat 8 scene list and the corresponding metdata files. Read https://docs.opendata.aws/landsat-pds/readme.html to learn more.\n",
    "\n",
    "The old way to download the metadatafiles into Path_Row folders was by using:\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/c1/L8/031/005/ Output/path/LS8aws/Path031_Row005/ --recursive --exclude \"*\" --include \"*MTL.txt\" \n",
    "    \n",
    "The updated way to download the metadatafiles into Path_Row folders is by using:\n",
    "    \n",
    "    aws s3api get-object --bucket usgs-landsat --key collection02/level-2/standard/oli-tirs/yyyy/path/row/LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1/LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1_MTL.txt  --request-payer requester LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1_MTL.txt\n",
    "\n",
    "__NOTE: Including the --request-payer requester as part of this line indicates that the referenced user will be charged for data download.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoxID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>0</td>\n",
       "      <td>031</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>1</td>\n",
       "      <td>030</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>2</td>\n",
       "      <td>029</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>3</td>\n",
       "      <td>028</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>4</td>\n",
       "      <td>027</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>5</td>\n",
       "      <td>032</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>6</td>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Path  Row\n",
       "BoxID                     \n",
       "008            0  031  006\n",
       "008            1  030  006\n",
       "008            2  029  006\n",
       "008            3  028  006\n",
       "008            4  027  006\n",
       "008            5  032  005\n",
       "008            6  031  005"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv file from Step 2\n",
    "boxes_pr_df = pd.read_csv(basepath+PR_FILENAME, dtype=str)\n",
    "boxes_pr_df = boxes_pr_df.set_index('BoxID'); boxes_pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded metadata files are stored in: /media/jukes/jukes1/LS8aws/Path031_Row006_c1/\n",
      "Path031_Row006_c1  EXISTS ALREADY. SKIP.\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/etm/1999/031/006/LE07_L1TP_031006_19990829_20200918_02_T1/LE07_L1TP_031006_19990829_20200918_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LE07_L1TP_031006_19990829_20200918_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/etm/2002/031/006/LE07_L1TP_031006_20020330_20200916_02_T1/LE07_L1TP_031006_20020330_20200916_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LE07_L1TP_031006_20020330_20200916_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2013/031/006/LC08_L1TP_031006_20130523_20200913_02_T1/LC08_L1TP_031006_20130523_20200913_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20130523_20200913_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2013/031/006/LC08_L1TP_031006_20130827_20200912_02_T1/LC08_L1TP_031006_20130827_20200912_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20130827_20200912_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2013/031/006/LC08_L1TP_031006_20130928_20200912_02_T1/LC08_L1TP_031006_20130928_20200912_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20130928_20200912_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140323_20200911_02_T1/LC08_L1TP_031006_20140323_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140323_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140526_20200911_02_T1/LC08_L1TP_031006_20140526_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140526_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140627_20200911_02_T1/LC08_L1TP_031006_20140627_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140627_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140713_20201015_02_T1/LC08_L1TP_031006_20140713_20201015_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140713_20201015_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140814_20200911_02_T1/LC08_L1TP_031006_20140814_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140814_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140830_20200911_02_T1/LC08_L1TP_031006_20140830_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140830_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20140915_20200911_02_T1/LC08_L1TP_031006_20140915_20200911_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20140915_20200911_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2014/031/006/LC08_L1TP_031006_20141001_20200910_02_T1/LC08_L1TP_031006_20141001_20200910_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20141001_20200910_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20150310_20200909_02_T1_MTL.txt\n",
      "\n",
      "Command: aws s3api get-object --bucket usgs-landsat --key collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150411_20200909_02_T1/LC08_L1TP_031006_20150411_20200909_02_T1_MTL.txt --request-payer requester /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20150411_20200909_02_T1_MTL.txt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-db808b8ba903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Command:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m# if an error is produced, check the error output on the terminal window that runs this notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through the dataframe containing overlapping path, row info:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; folder_name = 'Path'+p+'_Row'+r+'_c1' # folder name\n",
    "    bp_out = downloadpath+folder_name+'/' # output path for the downloaded files\n",
    "    print(\"Downloaded metadata files are stored in:\",bp_out)\n",
    "    \n",
    "    # create Path_Row folders if they don't exist already\n",
    "    if os.path.exists(bp_out):\n",
    "        print(folder_name, \" EXISTS ALREADY. SKIP.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out)\n",
    "        print(folder_name+\" directory made\")\n",
    "    \n",
    "    for sat in sats: # for each satellite\n",
    "        if sat == 'L8':\n",
    "            collectionfolder = 'oli-tirs/'; years = L8_yrs; prefix='LC08' # set folder, years, file prefix\n",
    "        elif sat == 'L7':\n",
    "            collectionfolder = 'etm/'; years = L7_yrs; prefix='LE07' # set folder, years, file prefix\n",
    "        \n",
    "        # loop through years\n",
    "        for year in years:\n",
    "            # grab list of images for the year\n",
    "            find_imgs = 'aws s3 ls --request-payer requester s3://usgs-landsat/'+collectionpath+collectionfolder\n",
    "            find_imgs += year+'/'\n",
    "            find_imgs += p+'/'+r+'/'\n",
    "            result = subprocess.check_output(find_imgs,shell=True)\n",
    "            results = result.split() # split string\n",
    "            \n",
    "            imagenames = []\n",
    "            for line in results: # loop through strings\n",
    "                line = str(line)\n",
    "                if prefix in line and 'T1' in line: # find just the Tier-1 image names with the correct prefixes\n",
    "                    imgname = line[2:-2]; imagenames.append(imgname)\n",
    "\n",
    "            # download the metadata (MTL.txt) file\n",
    "            for imgname in imagenames:\n",
    "                command = 'aws s3api get-object --bucket usgs-landsat --key '+collectionpath+collectionfolder\n",
    "                command += year+'/'\n",
    "                command += p+'/'+r+'/'\n",
    "                command += imgname+'/'+imgname+'_MTL.txt'\n",
    "                command += ' --request-payer requester '\n",
    "                command += bp_out+imgname+'_MTL.txt'\n",
    "\n",
    "                # # Old way to obtain files from AWS\n",
    "                # command = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*MTL.txt\"'\n",
    "\n",
    "                print('Command:',command); print()\n",
    "                subprocess.run(command,shell=True,check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Calculate cloud % over terminus box using Landsat quality band\n",
    "\n",
    "\n",
    "### Reproject terminus boxes into UTM projections to match Landsat files\n",
    "\n",
    "If the terminus box shapefiles were not originally in UTM projection, will need to reproject them into UTM to match the Landsat projection. The code automatically finds the UTM zones from the metadata files and fills in the following syntax to reproject:\n",
    "    \n",
    "    ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326zone -s_srs EPSG:3413 output.shp input.shp\n",
    "    \n",
    "If they are already in UTM projection, skip this step and rename the files to end with \"\\_UTM\\_##.shp\" where ## corresponds to the zone number (e.g., \"\\_UTM\\_07.shp\", \"\\_UTM\\_21.shp\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "      <th>Zone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoxID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>0</td>\n",
       "      <td>031</td>\n",
       "      <td>006</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>1</td>\n",
       "      <td>030</td>\n",
       "      <td>006</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>2</td>\n",
       "      <td>029</td>\n",
       "      <td>006</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>3</td>\n",
       "      <td>028</td>\n",
       "      <td>006</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008</th>\n",
       "      <td>4</td>\n",
       "      <td>027</td>\n",
       "      <td>006</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Path  Row Zone\n",
       "BoxID                          \n",
       "008            0  031  006   19\n",
       "008            1  030  006   19\n",
       "008            2  029  006   19\n",
       "008            3  028  006   19\n",
       "008            4  027  006   20"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones = {} # initialize dictionary to hold UTM zone for each Landsat scene path row\n",
    "zone_list = [] # list of zones\n",
    "\n",
    "# Loop through all scenes:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    BoxID = str(index)\n",
    "    p = row['Path']; r = row['Row']; folder_name = 'Path'+p+'_Row'+r+'_c1' # Landsat path and row from dataframe\n",
    "    pr_folderpath = downloadpath+folder_name+'/' # path to the downloaded metadata files\n",
    "    pathtoshp = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # path to the terminus box shapefiles (all projections)\n",
    "    \n",
    "    if len(os.listdir(pr_folderpath)) > 0: # if there are files in the folder\n",
    "        # grab UTM Zone from the first metadata file\n",
    "        mtl_scene = glob.glob(pr_folderpath+'*_MTL.txt')[0]\n",
    "        mtl = open(mtl_scene, 'r')\n",
    "        \n",
    "        # loop through lines in the metadata file to find the UTM ZONE\n",
    "        for line in mtl:  \n",
    "            variable = line.split(\"=\")[0] # grab the variable name\n",
    "            if (\"UTM_ZONE\" in variable):\n",
    "                zone = '%02d' % int(line.split(\"=\")[1][1:-1]) # grab the 2-digit zone number\n",
    "                zones.update({folder_name: zone}); zone_list.append(zone) # add to zone lists\n",
    "                break\n",
    "                \n",
    "        # reproject shapefile(s) into UTM\n",
    "        zone = zones[folder_name]\n",
    "        rp_shp = 'ogr2ogr -f \"ESRI Shapefile\" '+pathtoshp+'_UTM_'+zone+'.shp '+pathtoshp+'_WGS.shp -t_srs EPSG:326'+zone+' -s_srs EPSG:4326'\n",
    "        subprocess.run(rp_shp, shell=True,check=True)\n",
    "        \n",
    "    else: # if no files in folder, zone = nan, must fill in manually\n",
    "        zone_list.append(np.nan)\n",
    "        \n",
    "boxes_pr_df['Zone'] = zone_list # add to the path row dataframe\n",
    "boxes_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite path row csv file with UTM zone information, see above for variable PR_FILENAME\n",
    "boxes_pr_df.to_csv(path_or_buf = basepath+PR_FILENAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download subset of the quality band file for each scene\n",
    "\n",
    "Use gdalwarp commands and the __vsi3__ link to download subset of the quality band we will use to determine cloud cover over the terminus:\n",
    "\n",
    "    gdalwarp -cutline path_to_shp.shp -crop_to_cutline /vsi3/usgs-landsat/collection02/level-1/standard/oli-tirs/yyyy/path/row/scene/scene_QA_PIXEL.TIF path_to_subset_QA_PIXEL.TIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC08_L1TP_031006_20150310_20200909_02_T1\n",
      "Command: gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Box008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL.TIF /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL_Box008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 \n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Box008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL.TIF /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL_Box008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 ' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-a2d4ebdaf534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Uncomment the following to run:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQAPIXEL_dwnld_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_QAPIXEL_Box'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.TIF subset downloaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Box008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL.TIF /media/jukes/jukes1/LS8aws/Path031_Row006_c1/LC08_L1TP_031006_20150310_20200909_02_T1_QA_PIXEL_Box008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 ' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "# Loop through all scenes:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; zone = row['Zone'] # grab path, row, zone\n",
    "    BoxID = str(index)\n",
    "    folder_name = 'Path'+p+'_Row'+r+'_c1'\n",
    "    pr_folderpath = downloadpath+folder_name+'/' # path to the downloaded metadata files\n",
    "    pathtoshp = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # path to the terminus box shapefiles (all projections)\n",
    "    pathtoshp_rp = pathtoshp+'_UTM_'+zone # path to the UTM projected box shapefile\n",
    "\n",
    "    files = os.listdir(pr_folderpath) # grab the names of the Landsat scenes\n",
    "    \n",
    "    # for all files in the path row folders\n",
    "    for file in files:\n",
    "        scene = file[:-19] # slice the filename to grab the scene name\n",
    "        print(scene)\n",
    "        \n",
    "        if 'L1TP' in scene and 'T1' in scene: # L1TP scenes\n",
    "            scene_year = scene[17:21] # grab the year from the scene name\n",
    "            \n",
    "            if scene.startswith('LC08'):\n",
    "                collectionfolder='oli-tirs/'\n",
    "            elif scene.startswith('LE07'):\n",
    "                collectionfolder='etm/'\n",
    "                \n",
    "            # set path to the QA pixel Landsat files\n",
    "            pathtoQAPIXEL='/vsis3/usgs-landsat/'+collectionpath+collectionfolder\n",
    "            pathtoQAPIXEL+=year+'/'\n",
    "            pathtoQAPIXEL+=p+'/'+r+'/'\n",
    "            pathtoQAPIXEL+=scene+'/'+scene+\"_QA_PIXEL.TIF\"\n",
    "            \n",
    "            # set path to the subset QA pixel files inside the path row folders\n",
    "            subsetout = pr_folderpath+scene+'_QA_PIXEL_Box'+BoxID+'.TIF' \n",
    "\n",
    "            # construct download command\n",
    "            QAPIXEL_dwnld_cmd='gdalwarp -overwrite -cutline '+pathtoshp_rp+'.shp -crop_to_cutline '+pathtoQAPIXEL+' '+subsetout\n",
    "            QAPIXEL_dwnld_cmd+=' --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 '\n",
    "            print(\"Command:\", QAPIXEL_dwnld_cmd); print() # check command syntax before downloading\n",
    "\n",
    "            subprocess.run(QAPIXEL_dwnld_cmd, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Create buffer zone around terminus boxes and rasterize terminus boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jukes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FionaDeprecationWarning: Collection.__next__() is buggy and will be removed in Fiona 2.0. Switch to `next(iter(collection))`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Buff_dist_m</th>\n",
       "      <th>min_dim_px</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008</td>\n",
       "      <td>9438</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoxID  Buff_dist_m  min_dim_px\n",
       "0   008         9438         298"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffers = []; mindimensions = [] # store the buffer distances and minimum dimensions\n",
    "\n",
    "# Calculate a buffer distance around the terminus box:\n",
    "for BoxID in BoxIDs:\n",
    "    for file in os.listdir(basepath+'Box'+BoxID+'/'):\n",
    "        if 'UTM' in file and '.shp' in file and \"Box\" in file: # identify UTM projected box\n",
    "            boxpath = basepath+\"Box\"+BoxID+\"/\"+file  \n",
    "            termbox = fiona.open(boxpath)\n",
    "            \n",
    "    # grab the box coordinates:\n",
    "    box = termbox.next(); box_geom= box.get('geometry'); box_coords = box_geom.get('coordinates')[0]\n",
    "    points = []\n",
    "    for coord_pair in box_coords:\n",
    "        lat = coord_pair[0]; lon = coord_pair[1]; points.append([lat, lon])\n",
    "            \n",
    "    # Calculate distance between coord 1 and 2 and between 2 and 3\n",
    "    coord1 = points[0]; coord2 = points[1]; coord3 = points[2]   \n",
    "    dist1 = distance(coord1[0], coord1[1], coord2[0], coord2[1]);\n",
    "    dist2 = distance(coord2[0], coord2[1], coord3[0], coord3[1]) \n",
    "    buff_dist = int(np.max([dist1, dist2])) # pick the longer one as the buffer distance\n",
    "    mindim = int(np.min([dist1, dist2]))/15.0 # calculate the minimum dimension in pixels\n",
    "    \n",
    "    # record:\n",
    "    buffers.append(buff_dist)\n",
    "    mindimensions.append(int(mindim))\n",
    "\n",
    "# store as dataframe:\n",
    "buff_df = pd.DataFrame(list(zip(BoxIDs, buffers, mindimensions)), columns=['BoxID', 'Buff_dist_m', 'min_dim_px'])\n",
    "buff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv, change name as desired\n",
    "BOX_FILENAME = 'Buffdist'+csvext\n",
    "buff_df.to_csv(basepath+BOX_FILENAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a buffer zone shapefile using GDAL command **ogr2ogr** with the following syntax:\n",
    "\n",
    "    ogr2ogr Buffer###.shp path_to_terminusbox###.shp  -dialect sqlite -sql \"SELECT ST_Buffer(geometry, buffer_distance) AS geometry,*FROM 'Box###'\" -f \"ESRI Shapefile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: ogr2ogr /home/jukes/Documents/Sample_glaciers/Box008/Buffer008.shp /home/jukes/Documents/Sample_glaciers/Box008/Box008.shp -dialect sqlite -sql \"SELECT ST_Buffer(geometry, 9438) AS geometry,*FROM 'Box008'\" -f \"ESRI Shapefile\"\n"
     ]
    }
   ],
   "source": [
    "# loop through the buffer distance dataframe:\n",
    "for index, row in buff_df.iterrows():\n",
    "    BoxID = row['BoxID']\n",
    "    buff_dist = str(row['Buff_dist_m'])\n",
    "    \n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\" # path to box shapefile\n",
    "    outputbuffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\" # path and name of new buffer file\n",
    "    \n",
    "    # Set buffer command\n",
    "    buffer_cmd = 'ogr2ogr '+outputbuffer_path+\" \"+terminusbox_path+' -dialect sqlite -sql \"SELECT ST_Buffer(geometry, '+buff_dist+\") AS geometry,*FROM 'Box\"+BoxID+\"'\"+'\" -f \"ESRI Shapefile\"'\n",
    "    print(\"Command:\", buffer_cmd)\n",
    "    \n",
    "    subprocess.run(buffer_cmd, shell=True, check=True) # run on terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize terminus boxes (to be used as a mask during the WTMM filering) using the GDAL **gdal_rasterize** command, subset to the buffer zone using the GDAL **gdalwarp** command, and reprojected:\n",
    "\n",
    "1) Rasterize\n",
    "\n",
    "        gdal_rasterize -burn 1.0 -tr x_resolution y_resolution -a_nodata 0.0 path_to_terminusbox.shp path_to_terminusbox_raster.TIF\n",
    "    \n",
    "2) Subset\n",
    "\n",
    "        gdalwarp -cutline path_to_Buffer###.shp -crop_to_cutline path_to_terminusbox_raster.TIF path_to_subset_raster_cut.TIF\n",
    "    \n",
    "3) Reproject to UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008\n"
     ]
    }
   ],
   "source": [
    "for index, row in buff_df.iterrows():\n",
    "    BoxID = row['BoxID']\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # path to box\n",
    "    buffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID # path to buffer\n",
    "    zones = boxes_pr_df.loc[BoxID, 'Zone'] # grab zone matching BoxID from other dataframe\n",
    "    \n",
    "    terminusraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".TIF\" # path to rasterized box\n",
    "    cutraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\"_raster_cut.TIF\" # name for cropped file\n",
    "    \n",
    "    # Set commands\n",
    "    rasterize_cmd = 'gdal_rasterize -burn 1.0 -tr 15.0 15.0 -a_nodata 0.0 '+terminusbox_path+'.shp '+terminusraster_path\n",
    "    subsetbuffer_cmd = 'gdalwarp -overwrite -cutline '+buffer_path+'.shp -crop_to_cutline '+terminusraster_path+' '+cutraster_path\n",
    "    subprocess.run(rasterize_cmd, shell=True,check=True) # rasterize with command terminal\n",
    "    subprocess.run(subsetbuffer_cmd, shell=True,check=True) # subset to buffer with command terminal\n",
    "    \n",
    "    # Reprojection needs to happen for each zone\n",
    "    for zone in zones:\n",
    "        rp_shp = 'ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326'+zone+' -s_srs EPSG:'+source_srs+' '+buffer_path+\"_UTM_\"+zone+\".shp \"+buffer_path+'.shp'\n",
    "        subprocess.run(rp_shp, shell=True, check=True) # reproject\n",
    "\n",
    "    print(\"Box\"+BoxID) # check progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Download non-cloudy Landsat images from AWS\n",
    "\n",
    "To remove cloudy images, we will find the number of pixels in our terminus box that correspond to a cumulative pixel value of > 4096 in the QA_PIXEL band. If the fraction of cloudy pixels with values is above the threshold, we won't download the image. \n",
    "\n",
    "Additionally, we remove images that are primarily black (fill value of 0 or 1 in QA_PIXEL band). This ensures that the scenes that cut off halfway across the glacier are not included in further analysis. The fill percent threshold may need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# These are the recommended values. Adjust thresholds here:\n",
    "QAPIXEL_thresh = 22280.0 # QA pixel value threshold to be considered cloud\n",
    "cpercent_thresh = 50.0 # maximum cloud cover % in terminus box\n",
    "fpercent_thresh = 60.0 # maximum fill % in terminus box\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008  exists already. Skip creation of directory.\n",
      "LC08_L1TP_031006_20150310_20200909_02_T1 Cloud %  37 Fill % 46\n",
      "gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Buffer008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_B8.TIF /media/jukes/jukes1/LS8aws/Box008/LC08_L1TP_031006_2015_B8_Buffer008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 \n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Buffer008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_B8.TIF /media/jukes/jukes1/LS8aws/Box008/LC08_L1TP_031006_2015_B8_Buffer008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 ' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-a1f90a3419dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                          \u001b[0;31m# Once checked, uncomment the following to commence download:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'gdalwarp -overwrite -cutline /home/jukes/Documents/Sample_glaciers/Box008/Buffer008_UTM_19.shp -crop_to_cutline /vsis3/usgs-landsat/collection02/level-1/standard/oli-tirs/2015/031/006/LC08_L1TP_031006_20150310_20200909_02_T1/LC08_L1TP_031006_20150310_20200909_02_T1_B8.TIF /media/jukes/jukes1/LS8aws/Box008/LC08_L1TP_031006_2015_B8_Buffer008.TIF --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 ' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "# Download images that pass these thresholds:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    # grab paths\n",
    "    p = row['Path']; zone = row['Zone']; r = row['Row']; BoxID = index; \n",
    "    folder_name = 'Path'+p+'_Row'+r+'_c1'\n",
    "    pr_folderpath = downloadpath+folder_name+'/'\n",
    "    bp_out = downloadpath+'Box'+BoxID+'/' # folder name for downloaded images\n",
    "    if os.path.exists(bp_out): # create folder if it does not exist\n",
    "        print(\"Box\"+BoxID, \" exists already. Skip creation of directory.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out)\n",
    "        print(\"Box\"+BoxID+\" directory made.\")\n",
    "    \n",
    "    # path to the shapefile covering the region that will be downloaded\n",
    "    pathtobuffer = basepath+'Box'+BoxID+'/Buffer'+BoxID+'_UTM_'+zone+'.shp'  # buffer around box - recommended\n",
    "#     pathtobox = basepath+'Box'+BoxID+'/Box'+BoxID+'_UTM_'+zone+'.shp' # just the box\n",
    "    \n",
    "    for scene in os.listdir(pr_folderpath):\n",
    "        if scene.endswith(\".TIF\") and 'T1' in scene: # For Tier-1 images\n",
    "            scene = scene[:-19]\n",
    "            year = scene[17:21] # grab acquisition year\n",
    "            \n",
    "            if scene.startswith(\"LC08\"): # Landsat 8\n",
    "                collectionfolder = 'oli-tirs/'; bands = L8_bands\n",
    "            elif scene.startswith(\"LC07\"): # Landsat 7\n",
    "                collectionfolder = 'etm/'; bands = L7_bands\n",
    " \n",
    "            QApixelpath = pr_folderpath+scene+'_QAPIXEL_Box'+BoxID+'.TIF' # path to QA_PIXEL file\n",
    "            subsetQApixel = mpimg.imread(QApixelpath) # read in as numpy array\n",
    "            \n",
    "            # calculate percentages of cloud and fill bixels\n",
    "            totalpixels = subsetQApixel.shape[0]*subsetQApixel.shape[1] # total number of pixels\n",
    "            cloudQApixel = subsetQApixel[subsetQApixel > QAPIXEL_thresh] # cloudy pixels (value > QAPIXEL_thresh)\n",
    "            fillQApixel = subsetQApixel[subsetQApixel < 2.0] # fill pixels (value = 0 or 1)\n",
    "            cloudpixels = len(cloudQApixel); fillpixels = len(fillQApixel) # count the cloudy and fill pixels\n",
    "            cloudpercent = int(float(cloudpixels)/float(totalpixels)*100) # calculate percent cloudy\n",
    "            fillpercent = int(float(fillpixels)/float(totalpixels)*100) # calculate percent fill\n",
    "            print(scene, 'Cloud % ', cloudpercent, 'Fill %', fillpercent) # check values\n",
    "            \n",
    "            # evaluate thresholds\n",
    "            if cloudpercent <= cpercent_thresh and fillpercent <= fpercent_thresh:\n",
    "                # download the bands for that scene into your scene folders:\n",
    "                for band in bands:\n",
    "                        band = str(band) # string format\n",
    "                        \n",
    "                        # input path to your bands in AWS:\n",
    "                        pathin = '/vsis3/usgs-landsat/'+collectionpath+collectionfolder+year+'/'+p+\"/\"+r+\"/\"+scene+\"/\"+scene+\"_B\"+band+\".TIF\"\n",
    "                        \n",
    "                        outfilename = scene[0:-19]+\"_B\"+band+'_Buffer'+BoxID+'.TIF' # output file name\n",
    "                        pathout = downloadpath+'Box'+BoxID+'/'+outfilename # full output file path\n",
    "\n",
    "                        # construct download command\n",
    "                        download_cmd = 'gdalwarp -overwrite -cutline '+pathtobuffer+' -crop_to_cutline '+pathin+' '+pathout+ ' --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2 '\n",
    "                        print(download_cmd) # check\n",
    "\n",
    "                        # Once checked, uncomment the following to commence download:                   \n",
    "                        subprocess.run(download_cmd, shell=True, check=True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject the downloaded files from UTM into your desired projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box008 Reprojected folder exists already.\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "desired_proj = '3413' # EPSG code for desired projection\n",
    "suffix = '_PS' # suffix for reprojected images - something that indicates the projection\n",
    "######################################################################################\n",
    "\n",
    "for BoxID in list(set(boxes_pr_df.index)):\n",
    "    bp_out = downloadpath+'Box'+BoxID+'/' # path to downloaded files\n",
    "    \n",
    "    # create output reprojected folder if does not exist\n",
    "    if os.path.exists(bp_out+'reprojected/'):\n",
    "        print(\"Box\"+BoxID, \"Reprojected folder exists already.\")\n",
    "    else:\n",
    "        os.mkdir(bp_out+'reprojected/')\n",
    "        print(\"Box\"+BoxID+\" Reprojected directory made\")\n",
    "                      \n",
    "    downloadedimages = os.listdir(bp_out) # all downloaded images\n",
    "    for image in downloadedimages:\n",
    "        if image.endswith('.TIF'):\n",
    "            imagename = image[:-4] # remove suffix\n",
    "            print(image)\n",
    "\n",
    "            rp_PS = \"gdalwarp -t_srs EPSG:\"+desired_proj+' '+bp_out+image+\" \"+bp_out+'reprojected/'+imagename+suffix+\".TIF\"\n",
    "            subprocess.run(rp_PS, shell=True,check=True)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically grab the image acquisition dates from the metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path031_Row006_c1\n",
      "Path030_Row006_c1\n",
      "Path029_Row006_c1\n",
      "Path028_Row006_c1\n",
      "Path027_Row006_c1\n",
      "Path032_Row005_c1\n",
      "Path031_Row005_c1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scene</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Scene, datetime]\n",
       "Index: []"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimes = [] # list of scene datetimes\n",
    "scenes_dated = [] # list of scenes\n",
    "\n",
    "# Loop through the dataframe with your path row combinations:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; BoxID = index; \n",
    "    folder_name = 'Path'+p+'_Row'+r+'_c1'; print(folder_name)\n",
    "    \n",
    "    # Output folder paths\"\n",
    "    folderpath = downloadpath+folder_name+'/'\n",
    "    bp_out = downloadpath+'Box'+BoxID+'/reprojected/'\n",
    "      \n",
    "    scenecount = 0 # keep track of the number of scenes:\n",
    "    \n",
    "    downloaded_scenes = os.listdir(bp_out)\n",
    "    for scene in downloaded_scenes:\n",
    "        if scene.endswith('.TIF'):\n",
    "            scenename = scene[:-20] # MAY NEED TO ADJUST DEPENDING ON SUFFIX - CHANGE THIS TO start and end\n",
    "            print(scenename)\n",
    "            if scenename in os.listdir(folderpath):\n",
    "                scenefiles = os.listdir(folderpath+scenename+'/')\n",
    "                for file in scenefiles:\n",
    "                    if (\"MTL.txt\" in file): # open metadata file\n",
    "                        mdata = open(folderpath+scenename+\"/\"+scenename+\"_MTL.txt\", \"r\")\n",
    "                        for line in mdata:\n",
    "                            variable = line.split(\"=\")[0]\n",
    "                            if (\"DATE_ACQUIRED\" in variable):\n",
    "                                date = line.split(\"=\")[1][1:-1] # find acquisition date\n",
    "                        dates = datetime.datetime.strptime(date, '%Y-%m-%d') # save as datetime object\n",
    "                        datetimes.append(dates); scenes_dated.append(scenename) # store in lists\n",
    "                scenecount = scenecount+1\n",
    "\n",
    "# Store in a dataframe\n",
    "datetime_df = pd.DataFrame(list(zip(scenes_dated, datetimes)), columns=['Scene', 'datetime'])\n",
    "datetime_df = datetime_df.sort_values(by='datetime', ascending=True); datetime_df = datetime_df.drop_duplicates()\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dates to csv\n",
    "DATES_FILENAME = 'imgdates'+csvext \n",
    "datetime_df.to_csv(path_or_buf = basepath+DATES_FILENAME, sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pass at cloud filtering using range in pixel values\n",
    "\n",
    "Removes cloudy images that slipped through QA filtering. Skip if unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert all files in reprojected folder to png from TIF\n",
    "# for BoxID in BoxIDs:\n",
    "#     print(BoxID)\n",
    "#     command = 'cd '+downloadpath+'Box'+BoxID+'/reprojected/; '+'mogrify -format png *_PS.TIF'\n",
    "#     subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ######################################################################################\n",
    "# suffix = '_PS' # reprojection suffix\n",
    "# ######################################################################################\n",
    "\n",
    "# for BoxID in BoxIDs:\n",
    "#     imagepath = downloadpath+'Box'+BoxID+'/reprojected/'\n",
    "#     for img in os.listdir(imagepath):\n",
    "#         if img.endswith('Buffer'+BoxID+suffix+'.png'):\n",
    "#             image = cv2.imread(imagepath+img,-1) # read in image\n",
    "#             imageplt = mpimg.imread(imagepath+img)\n",
    "#             image_nofill = imageplt[imageplt > 0] # don't consider the fill points\n",
    "#             img_std = np.std(image_nofill) # st. dev in values\n",
    "#             if len(image_nofill.shape) > 1:\n",
    "#                 img_range = np.max(image_nofill) - np.min(image_nofill)\n",
    "#                 img_med = np.median(image_nofill)\n",
    "\n",
    "#                 if img_std < 0.04 and img_med > 0.15: # adjust these threholds\n",
    "#                     os.remove(imagepath+img) # remove png mimage\n",
    "#                     os.remove(imagepath+img[:-4]+'.TIF') # remove pgm image     \n",
    "#                     # show the image\n",
    "#     #                 imgplt_trim = plt.imshow(cv2.cvtColor(imageplt, cv2.COLOR_BGR2RGB))\n",
    "#     #                 plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab fraction of total images available that were excluded due to clouds and fill\n",
    "\n",
    "If you are interested in knowing how many images were filtered out using the cloud and fill thresholds, run the following cells. Otherwise, skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in path, row csv file if not already loaded\n",
    "# boxes_pr_df = pd.read_csv(basepath+PR_FILENAME, dtype=str)\n",
    "# boxes_pr_df = boxes_pr_df.set_index('BoxID'); BoxIDs = list(set(boxes_pr_df.index))\n",
    "# print(BoxIDs); boxes_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_tots = []; downloaded = []; fractions = []\n",
    "\n",
    "# for BoxID in BoxIDs:\n",
    "#     pathrows_BoxID = boxes_pr_df[boxes_pr_df.index == BoxID].copy() # grab path rows for that BoxID\n",
    "    \n",
    "#     im_tot = 0 # count number of total scenes available\n",
    "#     for idx, rw in pathrows_BoxID.iterrows():\n",
    "#         p = rw['Path']; r = rw['Row']\n",
    "#         ims_pr = len(os.listdir(downloadpath+'Path'+p+'_Row'+r+'_c1')) # grab number of scenes in that pathrow\n",
    "#         im_tot = im_tot + ims_pr\n",
    "    \n",
    "#     counter = 0\n",
    "#     if im_tot == 0: # if no images\n",
    "#         download_frac = np.NaN\n",
    "#     else:\n",
    "#         # count the files that passed thresholds and got downloaded\n",
    "#         for file in os.listdir(downloadpath+'Box'+BoxID+'/reprojected/'):\n",
    "#             if file.endswith('.png') and 'B8' in file: # panchromatic band (B8)\n",
    "#                 counter = counter + 1\n",
    "            \n",
    "#         download_frac = int(counter/im_tot*100) # calculate fraction downloaded\n",
    "#     im_tots.append(im_tot); downloaded.append(counter); fractions.append(download_frac) # store values\n",
    "\n",
    "# # store in dataframe\n",
    "# downloaded_df = pd.DataFrame(list(zip(BoxIDs, im_tots, downloaded, fractions)), columns = ['BoxID', 'Total_ims', 'Downloaded', '%'])\n",
    "# downloaded_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write percent downloaded to csv\n",
    "# DOWNLOADED_FILENAME = 'Images_downloaded'+csvext\n",
    "# downloaded_df.to_csv(basepath+DOWNLOADED_FILENAME, sep=',') # write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Calculate weighted average glacier flow direction using velocity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes ice velocity (vx, vy) rasters to determine each glacier of interest's weighted average flow direction. These files should be placed in the base directory (basepath). The rasters are subset using the terminus box shapefile or the Randolph Glacier Inventory outlines using a GDAL command (**gdalwarp**) with the following syntax:\n",
    "\n",
    "    gdalwarp -cutline path_to_terminusbox.shp -crop_to_cutline path_to_input_velocity.TIF path_to_output_velocity.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008.shp\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'gdalwarp -cutline /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008.shp -crop_to_cutline /home/jukes/Documents/Sample_glaciers/greenland_vel_mosaic250_vx_v1.tif /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008_greenland_vel_mosaic250_vx_v1.tif' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-0c270467adcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mv_subset1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gdalwarp -cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mterminus_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' -crop_to_cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvx_in\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvx_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mv_subset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gdalwarp -cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mterminus_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' -crop_to_cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvy_in\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvy_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_subset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_subset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'gdalwarp -cutline /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008.shp -crop_to_cutline /home/jukes/Documents/Sample_glaciers/greenland_vel_mosaic250_vx_v1.tif /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008_greenland_vel_mosaic250_vx_v1.tif' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "# Change to your velocity input file names and paths\n",
    "vx_name = 'greenland_vel_mosaic250_vx_v1.tif' # MEaSUREs product\n",
    "vy_name = 'greenland_vel_mosaic250_vy_v1.tif' # MEaSUREs product\n",
    "vpath = basepath # path to folder containing velocity files\n",
    "no_data_val = -2000000000.0 # no data value for the velocity maps\n",
    "vt = 'day' # time unit for velocity (e.g., day for m/day, year for m/year)\n",
    "######################################################################################\n",
    "\n",
    "for BoxID in BoxIDs:\n",
    "    terminus_path = basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+\".shp\"  # path to RGI shapefile\n",
    "\n",
    "    if not os.path.exists(terminus_path): # if the RGI shapefile does not exist\n",
    "        terminus_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"  # set the path to the box shapefile instead   \n",
    "    \n",
    "    print(terminus_path)\n",
    "        \n",
    "    # output paths for the cropped velocity data\n",
    "    vx_out = terminus_path[:-4]+'_'+vx_name\n",
    "    vy_out = terminus_path[:-4]+'_'+vy_name\n",
    "    \n",
    "    # input paths:\n",
    "    vx_in = basepath+vx_name\n",
    "    vy_in = basepath+vy_name\n",
    "    \n",
    "    # subset x and y velocity files\n",
    "    v_subset1 = 'gdalwarp -cutline '+terminus_path+' -crop_to_cutline '+vx_in+\" \"+vx_out\n",
    "    v_subset2 = 'gdalwarp -cutline '+terminus_path+' -crop_to_cutline '+vy_in+\" \"+vy_out\n",
    "    subprocess.run(v_subset1, shell=True, check=True)\n",
    "    subprocess.run(v_subset2, shell=True, check=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID+' done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, these subset velocity rasters are opened using the **rasterio** package and read into arrays. They are filtered for anomalous values and the velocity magnitudes are converted into weights. Then the **numpy.average()** function is used to calculated the weighted average flow directions where the flow directions of the pixels where the highest velocities are found are weighted more. \n",
    "\n",
    "The resulting average flow direction will be representative of the glacier's main flow. These directions will be used to rotate the images of the glaciers so that their flow is due right.\n",
    "\n",
    "__For slow-moving glaciers with uncertain velocities from feature tracking based velocity datasets, use manual determination of velocities. Here, we use the manual delineations of the Greenland peripheral glaciers in 2000 and 2015 to approximate the flow direction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['301', '289', '283', '265', '241', '223', '285', '181', '097', '091', '067', '083', '221', '173', '113', '101', '089', '082', '100', '112', '118', '130', '160', '196', '208', '226', '256', '262', '280', '298', '322', '072', '074', '080', '082', '084', '102', '114', '134', '132', '144', '159', '188', '189', '198', '207', '212', '222', '224', '234', '242', '243', '249', '254', '258', '264', '267', '272', '273', '278', '282', '284', '288', '297', '305', '306', '307', '315', '318', '321', '324', '327', '330', '331', '338', '341', '344', '354', '356', '357', '358', '359', '362', '363', '364', '369', '370', '371', '372', '373', '374', '376', '377', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '404', '405', '406', '407', '408', '409', '410', '414', '415', '416', '417', '418', '419', '420', '421', '422', '427', '430', '431', '434', '436', '438', '440']\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "# ONLY APPLIES TO GREENLAND PERIPHERAL GLACIERS\n",
    "badvelocities = ['301', '289', '283', '265', '241', '223', '285', '181', '097', '091', '067','083',\n",
    "                 '221', '173', '113', '101', '089', '082', '100', '112', '118', '130', '160', '196', \n",
    "                 '208', '226', '256', '262', '280', '298', '322', '072', '074', '080', '082', '084',\n",
    "                '102', '114', '134', '132', '144', '159', '188', '189', '198', '207', '212', '222',\n",
    "                '224', '234', '242', '243', '249', '254', '258', '264', '267', '272', '273', '278', \n",
    "                '282', '284', '288', '297', '305', '306', '307', '315', '318', '321', '324', '327',\n",
    "                '330', '331', '338', '341', '344', '354', '356', '357', '358', '359', '362', '363',\n",
    "                 '364', '369', '370', '371', '372', '373', '374', '376', '377', '379', '380', '381', \n",
    "                 '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393',\n",
    "                 '394', '395', '396', '397', '398', '399', '400', '401' ,'404', '405', '406', '407',\n",
    "                 '408', '409', '410', '414', '415', '416', '417', '418', '419', '420', '421', '422',\n",
    "                 '427', '430', '431', '434', '436', '438', '440']\n",
    "print(badvelocities)\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "/home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008_greenland_vel_mosaic250_vx_v1.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_shim.pyx\u001b[0m in \u001b[0;36mrasterio._shim.open_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008_greenland_vel_mosaic250_vx_v1.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-29c835a570c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgi_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if yes, open those files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mvx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Box\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/RGI_Box\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvx_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mvy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Box\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/RGI_Box\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvy_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if not, they were subset using the boxes. Open those files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             s = get_writer_for_path(path, driver=driver)(\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: /home/jukes/Documents/Sample_glaciers/Box008/RGI_Box008_greenland_vel_mosaic250_vx_v1.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "boxes = []; avg_rot = []; max_mag = []; num_cells = [] # to hold the boxIDs, rotation angle, max. glacier speed, and number of pixels\n",
    "\n",
    "for BoxID in BoxIDs:\n",
    "    rot_angles = []; max_magnitudes = [] # store angles and speeds from all pixels\n",
    "    \n",
    "    # determine if RGI outline was used to subset velocities\n",
    "    rgi_exists = 0\n",
    "    for file in os.listdir(basepath+\"Box\"+BoxID):\n",
    "        if file.startswith('RGI'):\n",
    "            rgi_exists = 1\n",
    "            \n",
    "    if rgi_exists == 1: # if yes, open those files    \n",
    "        vx = rio.open(basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+'_'+vx_name, \"r\") \n",
    "        vy = rio.open(basepath+\"Box\"+BoxID+\"/RGI_Box\"+BoxID+'_'+vy_name, \"r\") \n",
    "    else: # if not, they were subset using the boxes. Open those files\n",
    "        vx = rio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vx_name, \"r\") \n",
    "        vy = rio.open(basepath+\"Box\"+BoxID+\"/Box\"+BoxID+'_'+vy_name, \"r\") \n",
    "    vx_array = vx.read(); vy_array = vy.read() # read as numpy array\n",
    "    \n",
    "    # remove no data values\n",
    "    vx_masked = vx_array[vx_array != no_data_val]\n",
    "    vy_masked = vy_array[vy_array != no_data_val]\n",
    "    \n",
    "    # calculate flow direction\n",
    "    direction = np.arctan2(vy_masked, vx_masked)*180/np.pi \n",
    "    # transform so any negative angles are placed on 0 to 360 scale:\n",
    "    if len(direction[direction < 0]) > 0:\n",
    "        direction[direction < 0] = 360.0+direction[direction < 0]\n",
    "    \n",
    "    # calculate speed (flow magnitude)\n",
    "    magnitude = np.sqrt((vx_masked*vx_masked) + (vy_masked*vy_masked)) \n",
    "    \n",
    "    ncells = len(direction) # number of pixels\n",
    "    if ncells > 0:\n",
    "        # Determine if there are a large number of direction pixels with values > 200.0\n",
    "        # If so, it's probably pointing East\n",
    "        dir_range = direction.max() - direction.min()\n",
    "        if dir_range > 200.0 and len(direction[direction > 200]): # if large range and values above 200\n",
    "            direction[direction > 180] = direction[direction > 180] - 360.0 # transform those values on a negative scale\n",
    "            # calculate weights (0 - 1) from magnitudes\n",
    "            mag_range = magnitude.max() - magnitude.min()\n",
    "            stretch = 1/mag_range; weights = stretch*(magnitude - magnitude.min()) # weights for averaging\n",
    "            avg_dir = np.average(direction, weights=weights) # calculate average flow direction\n",
    "            if avg_dir < 0: # if negative:\n",
    "                avg_dir = avg_dir + 360.0 # transform back to 0 to 360 scale\n",
    "        else:\n",
    "            mag_range = magnitude.max() - magnitude.min(); stretch = 1/mag_range\n",
    "            weights = stretch*(magnitude - magnitude.min())\n",
    "            avg_dir = np.average(direction, weights=weights)\n",
    "                \n",
    "        if vt == 'day':\n",
    "            max_magnitude = magnitude.max()\n",
    "        elif vt == 'year':\n",
    "            yr_day_conv = 0.00273973 # conversion to m/d from m/a\n",
    "            max_magnitude = magnitude.max()*yr_day_conv  \n",
    "    else: # no velocity pixels remaining once cropped\n",
    "        avg_dir = np.NaN ; max_magnitude = np.NaN # no velocities to calculate this with\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    # ONLY APPLIES TO GREENLAND PERIPHERAL GLACIERS, COMMENT OUT FOR OTHER APPLICATIONS\n",
    "    path2000_2015 = '/phoebekinzelman/research/greenland/2000_2015/'\n",
    "    if BoxID in badvelocities:\n",
    "        # grab the 2000 and 2015 delineation centroids:\n",
    "        shp2000 = fiona.open(path2000_2015+'GreenlandPeriph_term2000_'+BoxID+'.shp'); feat2000= shp2000.next()\n",
    "        lineshp2000 = LineString(feat2000['geometry']['coordinates'])\n",
    "        cent2000 = np.array(lineshp2000.centroid)\n",
    "\n",
    "        shp2015 = fiona.open(path2000_2015+'GreenlandPeriph_term2015_'+BoxID+'.shp'); feat2015= shp2015.next()\n",
    "        lineshp2015 = LineString(feat2015['geometry']['coordinates'])\n",
    "        cent2015 = np.array(lineshp2015.centroid)\n",
    "\n",
    "        # grab displacements and use to calculate flow direction in degrees\n",
    "        y = cent2000[1] - cent2015[1]\n",
    "        x = cent2000[0] - cent2015[0]\n",
    "        avg_dir = np.arctan2(y,x)*180/np.pi\n",
    "        if avg_dir < 0:\n",
    "            avg_dir = 360.0+avg_dir\n",
    "         \n",
    "        # if max_magnitude cannot be calculated from the velocity raster (pixels == 0)\n",
    "        if ncells == 0:\n",
    "            # use displacements and time to approximate speed in m/d\n",
    "            yrs = 15.0\n",
    "            max_magnitude = np.sqrt((y*y)+(x*x))/yrs*yr_day_conv\n",
    "                \n",
    "        ncells = np.NaN\n",
    "    ##################################################################################################################\n",
    "    \n",
    "    # Append values to lists:\n",
    "    avg_rot.append(avg_dir); max_mag.append(max_magnitude); boxes.append(BoxID); num_cells.append(ncells)  \n",
    "\n",
    "# store the flow direction (rotation angle), maximum magnitude\n",
    "velocities_df = pd.DataFrame(list(zip(boxes,avg_rot, max_mag, num_cells)), columns=['BoxID','Flow_dir', 'Max_speed', 'Pixels'])\n",
    "velocities_df = velocities_df.sort_values(by='BoxID')\n",
    "velocities_df # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write velocity data to CSV\n",
    "VEL_FILENAME = 'Glacier_vel'+csvext \n",
    "velocities_df.to_csv(path_or_buf = basepath+VEL_FILENAME, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Rotate all images by flow direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the glacier velocity file as velocities_df if not already loaded:\n",
    "velocities_df = pd.read_csv(basepath+VEL_FILENAME, sep=',', dtype=str, usecols=[1,2,3,4])\n",
    "velocities_df = velocities_df.set_index('BoxID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists.\n"
     ]
    }
   ],
   "source": [
    "# make directory for rotated images in BoxID folders if it doesn't already exist\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    if os.path.exists(downloadpath+\"Box\"+BoxID+'/rotated_c1/'):\n",
    "        print(\"Already exists.\")\n",
    "    else:\n",
    "        os.mkdir(downloadpath+\"Box\"+BoxID+'/rotated_c1/')\n",
    "        print(\"Folder made for Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move rasterized terminus box into reprojected folder, since it will also need to be rotated:\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    boxfile = 'Box'+BoxID+'_raster_cut.TIF'\n",
    "    boxrasterpath = basepath+'Box'+BoxID+'/'+boxfile\n",
    "    newpath = downloadpath+'Box'+BoxID+'/reprojected/'+boxfile\n",
    "    shutil.copyfile(boxrasterpath, newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /Users/phoebekinzelman/research/greenland/LS8aws/Box008/reprojected/; magick mogrify -format png *.TIF\n",
      "madeit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mogrify: Unknown field with tag 33550 (0x830e) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 42113 (0xa481) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 33922 (0x8482) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34735 (0x87af) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: Unknown field with tag 34737 (0x87b1) encountered. `TIFFReadDirectory' @ warning/tiff.c/TIFFWarnings/959.\n",
      "mogrify: TooManyExceptions (exception processing is suspended) @ warning/exception.c/ThrowException/1049.\n"
     ]
    }
   ],
   "source": [
    "# convert all images to png for rotation:\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/reprojected/; '+'magick mogrify -format png *.TIF'\n",
    "    subprocess.run(command, shell=True,check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008\n",
      "LC08_L1TP_031005_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_028006_2021_B8_Buffer008_PS.png\n",
      "Box008_raster_cut.png\n",
      "LC08_L1TP_028006_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2019_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2018_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031006_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_027006_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2017_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2016_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2015_B8_Buffer008_PS.png\n",
      "LC08_L1TP_029006_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2014_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_030006_2020_B8_Buffer008_PS.png\n",
      "LC08_L1TP_031005_2013_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2021_B8_Buffer008_PS.png\n",
      "LC08_L1TP_032005_2020_B8_Buffer008_PS.png\n"
     ]
    }
   ],
   "source": [
    "# rotate\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index; print(\"Box\"+BoxID) # keep track of progress\n",
    "    for file in os.listdir(downloadpath+\"Box\"+BoxID+'/reprojected/'):\n",
    "        if file.endswith('.png'):\n",
    "            print(file)\n",
    "            img  = Image.open(downloadpath+\"Box\"+BoxID+'/reprojected/'+file)\n",
    "            rotated = img.rotate(-float(row['Flow_dir']))\n",
    "            rotated.save(downloadpath+\"Box\"+BoxID+'/rotated_c1/R_'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Crop all images to the same size\n",
    "\n",
    "All input images will need to be the same size for the automated terminus detection analysis. The function resize_pngs resizes all png files within a folder to the minimum image dimensions found. The function crops around the edges, centering the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for BoxID in BoxIDs:\n",
    "    resizepath = downloadpath+\"Box\"+BoxID+'/rotated_c1/' # path to rotated images\n",
    "    resize_pngs(resizepath) # crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all final files to pgm\n",
    "for index, row in velocities_df.iterrows():\n",
    "    BoxID = index\n",
    "    command = 'cd '+downloadpath+'Box'+BoxID+'/rotated_c1/; '+'mogrify -depth 16 -format pgm *.png'\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename the rasterized terminus box files if necessary\n",
    "# for BoxID in BoxIDs:\n",
    "#     files = os.listdir(downloadpath+'Box'+BoxID+'/rotated_c1/')\n",
    "#     for file in files:\n",
    "#         if file.startswith('R_Box'+BoxID+'_cut'):\n",
    "#             rpath = downloadpath+'Box'+BoxID+'/rotated_c1/'\n",
    "#             os.rename(rpath+file, rpath+'R_Box'+BoxID+'_raster_cut'+file[-4:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
