{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenland peripherial glacier terminus image processing\n",
    "\n",
    "### Jukes Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import packages, set base path, set glaciers of interest by BoxID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "#SET basepath to your own folder\n",
    "basepath='/home/jukes/Documents/Sample_glaciers/'\n",
    "\n",
    "#ENTER list of glaciers of interest by BoxID\n",
    "#make this into a widget where you can enter them in?\n",
    "BOXIDS = ['001', '002', '004', '033', '120', '174', '235', '259', '277', '531'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create buffer zone around terminus boxes and rasterize/subset terminus boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code pulls the buffer distances around the terminus boxes from an existing .csv file with the exported attributes tables for the peripheral glacier terminus boxes. These buffer distances will be used to create a buffer zone to subset the Landsat scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001: 1766 meters\n",
      "Box002: 6741 meters\n",
      "Box004: 7112 meters\n",
      "Box033: 5424 meters\n",
      "Box120: 1720 meters\n",
      "Box174: 3729 meters\n",
      "Box235: 2620 meters\n",
      "Box259: 6974 meters\n",
      "Box277: 2273 meters\n",
      "Box531: 2657 meters\n"
     ]
    }
   ],
   "source": [
    "#PULL buffer distances around terminus boxes from a csv file of attributes\n",
    "df=pd.read_csv(basepath+'Boxes_attributes.csv', sep=','); #reads in csv file \n",
    "df_buffdist = df[\"Buff_dist\"].copy(); #creates a dataframe of the buffer distances\n",
    "\n",
    "df_boxid = df[\"BoxID\"].copy(); #creates a data frame of the BoxIDs\n",
    "df_b = pd.concat([df_buffdist, df_boxid], axis=1); #concatenates the two columns\n",
    "\n",
    "#CREATE dictionary of buffer distances with BoxID as the key\n",
    "bd = df_b.set_index('BoxID').T.to_dict('list'); #turns the concatenated df into a dictionary\n",
    "\n",
    "#PRINT buffer distances for the glaciers of interest\n",
    "for BoxID in BOXIDS: \n",
    "    bd_key = int(BoxID)\n",
    "    buff_dist = str(int(bd[bd_key][0]))\n",
    "    print(\"Box\"+BoxID+\":\", buff_dist, \"meters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section creates a buffer zone using GDAL command **ogr2ogr** with the following syntax:\n",
    "\n",
    "    ogr2ogr Buffer###.shp path_to_terminusbox###.shp  -dialect sqlite -sql \"SELECT ST_Buffer(geometry, buffer_distance) AS geometry,*FROM 'Box###'\" -f \"ESRI Shapefile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001\n",
      "Box002\n",
      "Box004\n",
      "Box033\n",
      "Box120\n",
      "Box174\n",
      "Box235\n",
      "Box259\n",
      "Box277\n",
      "Box531\n"
     ]
    }
   ],
   "source": [
    "#export GDAL path command:\n",
    "export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "for BoxID in BOXIDS:\n",
    "    #SET path to the terminus box shapefiles\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"\n",
    "    outputbuffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\"\n",
    "    \n",
    "    #PULL the buffer distances as strings from the bd dictionary using the BoxID keys\n",
    "    bd_key = int(BoxID)\n",
    "    buff_dist = str(int(bd[bd_key][0]))\n",
    "    \n",
    "    #SET buffer command and print to check it\n",
    "    buffer_cmd = 'ogr2ogr '+outputbuffer_path+\" \"+terminusbox_path+' -dialect sqlite -sql \"SELECT ST_Buffer(geometry, '+buff_dist+\") AS geometry,*FROM 'Box\"+BoxID+\"'\"+'\" -f \"ESRI Shapefile\"'\n",
    "    #print(export_GDALpath, buffer_cmd)\n",
    "    \n",
    "    subprocess.call(export_GDALpath+buffer_cmd, shell=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminus box shapefiles are then rasterized (to be used as a mask during the WTMM filering) using the GDAL **gdal_rasterize** command and subset to the buffer zone using the GDAL **gdalwarp** command using the following syntax:\n",
    "\n",
    "1) Rasterize\n",
    "\n",
    "    gdal_rasterize -burn 1.0 -tr x_resolution y_resolution -a_nodata 0.0 path_to_terminusbox.shp path_to_terminusbox_raster.TIF\n",
    "\n",
    "The x_resolution and y_resolution are set to be 15.0 (meters) to match the Landsat B8 resolution.\n",
    "    \n",
    "2) Subset\n",
    "\n",
    "    gdalwarp -cutline path_to_Buffer###.shp -crop_to_cutline path_to_terminusbox_raster.TIF path_to_subset_raster_cut.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001\n",
      "Box002\n",
      "Box004\n",
      "Box033\n",
      "Box120\n",
      "Box174\n",
      "Box235\n",
      "Box259\n",
      "Box277\n",
      "Box531\n"
     ]
    }
   ],
   "source": [
    "#export GDAL path command:\n",
    "export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "for BoxID in BOXIDS:\n",
    "    #SET path to the terminus box shapefiles\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"\n",
    "    buffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\"\n",
    "    \n",
    "    #output raster path:\n",
    "    terminusraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".TIF\"\n",
    "    cutraster_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\"_cut.TIF\"\n",
    "    \n",
    "    #SET commands and print to check\n",
    "    rasterize_cmd = 'gdal_rasterize -burn 1.0 -tr 15.0 15.0 -a_nodata 0.0 '+terminusbox_path+' '+terminusraster_path\n",
    "    subsetbuffer_cmd = 'gdalwarp -cutline '+buffer_path+' -crop_to_cutline '+terminusraster_path+\" \"+cutraster_path\n",
    "    #print(export_GDALpath+rasterize_cmd)\n",
    "    #print(export_GDALpath+subsetbuffer_cmd)\n",
    "    \n",
    "    #RASTERIZE & SUBSET\n",
    "    subprocess.call(export_GDALpath+rasterize_cmd, shell=True)\n",
    "    subprocess.call(export_GDALpath+subsetbuffer_cmd, shell=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Subset downloaded Landsat scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoxID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>034</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>033</th>\n",
       "      <td>008</td>\n",
       "      <td>014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>232</td>\n",
       "      <td>017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>232</td>\n",
       "      <td>017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>011</td>\n",
       "      <td>002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Path  Row\n",
       "BoxID          \n",
       "001    034  005\n",
       "002    031  005\n",
       "004    031  005\n",
       "033    008  014\n",
       "120    232  017\n",
       "174    232  017\n",
       "235    232  015\n",
       "259    232  015\n",
       "277    232  015\n",
       "531    011  002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#READ in the Landsat path and row information for the glaciers\n",
    "pathrows_df = pd.read_csv(basepath+'LS_pathrows.csv', sep=',', usecols =[0,1,2], dtype=str, nrows =10)\n",
    "pathrows_df = pathrows_df.set_index('BoxID')\n",
    "pathrows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section of code accesses the Landsat 8 scenes downloaded and stored in Path###_Row### folders to 1) reproject them into Greenland Polar Stereographic Coordinates and 2) subset them using the buffer zone shapefiles created from the previous step. The two GDAL commands (**gdalwarp**) use the following syntax:\n",
    "\n",
    "1) Reproject\n",
    "\n",
    "    gdalwarp -t_srs ‘+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m’ path_to_input_image.TIF path_to_renamed_output.TIF\n",
    "\n",
    "2) Subset\n",
    "\n",
    "    gdalwarp -cutline _path_to_Buffer###.shp -crop_to_cutline path_to_input_image.TIF path_to_renamed_output.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001 EXISTS ALREADY. SKIP.\n",
      "LC80340052014215LGN00_B8.TIF\n",
      "LC80340052013148LGN00_B8.TIF\n",
      "LC80340052016125LGN00_B8.TIF\n",
      "LC80340052014183LGN00_B8.TIF\n",
      "LC80340052014135LGN00_B8.TIF\n",
      "LC80340052016157LGN00_B8.TIF\n",
      "LC80340052015106LGN00_B8.TIF\n",
      "LC80340052015170LGN00_B8.TIF\n",
      "LC80340052014247LGN00_B8.TIF\n",
      "LC80340052015202LGN00_B8.TIF\n",
      "LC80340052015266LGN00_B8.TIF\n",
      "LC80340052017079LGN00_B8.TIF\n",
      "LC80340052014087LGN00_B8.TIF\n",
      "LC80340052017095LGN00_B8.TIF\n",
      "LC80340052014231LGN00_B8.TIF\n",
      "LC80340052014151LGN00_B8.TIF\n",
      "LC80340052016141LGN00_B8.TIF\n",
      "LC80340052016205LGN00_B8.TIF\n",
      "LC80340052014263LGN00_B8.TIF\n",
      "LC80340052016189LGN00_B8.TIF\n",
      "LC80340052015122LGN00_B8.TIF\n",
      "LC80340052014167LGN00_B8.TIF\n",
      "LC80340052015090LGN00_B8.TIF\n",
      "LC80340052016237LGN00_B8.TIF\n",
      "LC80340052016221LGN00_B8.TIF\n",
      "LC80340052015250LGN00_B8.TIF\n",
      "LC80340052016061LGN00_B8.TIF\n",
      "LC80340052015234LGN00_B8.TIF\n",
      "LC80340052016077LGN00_B8.TIF\n",
      "LC80340052015218LGN00_B8.TIF\n",
      "LC80340052014103LGN00_B8.TIF\n",
      "LC80340052015282LGN00_B8.TIF\n",
      "LC80340052015138LGN00_B8.TIF\n",
      "LC80340052016285LGN00_B8.TIF\n",
      "LC80340052013260LGN00_B8.TIF\n",
      "LC80340052016253LGN00_B8.TIF\n",
      "LC80340052016109LGN00_B8.TIF\n",
      "LC80340052013244LGN00_B8.TIF\n",
      "LC80340052017111LGN00_B8.TIF\n",
      "LC80340052015154LGN00_B8.TIF\n",
      "LC80340052015074LGN00_B8.TIF\n",
      "LC80340052015186LGN00_B8.TIF\n",
      "LC80340052017063LGN00_B8.TIF\n",
      "LC80340052016173LGN00_B8.TIF\n",
      "Box001 done\n",
      "Box002 EXISTS ALREADY. SKIP.\n",
      "Box002 done\n",
      "Box004 EXISTS ALREADY. SKIP.\n",
      "Box004 done\n",
      "Box033 EXISTS ALREADY. SKIP.\n",
      "Box033 done\n",
      "Box120 EXISTS ALREADY. SKIP.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-53e093e95e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0msubsetbuffer_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gdalwarp -cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbuffer_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' -crop_to_cutline '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_toimages\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_B8_PS.TIF \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubsetout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_B8_PS_Buffer\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".TIF\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#             print(export_GDALpath+subsetbuffer_cmd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_GDALpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubsetbuffer_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Box\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBoxID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \"\"\"\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1451\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1454\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set path to LS8 scenes\n",
    "path_toscenedirectory = '/media/jukes/jukes1/LS8aws/'\n",
    "\n",
    "\n",
    "#export GDAL path command:\n",
    "export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "for BoxID in BOXIDS:\n",
    "    #set flag for projection\n",
    "    projected = False\n",
    "    \n",
    "    #SELECT folder with LS8 scenes for each glacier by Path and Row folder name\n",
    "    path = pathrows_df.loc[BoxID, 'Path']\n",
    "    row = pathrows_df.loc[BoxID, 'Row']\n",
    "    foldername = \"Path\"+path+\"_Row\"+row\n",
    "    path_toscenes = path_toscenedirectory+foldername+\"/\"\n",
    "#     print(path_toscenes)\n",
    "    \n",
    "    #set path to buffer zone\n",
    "    buffer_path = basepath+\"Box\"+BoxID+\"/Buffer\"+BoxID+\".shp\"\n",
    "    \n",
    "    #set output path for subset images\n",
    "    subsetout = path_toscenedirectory+'Box'+BoxID+'/'\n",
    "#     print(subsetout)\n",
    "    #create a BoxID folder within the scene directory (Path row folder)\n",
    "    if os.path.exists(subsetout):\n",
    "        print('Box'+BoxID+\" EXISTS ALREADY. SKIP.\")\n",
    "    else:\n",
    "        os.mkdir(subsetout)\n",
    "        print('Box'+BoxID+\" directory made\")\n",
    "    \n",
    "    \n",
    "    #STEPS for each image:\n",
    "    for scene in os.listdir(path_toscenes):\n",
    "        #go to files in each scene folder\n",
    "        path_toimages = path_toscenes+scene+'/'\n",
    "#         print(path_toimages)\n",
    "        \n",
    "#         for image in os.listdir(path_toimages):\n",
    "            #for each B8 image\n",
    "#             if image.endswith('B8_PS.TIF'):\n",
    "                # if reprojected image is detected, it's already been reprojected, flip the flag\n",
    "                #projected = True\n",
    "        \n",
    "        #only reproject if the images have not already been reprojected\n",
    "#         if projected == False:\n",
    "        if BoxID == '001':\n",
    "            for image in os.listdir(path_toimages):\n",
    "                if image.endswith('B8.TIF'):\n",
    "                    print(image)\n",
    "        \n",
    "                    #REPROJECT into Greenland Polar Stereographic if it hasn't already been projected\n",
    "                    reproject_cmd = \"gdalwarp -t_srs '+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m' \"+path_toimages+image+\" \"+path_toimages+scene+\"_B8_PS.TIF\"\n",
    "#                     print(export_GDALpath+reproject_cmd)\n",
    "                    subprocess.call(export_GDALpath+reproject_cmd, shell=True)\n",
    "\n",
    "        if projected == True:\n",
    "                print('images have already been reprojected for this folder')\n",
    "        \n",
    "        #SUBSET LS8 scenes to buffer zones around the terminus box\n",
    "        for image in os.listdir(path_toimages):\n",
    "            #save subsetted scenes as PGM files\n",
    "            subsetbuffer_cmd = 'gdalwarp -cutline '+buffer_path+' -crop_to_cutline '+path_toimages+scene+\"_B8_PS.TIF \"+subsetout+scene+\"_B8_PS_Buffer\"+BoxID+\".TIF\"\n",
    "#             print(export_GDALpath+subsetbuffer_cmd)\n",
    "            subprocess.call(export_GDALpath+subsetbuffer_cmd, shell=True)\n",
    "        \n",
    "    print(\"Box\"+BoxID, \"done\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calculate average flow direction (weighted by magnitude) for each glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes 2016-2017 ice velocity data from the ESA Cryoportal to determine each glacier of interest's weighted average flow direction. The ice velocity direction (calculated from yx velocity) and the velocity magnitude at each glacier's terminus  is subset using the terminus box shapefile using a GDAL command (**gdalwarp**) with the following syntax:\n",
    "\n",
    "    gdalwarp -cutline path_to_terminusbox.shp -crop_to_cutline path_to_input_velocity.TIF path_to_output_velocity_at_term###.TIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001\n",
      "Box002\n",
      "Box004\n",
      "Box033\n",
      "Box120\n",
      "Box174\n",
      "Box235\n",
      "Box259\n",
      "Box277\n",
      "Box531\n"
     ]
    }
   ],
   "source": [
    "#export GDAL path command:\n",
    "export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "for BoxID in BOXIDS:\n",
    "    #SET paths to the terminus box shapefiles and velocity data\n",
    "    terminusbox_path = basepath+\"Box\"+BoxID+\"/Box\"+BoxID+\".shp\"\n",
    "    v_dir_path = basepath+'dir_degree_yx_velocity.tif'\n",
    "    v_mag_path = basepath+'magnitude_velocity.tif'  \n",
    "    \n",
    "    #SET output paths\n",
    "    v_dir_output = basepath+\"Box\"+BoxID+\"/dir_degree_yx_velocity_at_term\"+BoxID+\".tif\"\n",
    "    v_mag_output = basepath+\"Box\"+BoxID+\"/magnitude_velocity_at_term\"+BoxID+\".tif\"\n",
    "    \n",
    "    #SET velocity subset commands and print to check it\n",
    "    v_subset_dir_cmd = 'gdalwarp -cutline '+terminusbox_path+' -crop_to_cutline '+v_dir_path+\" \"+v_dir_output\n",
    "    v_subset_mag_cmd = 'gdalwarp -cutline '+terminusbox_path+' -crop_to_cutline '+v_mag_path+\" \"+v_mag_output\n",
    "    #print(export_GDALpath+v_subset_dir_cmd)\n",
    "    #print(export_GDALpath+v_subset_mag_cmd)\n",
    "    \n",
    "    #SUBSET velocity rasters\n",
    "    subprocess.call(export_GDALpath+v_subset_dir_cmd, shell=True)\n",
    "    subprocess.call(export_GDALpath+v_subset_mag_cmd, shell=True)\n",
    "    \n",
    "    print(\"Box\"+BoxID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, these subset velocity rasters are opened using the **rasterio** package and read into arrays. They are filtered for anomalous values and the velocity magnitudes are converted into weights. Then the **numpy.average()** function is used to calculated the weighted average flow directions where the flow directions of the pixels where the highest velocities are found are weighted more. \n",
    "\n",
    "The resulting average flow direction will be representative of the glacier's main flow. These directions will be used to rotate the images of the glaciers so that their flow is due right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'001': 56.284283, '002': 155.98723, '004': -3.4834337, '033': 142.11816, '120': -77.3864, '174': 12.677643, '235': -145.85077, '259': 98.999275, '277': -65.120186, '531': 78.83521}\n",
      "{'001': 0.04377438, '002': 3.5832264, '004': 0.62308246, '033': 0.77165776, '120': 0.27788857, '174': 0.91450316, '235': 0.15709679, '259': 3.0749009, '277': 0.2860196, '531': 0.040784776}\n"
     ]
    }
   ],
   "source": [
    "#CREATE list of glacier average flow directions:\n",
    "rot_angles = {}\n",
    "max_magnitudes = {}\n",
    "\n",
    "for BoxID in BOXIDS :    \n",
    "        #READ velocity direction and magnitude data at terminus for each glacier into an array\n",
    "        direction = rasterio.open(basepath+'Box'+BoxID+'/dir_degree_yx_velocity_at_term'+BoxID+'.tif', \"r\")\n",
    "        dir_array = direction.read()\n",
    "        #print(dir_array.shape)\n",
    "        magnitude = rasterio.open(basepath+'Box'+BoxID+'/magnitude_velocity_at_term'+BoxID+'.tif', \"r\")\n",
    "        mag_array = magnitude.read()\n",
    "        #print(mag_array.shape)\n",
    "        \n",
    "        \n",
    "        #RESHAPE direction array and remove anomalous values\n",
    "        dir2 = dir_array.reshape(dir_array.shape[1] * dir_array.shape[2])\n",
    "        #direction must be between 180 and -180 degrees\n",
    "        mask_dir2 = (dir2 < 180) & (dir2 > -180)\n",
    "        masked_dir = dir2[mask_dir2]\n",
    "        #print(masked_dir.min(), masked_dir.max())\n",
    "\n",
    "        #RESHAPE magnitude array and remove anomalous values\n",
    "        mag2 = mag_array.reshape(mag_array.shape[1] * mag_array.shape[2])\n",
    "        #magnitude must be between 0 and 10 m/d\n",
    "        mask_mag2 = (mag2 < 100) & (mag2 > 0)\n",
    "        masked_mag = mag2[mask_mag2]\n",
    "        #print(masked_mag.min(), masked_mag.max())\n",
    "        \n",
    "    \n",
    "        #CALCULATE weights (0 - 1) from magnitudes\n",
    "        mag_range = masked_mag.max() - masked_mag.min()\n",
    "        stretch = 1/mag_range\n",
    "        weights = stretch*(masked_mag - masked_mag.min())\n",
    "        #print(weights.min(), weights.max()) #should be between 0 and 1\n",
    "        #print(weights.shape, masked_dir.shape)\n",
    "        \n",
    "        \n",
    "        #CALCULATE the weighted average rotation angle\n",
    "        avg_dir = np.average(masked_dir, weights=weights)\n",
    "        #print(avg_dir)\n",
    "        \n",
    "        #APPEND the rotation angles to the dictionary\n",
    "        rot_angles.update( {BoxID: avg_dir} )\n",
    "        \n",
    "        #APPEND the maximum flow magnitude\n",
    "        max_magnitudes.update( {BoxID: masked_mag.max()} )\n",
    "\n",
    "\n",
    "print(rot_angles)\n",
    "print(max_magnitudes)\n",
    "\n",
    "#EXPORT TO CSV for rotations???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Call FIJI scripts to perform rotations & resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN Rotate_LS.ijm\n",
    "\n",
    "# RUN resize.ijm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Call Tcl scripts to perform 2D WTMM and filter for terminus line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN parameters_gaussian.tcl\n",
    "#RUN scr_gaussian.tcl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Call post-processing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
