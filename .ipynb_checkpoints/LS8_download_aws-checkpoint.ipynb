{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to download LS8 images for Greenland peripheral glaciers using Amazon Web Services (aws) \n",
    "\n",
    "### Jukes Liu\n",
    "\n",
    "The following code automatically downloads Landsat 8 scenes available through Amazon Web Services that have less than a threshold % of cloud cover. The Landsat 8 scenes over each glacier are identified using their pre-determined path and row, stored in a .csv file. The scenes are filtered for cloud cover using their metadata files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Set up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install AWS using pip or pip3\n",
    "\n",
    "Must have Amazon Web Services installed on your terminal. Follow instructions at https://docs.aws.amazon.com/cli/latest/userguide/install-linux-al2017.html to get aws commands onto your shell terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in LS path and row for each peripheral glacier by BoxID into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LS Path and Row information for each peripheral glacier is stored in a .csv file. \n",
    "\n",
    "Note: Many glaciers exist in the same Landsat scene, so some Paths and Rows are repeated. Therefore, the subsequent code will not repeat download for a path and row combination that already exists in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Row</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoxID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>034</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>031</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>033</th>\n",
       "      <td>008</td>\n",
       "      <td>014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>232</td>\n",
       "      <td>017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>232</td>\n",
       "      <td>017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>232</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>011</td>\n",
       "      <td>002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Path  Row\n",
       "BoxID          \n",
       "001    034  005\n",
       "002    031  005\n",
       "004    031  005\n",
       "033    008  014\n",
       "120    232  017\n",
       "174    232  017\n",
       "235    232  015\n",
       "259    232  015\n",
       "277    232  015\n",
       "531    011  002"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set basepath\n",
    "basepath = '/home/jukes/Documents/Sample_glaciers/'\n",
    "#basepath = '/home/automated-glacier-terminus/'\n",
    "outputpath = '/media/jukes/jukes1/'\n",
    "\n",
    "#read the path row csv file into a dataframe\n",
    "pathrows_df = pd.read_csv(basepath+'LS_pathrows.csv', sep=',', usecols =[0,1,2], dtype=str, nrows =10)\n",
    "pathrows_df = pathrows_df.set_index('BoxID')\n",
    "pathrows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the df dimensions\n",
    "pathrows_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create output directory: LS8aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists already\n"
     ]
    }
   ],
   "source": [
    "#create LS8aws folder\n",
    "if os.path.exists(outputpath+'LS8aws')==True:\n",
    "    print(\"Path exists already\")\n",
    "else:\n",
    "    os.mkdir(outputpath+'LS8aws')\n",
    "    print(\"LS8aws directory made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download B8 (panchromatic band) and MTL.txt (metadata) files for all available images over the path/row of the glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Landsat8 scenes stored in AWS can be accessed using the landsat-pds bucket and the path and row information. Each of the bands and a metadata file can be accessed separately. \n",
    "\n",
    "We are interested in the panchromatic band (B8.TIF) and the metadata file to filter for cloud cover (MTL.txt). The download commands will use the following syntax:\n",
    "\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/L8/path/row/LC8pathrowyear001LGN00/LC8pathrowyear001LGN00_MTL.txt /path_to/output/\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/L8/path/row/LC8pathrowyear001LGN00/LC8pathrowyear001LGN00_B8.TIF /path_to/output/\n",
    "\n",
    "Access https://docs.opendata.aws/landsat-pds/readme.html to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A) For one BoxID (one glacier) at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoxID  001 path 034 row 005\n",
      "Path034_Row005\n",
      "s3://landsat-pds/L8/034/005/\n",
      "/media/jukes/jukes1/LS8aws/Path034_Row005/\n"
     ]
    }
   ],
   "source": [
    "#choose a glacier: Box001\n",
    "BoxID = pathrows_df.index[0]\n",
    "path = pathrows_df['Path'][0]\n",
    "row = pathrows_df['Row'][0] \n",
    "print('BoxID ', BoxID, 'path', path, 'row', row)\n",
    "\n",
    "#set path row folder name\n",
    "folder_name = 'Path'+path+'_Row'+row\n",
    "print(folder_name)\n",
    "\n",
    "#set input path\n",
    "bp_in = 's3://landsat-pds/L8/'\n",
    "totalp_in = bp_in+path+'/'+row+'/'\n",
    "print(totalp_in)\n",
    "\n",
    "#set output path\n",
    "bp_out = outputpath+'LS8aws/'+folder_name+'/'\n",
    "print(bp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Path_Row folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path034_Row005  EXISTS ALREADY. SKIP.\n"
     ]
    }
   ],
   "source": [
    "#create Path_row folder and write path names to txt files\n",
    "if os.path.exists(bp_out):\n",
    "    print(folder_name, \" EXISTS ALREADY. SKIP.\")\n",
    "else:\n",
    "    os.mkdir(bp_out)\n",
    "    print(folder_name+\" directory made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all the metadata text files using os.system aws commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following syntax:\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/L8/031/005/ Output/path/LS8aws/Path031_Row005/ --recursive --exclude \"*\" --include \"*.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check command syntax:\n",
    "# command = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*.txt\"'\n",
    "# print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #call the command line that downloads the metadata files using aws\n",
    "# subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download BQA band to get cloud mask using os.system aws commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following syntax:\n",
    "\n",
    "    aws --no-sign-request s3 cp s3://landsat-pds/L8/031/005/ Output/path/LS8aws/Path031_Row005/ --recursive --exclude \"*\" --include \"*BQA.TIF\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws --no-sign-request s3 cp s3://landsat-pds/L8/034/005/ /media/jukes/jukes1/LS8aws/Path034_Row005/ --recursive --exclude \"*\" --include \"*BQA.TIF\"\n"
     ]
    }
   ],
   "source": [
    "#Check command syntax:\n",
    "command2 = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*BQA.TIF\"'\n",
    "print(command2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the command line that downloads the BQA files using aws\n",
    "subprocess.call(command2, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1) Filter for cloud cover using metadata cloud cover percentage\n",
    "\n",
    "If the metadata files indicate land cloud cover is less than the threshold, then download the B8, otherwise, delete the folder. Not all metadata files contain the land cloud cover, some only contain the overall cloud cover. If land cloud cover is not found, use the cloud cover value to determine whether the image should be downloaded .Use the following metadata attributes:\n",
    "\n",
    "  GROUP = IMAGE_ATTRIBUTES\n",
    "  \n",
    "    CLOUD_COVER = 23.58\n",
    "    CLOUD_COVER_LAND = 20.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cloud cover % thresholds and paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set cloud cover % thresholds\n",
    "# ccland_thresh = 30.0\n",
    "# cc_thresh = 50.0\n",
    "\n",
    "# #set paths:\n",
    "# #set path row folder name\n",
    "# folder_name = 'Path'+path+'_Row'+row\n",
    "# print(folder_name)\n",
    "\n",
    "# #set input path\n",
    "# bp_in = 's3://landsat-pds/L8/'\n",
    "# totalp_in = bp_in+path+'/'+row+'/'\n",
    "# print(totalp_in)\n",
    "\n",
    "# #set output path\n",
    "# bp_out = outputpath+'LS8aws/'+folder_name+'/'\n",
    "# print(bp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop, filter, download all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #loop through all the metadata files in the path_row folder:\n",
    "# for image in os.listdir(bp_out):\n",
    "#     if image.startswith(\"LC\"):\n",
    "#         #list the name of the image folder\n",
    "#         print(image)\n",
    "        \n",
    "#         #open the metadata file within that folder\n",
    "#         mdata = open(bp_out+image+\"/\"+image+\"_MTL.txt\", \"r\")\n",
    "        \n",
    "#         #set a detection variable for whether or not the metadata contains land cloud cover\n",
    "#         ccl_detected = False\n",
    "        \n",
    "#         #loop through each line in metadata to find Land Cloud Cover\n",
    "#         for line in mdata:\n",
    "#             cc_variable = line.split(\"=\")[0]\n",
    "            \n",
    "#             #if there is land cloud cover:\n",
    "#             if (\"CLOUD_COVER_LAND\" in cc_variable):\n",
    "#                 #save it:\n",
    "#                 ccl = np.float(line.split(\"=\")[1])\n",
    "                         \n",
    "#                 #switch the ccl_detected variable to True!\n",
    "#                 ccl_detected = True\n",
    "                    \n",
    "#                 #if the ccl is less than the threshold, delete the file\n",
    "#                 if ccl > ccland_thresh:\n",
    "#                     #remove the image directory\n",
    "#                     subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "#                     print(ccl, ' > ', ccland_thresh, \", \", image, \"removed\")\n",
    "#                 #otherwise: \n",
    "#                 else:\n",
    "#                     #DOWNLOAD THE B8 FILE\n",
    "#                     subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "#                     print(image, \"B8 downloaded -ccl \")\n",
    "        \n",
    "#         #Was the ccl detected?\n",
    "#         print(\"CCL detected = \", ccl_detected)\n",
    "                        \n",
    "#         #if False,use the overall cloud cover:\n",
    "#         if ccl_detected == False:   \n",
    "#             print(\"CCL not detected, use CC.\")\n",
    "            \n",
    "#             #open the metadata file again\n",
    "#             mdata = open(bp_out+image+\"/\"+image+\"_MTL.txt\", \"r\")\n",
    "#             for line in mdata:\n",
    "#                 variable = line.split(\"=\")[0]\n",
    "                \n",
    "#                 #now there should only be one line starting with cloud_cover\n",
    "#                 if (\"CLOUD_COVER\" in variable):       \n",
    "#                     #save the cloud cover:\n",
    "#                     cc = np.float(line.split(\"=\")[1])\n",
    "\n",
    "#                     #if the cc is less than the threshold, delete the file:\n",
    "#                     if cc > cc_thresh:\n",
    "#                         #remove the image directory\n",
    "#                         subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "#                         print(cc, ' > ', cc_thresh, \", \", image, \"removed\")\n",
    "\n",
    "#                     #otherwise: \n",
    "#                     else:\n",
    "#                         #DOWNLOAD THE B8 FILE\n",
    "#                         subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "#                         print(image, \"B8 downloaded -cc\")\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2) Filter for cloud cover using BQA cloud info\n",
    "\n",
    "The Landsat-8 BQA band includes a basic classification of surface types. These classes include water, snow/ice, and cloud. The values for cloud pixels are higher than others, > 50,000 in the 16-bit images. Therefore, we calculate the percentage of cloud pixels in each glacier's terminus box. If the percentage is above the threshold, we remove the directory and do not download the image. If the percentage is lower than the threshold, then we download the panchromatic image.\n",
    "\n",
    "Set cloud percentage threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpercent_thresh = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load terminus box shapefile for subsetting and load the raster to count total number of pixels within the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jukes/Documents/Sample_glaciers/Box001/Box001.shp\n",
      "/home/jukes/Documents/Sample_glaciers/Box001/Box001_raster_cut.png\n"
     ]
    }
   ],
   "source": [
    "#set path to terminus box shp\n",
    "boxpath = basepath+'Box'+BoxID+'/Box'+BoxID+'.shp'\n",
    "print(boxpath)\n",
    "\n",
    "#set path to terminus box raster\n",
    "boxrasterpath = basepath+'Box'+BoxID+'/Box'+BoxID+'_raster_cut.png'\n",
    "print(boxrasterpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pixels in the raster: 3023\n"
     ]
    }
   ],
   "source": [
    "#load in box raster and count where raster == 1 using np.count_nonzero\n",
    "#read the raster into an ndarray\n",
    "termbox = mpimg.imread(boxrasterpath)\n",
    "termplot = plt.imshow(termbox)\n",
    "plt.show()\n",
    "\n",
    "totalpixels = np.count_nonzero(termbox)\n",
    "#print(termbox.shape[0]*termbox.shape[1])\n",
    "print('Total number of pixels in the raster:',totalpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-project and subset the BQA band to the glacier BOI terminus box using GDAL commands with syntax:\n",
    "\n",
    "1) Reproject into Greenland Polar Stereographic projection\n",
    "\n",
    "    gdalwarp -t_srs ‘+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m’ path_to_input_BQA.TIF path_to_renamed_output_BQA.TIF\n",
    "\n",
    "2) Subset\n",
    "\n",
    "    gdalwarp -cutline path_to_Box###.shp -crop_to_cutline path_to_BQA.TIF path_to_result_BQA.TIF\n",
    "\n",
    "Then calculate the number of cloud pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC80340052014215LGN00_BQA.TIF\n",
      "Cloud % in box: 6.45054581541515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-c86da8f39fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;31m#DOWNLOAD THE B8 FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source activate aws; aws --no-sign-request s3 cp '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotalp_in\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbp_out\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' --recursive --exclude \"*\" --include \"*B8.TIF\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"B8 downloaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#export GDAL path command:\n",
    "export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "#loop through all the metadata files in the path_row folder:\n",
    "for image in os.listdir(bp_out):\n",
    "    if image.startswith(\"LC\"):\n",
    "        #list the name of the image folder\n",
    "#         print(image)\n",
    "\n",
    "        #set the path to the BQA band\n",
    "        bandpath = bp_out+image+'/'\n",
    "        \n",
    "        for band in os.listdir(bandpath):\n",
    "            if band.endswith(\"BQA.TIF\"):\n",
    "                print(band)\n",
    "                \n",
    "                bandpath = bandpath+band\n",
    "                rp_bandpath = bandpath[:-4]+\"_PS.TIF\"\n",
    "                cut_rp_bandpath = rp_bandpath[:-4]+\"_Box\"+BoxID+\".TIF\"\n",
    "#                 print(bandpath)\n",
    "#                 print(rp_bandpath)\n",
    "#                 print(cut_rp_bandpath)\n",
    "                \n",
    "#                 #reproject the band:\n",
    "#                 reproject_cmd = \"gdalwarp -t_srs '+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m' \"+bandpath+\" \"+rp_bandpath\n",
    "# #                 print(export_GDALpath+reproject_cmd)\n",
    "#                 subprocess.call(export_GDALpath+reproject_cmd, shell=True)\n",
    "#                 print('BQA reprojected')\n",
    "\n",
    "#                 #subset the reprojected BQA band to terminus box:\n",
    "#                 subsetBQA_cmd = 'gdalwarp -cutline '+boxpath+' -crop_to_cutline '+rp_bandpath+\" \"+cut_rp_bandpath\n",
    "# #                 print(export_GDALpath+subsetBQA_cmd)\n",
    "#                 subprocess.call(export_GDALpath+subsetBQA_cmd, shell=True)\n",
    "#                 print('BQA subset')\n",
    "        \n",
    "                #load the subset BQA band and mask out any values that are less than 50000 (clouds)\n",
    "                subsetBQA = mpimg.imread(cut_rp_bandpath)\n",
    "                cloudBQA = subsetBQA[subsetBQA >= 50000]\n",
    "#                 print(cloudBQA)\n",
    "                \n",
    "                #count the number of cloud pixels\n",
    "                cloudpixels = len(cloudBQA)\n",
    "#                 print(cloudpixels)\n",
    "                \n",
    "                #calculate divided by the total\n",
    "                cloudpercent = cloudpixels/totalpixels*100\n",
    "                print('Cloud % in box:', cloudpercent)\n",
    "                \n",
    "                #if statement cloud percent is greater than the threshold, delete the folder:\n",
    "                if cloudpercent > cpercent_thresh:\n",
    "                    #remove the image directory\n",
    "                    subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "                    print(cloudpercent, ' > ', cpercent_thresh, \", \", image, \"removed\")\n",
    "                #otherwise: \n",
    "                else:\n",
    "                    #DOWNLOAD THE B8 FILE\n",
    "                    subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "                    print(image, \"B8 downloaded\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B) For ALL glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Metadata cloud cover % threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #SET cloud cover thresholds for filtering\n",
    "# ccland_thresh = 30.0\n",
    "# cc_thresh = 50.0\n",
    "\n",
    "\n",
    "\n",
    "# #LOOP through each of the glaciers in the DataFrame and download for each path and row\n",
    "# for i in range(0, len(pathrows_df.index)):\n",
    "#     #SET path and row variables to the LS path and rows of the box\n",
    "#     path = pathrows_df['Path'][i]\n",
    "#     row = pathrows_df['Row'][i]\n",
    "#     #print(path, row)\n",
    "    \n",
    "#     #1) CREATE path and row folders to download into and set input output paths\n",
    "#     #SET path row folder name\n",
    "#     folder_name = 'Path'+path+'_Row'+row\n",
    "#     print(folder_name)\n",
    "    \n",
    "#     #SET input path\n",
    "#     bp_in = 's3://landsat-pds/L8/'\n",
    "#     totalp_in = bp_in+path+'/'+row+'/'\n",
    "#     #print(totalp_in)\n",
    "\n",
    "#     #SET output path\n",
    "#     bp_out = outputpath+'LS8aws/'+folder_name+'/'\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #IF the folder exists, it's already been downloaded, do not attempt download.\n",
    "#     if os.path.exists(bp_out):\n",
    "#         print(folder_name, \" EXISTS ALREADY. SKIP.\")\n",
    "#     #2) OTHERWISE, create the folder and download into it\n",
    "#     else:\n",
    "#         os.mkdir(bp_out)\n",
    "#         print(folder_name+\" directory made\")\n",
    "\n",
    "        \n",
    "#         #3) DOWNLOAD metadata files into the new path-row folder\n",
    "#         #CHECK COMMAND SYNTAX\n",
    "#         command = 'source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*.txt\"'\n",
    "#         #print(command)\n",
    "#         subprocess.call(command, shell=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         #4) LOOP through all the files in the path_row folder to download based on cloud cover\n",
    "#         # in the metadata files\n",
    "#         for image in os.listdir(bp_out):\n",
    "#             if image.startswith(\"LC\"):\n",
    "#                 #list the name of the image folder\n",
    "#                 print(image)\n",
    "\n",
    "#                 #open the metadata file within that folder\n",
    "#                 mdata = open(bp_out+image+\"/\"+image+\"_MTL.txt\", \"r\")\n",
    "\n",
    "#                 #set a detection variable for whether or not the metadata contains land cloud cover\n",
    "#                 ccl_detected = False\n",
    "\n",
    "#                 #loop through each line in metadata to find the land cloud cover\n",
    "#                 for line in mdata:\n",
    "#                     cc_variable = line.split(\"=\")[0]\n",
    "\n",
    "#                     #if there is land cloud cover:\n",
    "#                     if (\"CLOUD_COVER_LAND\" in cc_variable):\n",
    "#                         #save it:\n",
    "#                         ccl = np.float(line.split(\"=\")[1])\n",
    "\n",
    "#                         #switch the ccl detected variable to True\n",
    "#                         ccl_detected = True\n",
    "\n",
    "#                         #if the ccl is less than the threshold, delete the file\n",
    "#                         if ccl > ccland_thresh:\n",
    "#                             #remove the image directory\n",
    "#                             #subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "#                             print(ccl, ' > ', ccland_thresh, \", \", image, \"removed\")\n",
    "#                         #otherwise: \n",
    "#                         else:\n",
    "#                             #download the B8 file\n",
    "#                             subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "#                             print(image, \"B8 downloaded -ccl \")\n",
    "\n",
    "#                 print(\"CCL detected = \", ccl_detected)\n",
    "\n",
    "#                 #if False,use the overall cloud cover:\n",
    "#                 if ccl_detected == False:   \n",
    "#                     print(\"CCL not detected, use CC.\")\n",
    "\n",
    "#                     #open the metadata file again\n",
    "#                     mdata = open(bp_out+image+\"/\"+image+\"_MTL.txt\", \"r\")\n",
    "#                     for line in mdata:\n",
    "#                         variable = line.split(\"=\")[0]\n",
    "\n",
    "#                         #now there should only be one line starting with cloud_cover\n",
    "#                         if (\"CLOUD_COVER\" in variable):       \n",
    "#                             #save the cloud cover:\n",
    "#                             cc = np.float(line.split(\"=\")[1])\n",
    "\n",
    "#                             #if the cc is less than the threshold, delete the file:\n",
    "#                             if cc > cc_thresh:\n",
    "#                                 #remove the image directory\n",
    "#                                 #subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "#                                 print(cc, ' > ', cc_thresh, \", \", image, \"removed\")\n",
    "\n",
    "#                             #otherwise: \n",
    "#                             else:\n",
    "#                                 #DOWNLOAD THE B8 FILE\n",
    "#                                 subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "#                                 print(image, \"B8 downloaded -cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: BQA cloud cover threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADm9JREFUeJzt3X+spFV9x/H3p7AsVbSwVci6bArabSImdSE3iLFprNSC/LOYlGb5QzeGZE2LiSb+s9ik2qQk2lRJTFrtGohrY0Xqj7AxtBS3NMY/BFaKCGyRValcd8PWikhrugX89o85C8Ny2Tv3zsydu/e8X8lknufMmZnvPMlnznmenb0nVYWkPvzKrAuQtHIMvNQRAy91xMBLHTHwUkcMvNSRqQU+yeVJHk5yMMmuab2PpNFlGv8On+QU4HvA24F54B7g6qp6aOJvJmlk0xrhLwYOVtUPqur/gJuBbVN6L0kjOnVKr7sJeGxofx5400t1Pi3r63RePqVSpLXtKZ74SVW9epS+0wp8Fmh7wblDkp3AToDTeRlvyqVTKkVa275eX/qPUftOa0o/D2we2j8XODTcoap2V9VcVc2tY/2UypA0bFqBvwfYkuT8JKcB24G9U3ovSSOaypS+qp5J8j7gduAU4KaqenAa7yVpdNM6h6eqbgNum9brS1o6f2kndcTASx0x8FJHDLzUEQMvdcTASx0x8FJHDLzUEQMvdcTASx0x8FJHDLzUEQMvdcTASx0x8FJHDLzUEQMvdcTASx0x8FJHDLzUEQMvdcTASx0x8FJHDLzUEQMvdWSslWeSPAo8BTwLPFNVc0k2AF8EzgMeBf6oqp4Yr0xJkzCJEf73qmprVc21/V3AvqraAuxr+5JWgWlM6bcBe9r2HuDKKbyHpGUYN/AF/HOSbyfZ2drOqarDAO3+7IWemGRnkv1J9j/N0THLkDSKcVePfUtVHUpyNnBHkn8f9YlVtRvYDfDKbKgx65A0grFG+Ko61O6PAF8FLgYeT7IRoN0fGbdISZOx7MAneXmSVxzbBv4AeADYC+xo3XYAt45bpKTJGGdKfw7w1STHXufvq+qfktwD3JLkGuBHwFXjlylpEpYd+Kr6AfDGBdr/C7h0nKIkTYe/tJM6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6smjgk9yU5EiSB4baNiS5I8kj7f6s1p4kn0xyMMn9SS6aZvGSlmaUEf6zwOXHte0C9lXVFmBf2wd4B7Cl3XYCn5pMmZImYdHAV9U3gJ8e17wN2NO29wBXDrV/rga+BZx5bOloSbO33HP4c6rqMEC7P7u1bwIeG+o339peJMnOJPuT7H+ao8ssQ9JSTPqiXRZoq4U6VtXuqpqrqrl1rJ9wGZIWstzAP35sqt7uj7T2eWDzUL9zgUPLL0/SJC038HuBHW17B3DrUPu729X6S4Anj039Jc3eqYt1SPIF4K3Aq5LMAx8GPgrckuQa4EfAVa37bcAVwEHgF8B7plCzpGVaNPBVdfVLPHTpAn0LuHbcoiRNh7+0kzpi4KWOGHipIwZe6oiBlzpi4KWOGHipI4v+O7z6dfuh+16wf9lrts6oEk2KI7xGdvwXgE4+jvBakuHQO+KffBzhtaBRRnNH/JOPI7xeZClBdsQ/uTjCa2Ic8Vc/R3hNlCP+6uYIrxeY5CjtiL/6OMLrOdMIqCP+6uIIrxXjiD97jvBaUY74s+UIL2A2o68j/soz8DJ4HTHwUkcMvGbGc/iVZ+A753S+Lwa+Y7MMu6P7bCwa+CQ3JTmS5IGhto8k+XGS+9rtiqHHrktyMMnDSS6bVuGSlm6UEf6zwOULtN9QVVvb7TaAJBcA24E3tOf8TZJTJlWsJsepfJ8WDXxVfQP46Yivtw24uaqOVtUPGawxd/EY9WkNcjo/O+Ocw78vyf1tyn9Wa9sEPDbUZ761SYBhn7XlBv5TwOuArcBh4OOtPQv0rYVeIMnOJPuT7H+ao8ssQ8vhdL5fywp8VT1eVc9W1S+Bz/D8tH0e2DzU9Vzg0Eu8xu6qmququXWsX04ZWgbD3rdlBT7JxqHddwLHruDvBbYnWZ/kfGALcPd4JWqtcDo/e4v+b7kkXwDeCrwqyTzwYeCtSbYymK4/CrwXoKoeTHIL8BDwDHBtVT07ndK1VP67uxYNfFVdvUDzjSfofz1w/ThFafKcygv8pZ3UFQOvqXM6v3oY+A547q5jDPwa57m7hhl4qSP+EUtNhVP51ckRfg1zOq/jGfg1yrBrIQZeE+d0fvUy8Joow766Gfg1yOm8XoqBX2P8kY1OxMBLHTHwUkcM/BridF6LMfAam2E/eRj4NcIr8xqFgZc6YuA1FqfzJxcDv0Zc9pqtKx4+w37yMfBrzCyCr5OHgV+jDL4WYuDXuGkF3y+Tk5OB78Qkg2/YT14GvjNO9fu2aOCTbE5yZ5IDSR5M8v7WviHJHUkeafdntfYk+WSSg2056Yum/SG0dAa/T6OM8M8AH6yq1wOXANcmuQDYBeyrqi3AvrYP8A4Gi0huAXYyWFpaq9Sx4I8afr8kTm6LBr6qDlfVvW37KeAAsAnYBuxp3fYAV7btbcDnauBbwJnHrTark5RhP/kt6c9UJzkPuBC4Czinqg7D4Eshydmt2ybgsaGnzbe2w+MWq+kaDrS/zV+bRr5ol+QM4MvAB6rq5yfqukBbLfB6O5PsT7L/aY6OWoZWiKP52jTSCJ9kHYOwf76qvtKaH0+ysY3uG4EjrX0e2Dz09HOBQ8e/ZlXtBnYDvDIbXvSFoNkz9GvPKFfpw2A9+ANV9Ymhh/YCO9r2DuDWofZ3t6v1lwBPHpv6S5qtUUb4twDvAr6b5NiJ3YeAjwK3JLkG+BFwVXvsNuAK4CDwC+A9E61Y0rItGviq+iYLn5cDXLpA/wKuHbMuSVPgL+2kjhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljhh4qSMGXuqIgZc6YuCljoyyttzmJHcmOZDkwSTvb+0fSfLjJPe12xVDz7kuycEkDye5bJofQNLoRllb7hngg1V1b5JXAN9Ockd77Iaq+qvhzkkuALYDbwBeA3w9yW9V1bOTLFzS0i06wlfV4aq6t20/BRwANp3gKduAm6vqaFX9kMGikhdPolhJ41nSOXyS84ALgbta0/uS3J/kpiRntbZNwGNDT5vnxF8QklbIyIFPcgbwZeADVfVz4FPA64CtwGHg48e6LvD0WuD1dibZn2T/0xxdcuGSlm6kwCdZxyDsn6+qrwBU1eNV9WxV/RL4DM9P2+eBzUNPPxc4dPxrVtXuqpqrqrl1rB/nM0ga0ShX6QPcCByoqk8MtW8c6vZO4IG2vRfYnmR9kvOBLcDdkytZ0nKNcpX+LcC7gO8mua+1fQi4OslWBtP1R4H3AlTVg0luAR5icIX/Wq/QS6vDooGvqm+y8Hn5bSd4zvXA9WPUJWkK/KWd1BEDL3XEwEsdMfBSRwy81BEDL3XEwEsdMfBSRwy81BEDL3XEwEsdMfBSRwy81BEDL3XEwEsdMfBSRwy81BEDL3XEwEsdMfBSRwy81BEDL3XEwEsdMfBSRwy81JFR1pY7PcndSb6T5MEkf97az09yV5JHknwxyWmtfX3bP9geP2+6H0HSqEYZ4Y8Cb6uqNzJYGvryJJcAHwNuqKotwBPANa3/NcATVfWbwA2tn6RVYNHA18B/t9117VbA24AvtfY9wJVte1vbpz1+aVuBVtKMjbo+/Clt5dgjwB3A94GfVdUzrcs8sKltbwIeA2iPPwn8+gKvuTPJ/iT7n+boeJ9C0khGCnxVPVtVW4FzgYuB1y/Urd0vNJrXixqqdlfVXFXNrWP9qPVKGsOSrtJX1c+AfwUuAc5Mcmy56XOBQ217HtgM0B7/NeCnkyhW0nhGuUr/6iRntu1fBX4fOADcCfxh67YDuLVt7237tMf/papeNMJLWnmnLt6FjcCeJKcw+IK4paq+luQh4OYkfwH8G3Bj638j8HdJDjIY2bdPoW5Jy7Bo4KvqfuDCBdp/wOB8/vj2/wWumkh1kibKX9pJHTHwUkcMvNQRAy91xMBLHTHwUkcMvNQRAy91xMBLHclq+Jl7kv8E/gf4yaxrWSVehcdimMfjeQsdi9+oqleP8uRVEXiAJPuram7WdawGHosX8ng8b9xj4ZRe6oiBlzqymgK/e9YFrCIeixfyeDxvrGOxas7hJU3fahrhJU3ZzAOf5PIkD7eFK3bNup6VkOSmJEeSPDDUtiHJHW1hjzuSnNXak+ST7fjcn+Si2VU+eUk2J7kzyYG20Mn7W3t3x2NFFn2pqpndgFMY/Mnr1wKnAd8BLphlTSv0uX8XuAh4YKjtL4FdbXsX8LG2fQXwjwz+GvAlwF2zrn/Cx2IjcFHbfgXwPeCCHo9H+0xntO11wF3tM94CbG/tnwb+uG3/CfDptr0d+OKi7zHjD/hm4Pah/euA62Z94Ffos593XOAfBja27Y3Aw237b4GrF+q3Fm8M/hjq23s/HsDLgHuBNzH4oc2prf25zAC3A29u26e2fjnR6856Sv/cohXN8IIWvTmnqg4DtPuzW3s3x6hNSS9kMLJ1eTymsejLsFkHfqRFKzrXxTFKcgbwZeADVfXzE3VdoG3NHI+awqIvw2Yd+OcWrWiGF7TozeNJNgK0+yOtfc0foyTrGIT981X1ldbc7fGA6S36MuvA3wNsaVchT2Nw4WHvjGualeEFPI5f2OPd7er0JcCTx6a6a0FbaPRG4EBVfWLooe6Ox4os+rIKLk5cweDK7PeBP511PSv0mb8AHAaeZvAtfQ2Dc699wCPtfkPrG+Cv2/H5LjA36/onfCx+h8E09H7gvna7osfjAfw2g0Vd7gceAP6stb8WuBs4CPwDsL61n972D7bHX7vYe/hLO6kjs57SS1pBBl7qiIGXOmLgpY4YeKkjBl7qiIGXOmLgpY78PzJyY3Ky1ankAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pixels in the raster: 3023\n",
      "/media/jukes/jukes1/LS8aws/Box001/LC80340052014215LGN00/\n",
      "LC80340052014215LGN00_BQA.TIF\n",
      "BQA reprojected\n",
      "BQA subset\n",
      "Cloud % in box: 6.45054581541515\n"
     ]
    }
   ],
   "source": [
    "#SET cloud cover threshold\n",
    "cpercent_thresh = 20.0\n",
    "\n",
    "\n",
    "#LOOP through each of the glaciers in the DataFrame and download for each path and row\n",
    "for i in range(0, len(pathrows_df.index)):\n",
    "    #SET path and row variables to the LS path and rows of the box\n",
    "    path = pathrows_df['Path'][i]\n",
    "    row = pathrows_df['Row'][i]\n",
    "    BoxID = pathrows_df.index[i]\n",
    "#     print(path, row, BoxID)\n",
    "    \n",
    "    #1) CREATE path and row folders to download into and set input output paths\n",
    "    #SET path row folder name\n",
    "    folder_name = 'Path'+path+'_Row'+row\n",
    "#     print(folder_name)\n",
    "    \n",
    "    #SET input path\n",
    "    bp_in = 's3://landsat-pds/L8/'\n",
    "    totalp_in = bp_in+path+'/'+row+'/'\n",
    "    #print(totalp_in)\n",
    "\n",
    "    #SET output path\n",
    "    bp_out = outputpath+'LS8aws/Box'+BoxID+'/'\n",
    "    \n",
    "    #Download all the metadata files\n",
    "    # #Check command syntax:\n",
    "    dwn_command = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*.txt\"'\n",
    "#     print(dwn_command)\n",
    "    subprocess.call(dwn_command, shell=True)\n",
    "    \n",
    "     \n",
    "    #IF the folder exists, it's already been downloaded, do not attempt download.\n",
    "    if os.path.exists(bp_out):\n",
    "#         print(folder_name, \" EXISTS ALREADY. SKIP.\")\n",
    "#     #2) OTHERWISE, create the folder and download into it\n",
    "#     else:\n",
    "#         os.mkdir(bp_out)\n",
    "#         print(folder_name+\" directory made\")\n",
    "\n",
    "        \n",
    "        #3) DOWNLOAD BQA files into the new path-row folder\n",
    "        #CHECK COMMAND SYNTAX\n",
    "        command2 = 'aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*BQA.TIF\"'\n",
    "#         print(command2)\n",
    "        subprocess.call(command2, shell=True)\n",
    "        \n",
    "                \n",
    "        #4) CALCULATE CLOUD COVER IN TERMINUS BOX\n",
    "        #set path to terminus box shp\n",
    "        boxpath = basepath+'Box'+BoxID+'/Box'+BoxID+'.shp'\n",
    "#         print(boxpath)\n",
    "\n",
    "        #set path to terminus box raster\n",
    "        boxrasterpath = basepath+'Box'+BoxID+'/Box'+BoxID+'_raster_cut.png'\n",
    "#         print(boxrasterpath)\n",
    "                \n",
    "        #load in box raster and count where raster == 1 using np.count_nonzero\n",
    "        #read the raster into an ndarray\n",
    "        termbox = mpimg.imread(boxrasterpath)\n",
    "        termplot = plt.imshow(termbox)\n",
    "        plt.show()\n",
    "\n",
    "        totalpixels = np.count_nonzero(termbox)\n",
    "        #print(termbox.shape[0]*termbox.shape[1])\n",
    "        print('Total number of pixels in the raster:',totalpixels)\n",
    "        \n",
    "        #export GDAL path command:\n",
    "        export_GDALpath = 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH ; '\n",
    "\n",
    "        #loop through all the BQA files in the path_row folder:\n",
    "        for image in os.listdir(bp_out):\n",
    "            if image.endswith(\"00\"):\n",
    "                #list the name of the image folder\n",
    "        #         print(image)\n",
    "\n",
    "                #set the path to the BQA band\n",
    "                bandpath = bp_out+image+'/'\n",
    "                print(bandpath)\n",
    "\n",
    "                for band in os.listdir(bandpath):\n",
    "                    if band.endswith(\"BQA.TIF\"):\n",
    "                        print(band)\n",
    "\n",
    "                        bandpath = bandpath+band\n",
    "                        rp_bandpath = bandpath[:-4]+\"_PS.TIF\"\n",
    "                        cut_rp_bandpath = rp_bandpath[:-4]+\"_Box\"+BoxID+\".TIF\"\n",
    "        #                 print(bandpath)\n",
    "        #                 print(rp_bandpath)\n",
    "        #                 print(cut_rp_bandpath)\n",
    "\n",
    "        #                 #reproject the band:\n",
    "                        reproject_cmd = \"gdalwarp -t_srs '+proj=stere +lat_ts=70 +lat_0=90 +lon_0=-45 +y=0 +x=0 +k=1 +datum=WGS84 +units=m' \"+bandpath+\" \"+rp_bandpath\n",
    "        #                 print(export_GDALpath+reproject_cmd)\n",
    "                        subprocess.call(export_GDALpath+reproject_cmd, shell=True)\n",
    "                        print('BQA reprojected')\n",
    "\n",
    "                        #subset the reprojected BQA band to terminus box:\n",
    "                        subsetBQA_cmd = 'gdalwarp -cutline '+boxpath+' -crop_to_cutline '+rp_bandpath+\" \"+cut_rp_bandpath\n",
    "        #                 print(export_GDALpath+subsetBQA_cmd)\n",
    "                        subprocess.call(export_GDALpath+subsetBQA_cmd, shell=True)\n",
    "                        print('BQA subset')\n",
    "\n",
    "                        #load the subset BQA band and mask out any values that are less than 50000 (clouds)\n",
    "                        subsetBQA = mpimg.imread(cut_rp_bandpath)\n",
    "                        cloudBQA = subsetBQA[subsetBQA >= 50000]\n",
    "        #                 print(cloudBQA)\n",
    "\n",
    "                        #count the number of cloud pixels\n",
    "                        cloudpixels = len(cloudBQA)\n",
    "        #                 print(cloudpixels)\n",
    "\n",
    "                        #calculate divided by the total\n",
    "                        cloudpercent = cloudpixels/totalpixels*100\n",
    "                        print('Cloud % in box:', cloudpercent)\n",
    "                \n",
    "                        #5) IF cloud percent is greater than the threshold, delete the folder:\n",
    "                        if cloudpercent > cpercent_thresh:\n",
    "                            #remove the image directory\n",
    "                            subprocess.call('rm -r '+bp_out+image, shell=True)\n",
    "                            print(cloudpercent, ' > ', cpercent_thresh, \", \", image, \"removed\")\n",
    "                        #otherwise: \n",
    "                        else:\n",
    "                            #DOWNLOAD THE B8 FILE\n",
    "                            subprocess.call('source activate aws; aws --no-sign-request s3 cp '+totalp_in+' '+bp_out+' --recursive --exclude \"*\" --include \"*B8.TIF\"', shell=True)\n",
    "                            print(image, \"B8 downloaded\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Grab image dates from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340052014215</th>\n",
       "      <td>2014-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052013148</th>\n",
       "      <td>2013-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016125</th>\n",
       "      <td>2016-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014183</th>\n",
       "      <td>2014-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014135</th>\n",
       "      <td>2014-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016157</th>\n",
       "      <td>2016-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015106</th>\n",
       "      <td>2015-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016093</th>\n",
       "      <td>2016-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015170</th>\n",
       "      <td>2015-06-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014247</th>\n",
       "      <td>2014-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015202</th>\n",
       "      <td>2015-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015266</th>\n",
       "      <td>2015-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052017079</th>\n",
       "      <td>2017-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014087</th>\n",
       "      <td>2014-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052017095</th>\n",
       "      <td>2017-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014231</th>\n",
       "      <td>2014-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014151</th>\n",
       "      <td>2014-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016141</th>\n",
       "      <td>2016-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016205</th>\n",
       "      <td>2016-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014263</th>\n",
       "      <td>2014-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016189</th>\n",
       "      <td>2016-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015122</th>\n",
       "      <td>2015-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052014167</th>\n",
       "      <td>2014-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015090</th>\n",
       "      <td>2015-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016237</th>\n",
       "      <td>2016-08-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016221</th>\n",
       "      <td>2016-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015250</th>\n",
       "      <td>2015-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016061</th>\n",
       "      <td>2016-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052015234</th>\n",
       "      <td>2015-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340052016077</th>\n",
       "      <td>2016-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016092</th>\n",
       "      <td>2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022013243</th>\n",
       "      <td>2013-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016188</th>\n",
       "      <td>2016-07-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015217</th>\n",
       "      <td>2015-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022013115</th>\n",
       "      <td>2013-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015201</th>\n",
       "      <td>2015-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016220</th>\n",
       "      <td>2016-08-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015105</th>\n",
       "      <td>2015-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022014182</th>\n",
       "      <td>2014-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016252</th>\n",
       "      <td>2016-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015089</th>\n",
       "      <td>2015-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016108</th>\n",
       "      <td>2016-04-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016124</th>\n",
       "      <td>2016-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015185</th>\n",
       "      <td>2015-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022013275</th>\n",
       "      <td>2013-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016236</th>\n",
       "      <td>2016-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016076</th>\n",
       "      <td>2016-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015249</th>\n",
       "      <td>2015-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015233</th>\n",
       "      <td>2015-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015121</th>\n",
       "      <td>2015-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022014150</th>\n",
       "      <td>2014-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022013259</th>\n",
       "      <td>2013-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015137</th>\n",
       "      <td>2015-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022014262</th>\n",
       "      <td>2014-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022014198</th>\n",
       "      <td>2014-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015169</th>\n",
       "      <td>2015-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022015073</th>\n",
       "      <td>2015-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022013147</th>\n",
       "      <td>2013-05-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022016156</th>\n",
       "      <td>2016-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110022014166</th>\n",
       "      <td>2014-06-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "340052014215 2014-08-03\n",
       "340052013148 2013-05-28\n",
       "340052016125 2016-05-04\n",
       "340052014183 2014-07-02\n",
       "340052014135 2014-05-15\n",
       "340052016157 2016-06-05\n",
       "340052015106 2015-04-16\n",
       "340052016093 2016-04-02\n",
       "340052015170 2015-06-19\n",
       "340052014247 2014-09-04\n",
       "340052015202 2015-07-21\n",
       "340052015266 2015-09-23\n",
       "340052017079 2017-03-20\n",
       "340052014087 2014-03-28\n",
       "340052017095 2017-04-05\n",
       "340052014231 2014-08-19\n",
       "340052014151 2014-05-31\n",
       "340052016141 2016-05-20\n",
       "340052016205 2016-07-23\n",
       "340052014263 2014-09-20\n",
       "340052016189 2016-07-07\n",
       "340052015122 2015-05-02\n",
       "340052014167 2014-06-16\n",
       "340052015090 2015-03-31\n",
       "340052016237 2016-08-24\n",
       "340052016221 2016-08-08\n",
       "340052015250 2015-09-07\n",
       "340052016061 2016-03-01\n",
       "340052015234 2015-08-22\n",
       "340052016077 2016-03-17\n",
       "...                 ...\n",
       "110022016092 2016-04-01\n",
       "110022013243 2013-08-31\n",
       "110022016188 2016-07-06\n",
       "110022015217 2015-08-05\n",
       "110022013115 2013-04-25\n",
       "110022015201 2015-07-20\n",
       "110022016220 2016-08-07\n",
       "110022015105 2015-04-15\n",
       "110022014182 2014-07-01\n",
       "110022016252 2016-09-08\n",
       "110022015089 2015-03-30\n",
       "110022016108 2016-04-17\n",
       "110022016124 2016-05-03\n",
       "110022015185 2015-07-04\n",
       "110022013275 2013-10-02\n",
       "110022016236 2016-08-23\n",
       "110022016076 2016-03-16\n",
       "110022015249 2015-09-06\n",
       "110022015233 2015-08-21\n",
       "110022015121 2015-05-01\n",
       "110022014150 2014-05-30\n",
       "110022013259 2013-09-16\n",
       "110022015137 2015-05-17\n",
       "110022014262 2014-09-19\n",
       "110022014198 2014-07-17\n",
       "110022015169 2015-06-18\n",
       "110022015073 2015-03-14\n",
       "110022013147 2013-05-27\n",
       "110022016156 2016-06-04\n",
       "110022014166 2014-06-15\n",
       "\n",
       "[331 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#create dictionary of datetime objects for the images:\n",
    "datetime_objs = {}\n",
    "\n",
    "#LOOP through each of the glaciers in the DataFrame and download for each path and row\n",
    "for i in range(0, len(pathrows_df.index)):\n",
    "    #SET path and row variables to the LS path and rows of the box\n",
    "    path = pathrows_df['Path'][i]\n",
    "    row = pathrows_df['Row'][i]\n",
    "    BoxID = pathrows_df.index[i]\n",
    "    #print(path, row)\n",
    "    \n",
    "    #SET path row folder name\n",
    "    folder_name = 'Path'+path+'_Row'+row\n",
    "#     print(folder_name)\n",
    "    \n",
    "    #SET output path\n",
    "    bp_out = outputpath+'LS8aws/Box'+BoxID+'/'\n",
    "        \n",
    "    # LOOP through all the metadata files to grab the image dates\n",
    "    # in the metadata files\n",
    "    for scene in os.listdir(bp_out):\n",
    "        if folder_name.startswith(\"Path\") and scene.endswith(\"00\"):\n",
    "            #list the name of the image folder\n",
    "#             print(scene)\n",
    "            scenetag = scene[8:19]\n",
    "#             print(scenetag)\n",
    "            scenecount = scenecount+1\n",
    "\n",
    "            #open the metadata file within that folder\n",
    "            for file in os.listdir(bp_out+scene+\"/\"):                \n",
    "                if (\"MTL.txt\" in file):\n",
    "                    \n",
    "                    mdata = open(bp_out+scene+\"/\"+scene+\"_MTL.txt\", \"r\")\n",
    "\n",
    "                    #loop through each line in metadata to find the date and time of acquisition\n",
    "                    for line in mdata:\n",
    "                        variable = line.split(\"=\")[0]\n",
    "\n",
    "                        if (\"DATE_ACQUIRED\" in variable):\n",
    "                            #save it:\n",
    "                            date = line.split(\"=\")[1][1:-1]\n",
    "#                             print(date)\n",
    "\n",
    "                        #if (\"SCENE_CENTER_TIME\" in variable): \n",
    "                            #save it:\n",
    "                            #time = line.split(\"=\")[1][2:-2]\n",
    "                            #print(time)\n",
    "                \n",
    "                    #combine them into a datetime object\n",
    "                    datetime_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                    datetime_objs.update( {scene[4:-5]: datetime_obj} )\n",
    "                         \n",
    "datetime_df = pd.DataFrame.from_dict(datetime_objs, orient='index')\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export image dates to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.to_csv(path_or_buf = basepath+'datetags.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For download using Google instead, follow these instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gsutil: https://krstn.eu/landsat-batch-download-from-google/\n",
    "\n",
    "To access a scene for Path 124, Row 053, use this syntax:\n",
    "\n",
    "gsutil cp -n gs://earthengine-public/landsat/L8/124/053/LC81240532013107LGN01.tar.bz /landsat/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
