{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to calculate the auto and manual difference using an objective function\n",
    "\n",
    "Weighted least squares solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, functions, manual and automated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "manual_path = '/media/jukes/jukes1/Manual/'; manual_filename = 'manual_tpos_c1.csv'\n",
    "auto_path = '/home/jukes/Documents/Sample_glaciers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jukes/automated-glacier-terminus') #import necessary functions:\n",
    "from automated_terminus_functions import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxID</th>\n",
       "      <th>datetimes</th>\n",
       "      <th>Scene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>2016-03-19</td>\n",
       "      <td>LC80320052016079LGN00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>LC82330152016255LGN00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002</td>\n",
       "      <td>2014-03-28</td>\n",
       "      <td>LC80340052014087LGN00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002</td>\n",
       "      <td>2013-08-27</td>\n",
       "      <td>LC80310052013239LGN00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>LC82320182016360LGN00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BoxID   datetimes                  Scene\n",
       "0   002  2016-03-19  LC80320052016079LGN00\n",
       "1   120  2016-09-11  LC82330152016255LGN00\n",
       "2   002  2014-03-28  LC80340052014087LGN00\n",
       "3   002  2013-08-27  LC80310052013239LGN00\n",
       "4   120  2016-12-25  LC82320182016360LGN00"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MANUAL info\n",
    "condition_df = pd.read_csv(manual_path+'LS8_manual_delineation_info.csv', dtype=str)\n",
    "\n",
    "# TEST images\n",
    "test_df = pd.read_csv(manual_path+'train.csv', dtype=str, header=None)\n",
    "test_df = test_df.rename(columns={0: 'BoxID', 1: 'datetimes', 2: 'Scene'})\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine_df = condition_df.merge(test_df, how='inner', on=['datetimes', 'Scene', 'BoxID'])\n",
    "# examine_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MANUAL TERMINUS POSITIONS\n",
    "manual_df = pd.read_csv(manual_path+manual_filename, dtype=str,sep=',')\n",
    "\n",
    "#SPLIT INTO 3 DATAFRAMES FOR 3 FLOWLINES:\n",
    "manual50 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                      'tpos50']].copy().reset_index(drop=True).rename(columns={\"tpos50\": \"tpos\"})\n",
    "manual25 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                      'tpos25']].copy().reset_index(drop=True).rename(columns={\"tpos25\": \"tpos\"})\n",
    "manual75 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y',\n",
    "                                      'tpos75']].copy().reset_index(drop=True).rename(columns={\"tpos75\": \"tpos\"})\n",
    "# manual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newIDs = []\n",
    "# for item in np.array(condition_df['BoxID']):\n",
    "#     if type(item) != float:\n",
    "#         newIDs.append(item.rjust(3, '0'))\n",
    "#     else:\n",
    "#         newIDs.append('NaN')\n",
    "# condition_df['BoxID'] = newIDs \n",
    "# condition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST image conditions by condition - manual\n",
    "# merge1 = manual_df.merge(condition_df, how='inner', on=['datetimes', 'BoxID']).drop(['Unnamed: 0_x', \n",
    "#                                                                  'Unnamed: 0_y',\n",
    "#                                                                  'Line_x', 'Line_y', \n",
    "#                                                                  'Jukes', 'Not_exact_date'], axis=1)\n",
    "# merge2 = merge1.merge(test_df, how='inner', on=['datetimes', 'BoxID', 'Scene'])\n",
    "# merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at 002 specifically\n",
    "# BoxID = '002'\n",
    "# auto50 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline50_filtered.csv', dtype=str,sep=',')\n",
    "# auto50 = auto50[['BoxID','datetimes', 'Scene', 'tpos']].copy()\n",
    "# auto25 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline25_filtered.csv', dtype=str,sep=',')\n",
    "# auto25 = auto25[['BoxID','datetimes', 'Scene', 'tpos']].copy()\n",
    "# auto75 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline75_filtered.csv', dtype=str,sep=',')\n",
    "# auto75 = auto75[['BoxID','datetimes', 'Scene', 'tpos']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto75[auto75['datetimes']=='2016-07-07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxIDs = ['001', '002', '120', '174', '259']\n",
    "# BoxIDs = ['002']\n",
    "dfs = []\n",
    "\n",
    "for BoxID in BoxIDs:\n",
    "    auto50 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline50_filtered.csv', dtype=str,sep=',')\n",
    "    auto50 = auto50[['BoxID','datetimes', 'Scene', 'tpos']].copy()\n",
    "    auto25 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline25_filtered.csv', dtype=str,sep=',')\n",
    "    auto25 = auto25[['BoxID','datetimes', 'Scene', 'tpos']].copy()\n",
    "    auto75 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline75_filtered.csv', dtype=str,sep=',')\n",
    "    auto75 = auto75[['BoxID','datetimes', 'Scene', 'tpos']].copy()\n",
    "    autodfs = [auto50, auto25, auto75]\n",
    "\n",
    "#     manual = merge2[merge2.BoxID == BoxID].copy() # USE MERGE 2 IF SUBCATEGORIES\n",
    "    manual = merge1[merge1.BoxID == BoxID].copy() # USE MERGE 1 FOR FULL SET\n",
    "    manual50 = manual[['BoxID','datetimes', 'Scene', 'tpos50', 'Condition']].copy().rename(columns={\"tpos50\": \"tpos\"})\n",
    "    manual25 = manual[['BoxID','datetimes', 'Scene', 'tpos25', 'Condition']].copy().rename(columns={\"tpos25\": \"tpos\"})\n",
    "    manual75 = manual[['BoxID','datetimes', 'Scene', 'tpos75', 'Condition']].copy().rename(columns={\"tpos75\": \"tpos\"})\n",
    "    manualdfs = [manual50, manual25, manual75]\n",
    "\n",
    "    cdfs = []\n",
    "    for i in range(0, len(manualdfs)):\n",
    "        adf = autodfs[i]; mdf = manualdfs[i]\n",
    "        cdf = mdf.merge(adf, how='inner', on='datetimes')\n",
    "        cdf = cdf.astype({'tpos_x': 'float', 'tpos_y': 'float'})\n",
    "        cdf['diff'] = abs(np.array(cdf.tpos_x) - np.array(cdf.tpos_y))\n",
    "        cdfs.append(cdf)\n",
    "    dfs.append(pd.concat(cdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Clear',\n",
       " 'Clear ',\n",
       " 'Clouds',\n",
       " 'Cloudy',\n",
       " 'Cloudy, sea ice',\n",
       " 'Dim',\n",
       " 'Dim, cloudy, sea ice',\n",
       " 'Dim, sea ice ',\n",
       " 'Sea ice',\n",
       " 'Sea ice ',\n",
       " 'Shadow',\n",
       " 'Shadow ',\n",
       " 'Shadow, clear',\n",
       " 'Shadow, sea ice',\n",
       " 'Shadow, sea ice ',\n",
       " 'Shadows, sea ice',\n",
       " 'Shadows, sea ice ',\n",
       " 'Thin cloud ',\n",
       " 'Thin clouds',\n",
       " 'Thin clouds ',\n",
       " 'Thin clouds, sea ice'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare_cdf = pd.concat(dfs)\n",
    "# dates = set(compare_cdf.datetimes)\n",
    "# set(compare_cdf.Condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test\n",
    "# dimbright_df = compare_cdf[compare_cdf['Condition'] == 'Dim']\n",
    "# seaice_df = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Sea ice '], compare_cdf[compare_cdf['Condition'] == 'Sea ice']])\n",
    "# clear_df = compare_cdf[compare_cdf['Condition'] == 'Clear']\n",
    "# thinclouds_df = compare_cdf[compare_cdf['Condition'] == 'Thin clouds']\n",
    "# good_df = pd.concat([dimbright_df, clear_df, thinclouds_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #all\n",
    "# DB = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Dim'], compare_cdf[compare_cdf['Condition'] == 'Dim, sea ice ']])\n",
    "# CL = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Clear'], compare_cdf[compare_cdf['Condition'] == 'Clear ']])\n",
    "# SI = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Sea ice'], compare_cdf[compare_cdf['Condition'] == 'Sea ice  ']])\n",
    "# TC = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Thin clouds'], compare_cdf[compare_cdf['Condition'] == 'Thin clouds '], compare_cdf[compare_cdf['Condition'] == 'Thin clouds, sea ice']])\n",
    "# SH = pd.concat([compare_cdf[compare_cdf['Condition'] == 'Shadow'], compare_cdf[compare_cdf['Condition'] == 'Shadow, clear'], compare_cdf[compare_cdf['Condition'] == 'Shadow, sea ice'], compare_cdf[compare_cdf['Condition'] == 'Shadow, sea ice ']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = TC\n",
    "# df = df.reset_index(drop=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop repeated rows\n",
    "# df = df.drop([16])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-316bf390c17e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmisfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# misfit = [1080.0, 390.0, 45.0, 105.0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(misfit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(np.average(misfit))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# misfit = np.array(df['diff'])\n",
    "# # misfit = [1080.0, 390.0, 45.0, 105.0]\n",
    "# print(len(misfit))\n",
    "# # print(misfit)\n",
    "# # print(np.average(misfit))\n",
    "# print(np.median(misfit))\n",
    "# # print(np.std(misfit))\n",
    "# print(\"MAD = \", np.median(abs(misfit-np.median(misfit))))\n",
    "# # print(np.std(misfit)/np.average(misfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., 15.,  0.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# misfit-np.median(misfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SIGMAS (DATA ERRORS) ALONG EACH FLOWLINE (FROM INTERANALYST DIFFERENCES)\n",
    "# sigmas = [35.02, 27.65, 30.45]\n",
    "# sigma_avg = np.average(sigmas); print(sigma_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   BoxID_x   datetimes                Scene_x      tpos_x         Condition  \\\n",
       " 0      259  2013-08-18  LC82330152013230LGN00  892.783568           Sea ice   \n",
       " 1      259  2013-09-26  LC80010152013269LGN00  809.235751           Sea ice   \n",
       " 2      259  2013-09-28  LC82320152013271LGN00  871.872984           Sea ice   \n",
       " 3      259  2013-10-05  LC82330152013278LGN00  809.235751           Sea ice   \n",
       " 4      259  2014-05-10  LC82320152014130LGN00         NaN           Sea ice   \n",
       " 5      259  2014-05-26  LC82320152014146LGN00         NaN           Sea ice   \n",
       " 6      259  2014-07-04  LC82330152014185LGN00         NaN           Sea ice   \n",
       " 7      259  2014-08-12  LC80010152014224LGN00  838.726714           Sea ice   \n",
       " 8      259  2014-08-21  LC82330152014233LGN00  289.503886           Sea ice   \n",
       " 9      259  2014-09-06  LC82330152014249LGN00         NaN   Cloudy, sea ice   \n",
       " 10     259  2014-10-15  LC80010152014288LGN00  331.530542           Sea ice   \n",
       " 11     259  2014-11-02  LC82320152014306LGN00  301.682780  Shadows, sea ice   \n",
       " 12     259  2014-11-02  LC82320152014306LGN00         NaN  Shadows, sea ice   \n",
       " 13     259  2015-05-27  LC80010152015147LGN00         NaN           Sea ice   \n",
       " 14     259  2015-05-27  LC80010152015147LGN00  247.613610           Sea ice   \n",
       " 15     259  2015-06-14  LC82320152015165LGN00  343.529475           Sea ice   \n",
       " 16     259  2015-06-28  LC80010152015179LGN00  451.622077           Sea ice   \n",
       " 17     259  2015-07-07  LC82330152015188LGN00         NaN           Sea ice   \n",
       " 18     259  2015-07-14  LC80010152015195LGN00         NaN           Sea ice   \n",
       " 19     259  2015-07-30  LC80010152015211LGN00  239.295842           Sea ice   \n",
       " 20     259  2015-08-01  LC82320152015213LGN00  252.116045           Sea ice   \n",
       " 21     259  2015-08-08  LC82330152015220LGN00         NaN           Sea ice   \n",
       " 22     259  2015-08-17  LC82320152015229LGN00         NaN           Sea ice   \n",
       " 23     259  2015-10-02  LC80010152015275LGN00         NaN           Sea ice   \n",
       " 24     259  2015-10-02  LC80010152015275LGN00  256.539471           Sea ice   \n",
       " 25     259  2016-04-13  LC82320152016104LGN00         NaN           Sea ice   \n",
       " 26     259  2016-04-29  LC82320152016120LGN00         NaN           Sea ice   \n",
       " 27     259  2016-04-29  LC82320152016120LGN00         NaN           Sea ice   \n",
       " 28     259  2016-05-13  LC80010152016134LGN00         NaN           Sea ice   \n",
       " 29     259  2016-05-29  LC80010152016150LGN00         NaN           Sea ice   \n",
       " 30     259  2016-07-02  LC82320152016184LGN00         NaN           Sea ice   \n",
       " 31     259  2016-07-09  LC82330152016191LGN00         NaN           Sea ice   \n",
       " 32     259  2016-07-25  LC82330152016207LGN00         NaN           Sea ice   \n",
       " 33     259  2016-08-03  LC82320152016216LGN00         NaN           Sea ice   \n",
       " 34     259  2016-09-02  LC80010152016246LGN00         NaN           Sea ice   \n",
       " 35     259  2016-09-11  LC82330152016255LGN00  568.517810           Sea ice   \n",
       " 36     259  2016-09-11  LC82330152016255LGN00         NaN           Sea ice   \n",
       " 37     259  2016-09-27  LC82330152016271LGN00         NaN           Sea ice   \n",
       " 38     259  2016-10-13  LC82330152016287LGN00  289.503886   Cloudy, sea ice   \n",
       " 39     259  2016-10-13  LC82330152016287LGN00         NaN   Cloudy, sea ice   \n",
       " 40     259  2016-11-07  LC82320152016312LGN00         NaN   Shadow, sea ice   \n",
       " 41     259  2016-11-07  LC82320152016312LGN00  235.504777   Shadow, sea ice   \n",
       " \n",
       "    BoxID_y                                   Scene_y  tpos_y         diff  \n",
       " 0      259  LC08_L1TP_233015_20130818_20180426_01_T1   697.5   195.283568  \n",
       " 1      259  LC08_L1TP_001015_20130926_20170501_01_T1  1672.5   863.264249  \n",
       " 2      259  LC08_L1TP_232015_20130928_20180528_01_T1  1477.5   605.627016  \n",
       " 3      259  LC08_L1TP_233015_20131005_20170429_01_T1   592.5   216.735751  \n",
       " 4      259  LC08_L1TP_232015_20140510_20170422_01_T1   157.5          NaN  \n",
       " 5      259  LC08_L1TP_232015_20140526_20170422_01_T1   547.5          NaN  \n",
       " 6      259  LC08_L1TP_233015_20140704_20170421_01_T1   787.5          NaN  \n",
       " 7      259  LC08_L1TP_001015_20140812_20170420_01_T1  3112.5  2273.773286  \n",
       " 8      259  LC08_L1TP_233015_20140821_20170420_01_T1  1972.5  1682.996114  \n",
       " 9      259  LC08_L1TP_233015_20140906_20170419_01_T1   682.5          NaN  \n",
       " 10     259  LC08_L1TP_001015_20141015_20170418_01_T1  1867.5  1535.969458  \n",
       " 11     259  LC08_L1TP_232015_20141102_20180528_01_T1   367.5    65.817220  \n",
       " 12     259  LC08_L1TP_232015_20141102_20180528_01_T1   367.5          NaN  \n",
       " 13     259  LC08_L1TP_001015_20150527_20170408_01_T1   697.5          NaN  \n",
       " 14     259  LC08_L1TP_001015_20150527_20170408_01_T1   697.5   449.886390  \n",
       " 15     259  LC08_L1TP_232015_20150614_20170407_01_T1   262.5    81.029475  \n",
       " 16     259  LC08_L1TP_001015_20150628_20170407_01_T1   637.5   185.877923  \n",
       " 17     259  LC08_L1TP_233015_20150707_20170407_01_T1  3532.5          NaN  \n",
       " 18     259  LC08_L1TP_001015_20150714_20170407_01_T1  1642.5          NaN  \n",
       " 19     259  LC08_L1TP_001015_20150730_20170406_01_T1  2662.5  2423.204158  \n",
       " 20     259  LC08_L1TP_232015_20150801_20170406_01_T1  1357.5  1105.383955  \n",
       " 21     259  LC08_L1TP_233015_20150808_20180528_01_T1   592.5          NaN  \n",
       " 22     259  LC08_L1TP_232015_20150817_20170406_01_T1   592.5          NaN  \n",
       " 23     259  LC08_L1TP_001015_20151002_20170403_01_T1  1252.5          NaN  \n",
       " 24     259  LC08_L1TP_001015_20151002_20170403_01_T1  1252.5   995.960529  \n",
       " 25     259  LC08_L1TP_232015_20160413_20170326_01_T1   742.5          NaN  \n",
       " 26     259  LC08_L1TP_232015_20160429_20170326_01_T1  1012.5          NaN  \n",
       " 27     259  LC08_L1TP_232015_20160429_20170326_01_T1  1012.5          NaN  \n",
       " 28     259  LC08_L1TP_001015_20160513_20170324_01_T1  1162.5          NaN  \n",
       " 29     259  LC08_L1TP_001015_20160529_20180129_01_T1   667.5          NaN  \n",
       " 30     259  LC08_L1TP_232015_20160702_20170323_01_T1   367.5          NaN  \n",
       " 31     259  LC08_L1TP_233015_20160709_20170323_01_T1   592.5          NaN  \n",
       " 32     259  LC08_L1TP_233015_20160725_20170322_01_T1  1822.5          NaN  \n",
       " 33     259  LC08_L1TP_232015_20160803_20170322_01_T1   592.5          NaN  \n",
       " 34     259  LC08_L1TP_001015_20160902_20170321_01_T1   397.5          NaN  \n",
       " 35     259  LC08_L1TP_233015_20160911_20170321_01_T1   577.5     8.982190  \n",
       " 36     259  LC08_L1TP_233015_20160911_20170321_01_T1   577.5          NaN  \n",
       " 37     259  LC08_L1TP_233015_20160927_20170320_01_T1   577.5          NaN  \n",
       " 38     259  LC08_L1TP_233015_20161013_20170319_01_T1   592.5   302.996114  \n",
       " 39     259  LC08_L1TP_233015_20161013_20170319_01_T1   592.5          NaN  \n",
       " 40     259  LC08_L1TP_232015_20161107_20170318_01_T1  3397.5          NaN  \n",
       " 41     259  LC08_L1TP_232015_20161107_20170318_01_T1  3397.5  3161.995223  ,\n",
       "    BoxID_x   datetimes                Scene_x  tpos_x         Condition  \\\n",
       " 0      259  2013-08-18  LC82330152013230LGN00     NaN           Sea ice   \n",
       " 1      259  2013-09-26  LC80010152013269LGN00     NaN           Sea ice   \n",
       " 2      259  2013-09-28  LC82320152013271LGN00     NaN           Sea ice   \n",
       " 3      259  2013-10-05  LC82330152013278LGN00     NaN           Sea ice   \n",
       " 4      259  2013-10-21  LC82330152013294LGN00     NaN           Sea ice   \n",
       " 5      259  2014-05-10  LC82320152014130LGN00     NaN           Sea ice   \n",
       " 6      259  2014-05-26  LC82320152014146LGN00     NaN           Sea ice   \n",
       " 7      259  2014-07-11  LC80010152014192LGN00     NaN           Sea ice   \n",
       " 8      259  2014-08-12  LC80010152014224LGN00     NaN           Sea ice   \n",
       " 9      259  2014-09-06  LC82330152014249LGN00     NaN   Cloudy, sea ice   \n",
       " 10     259  2014-10-15  LC80010152014288LGN00     NaN           Sea ice   \n",
       " 11     259  2014-11-02  LC82320152014306LGN00     NaN  Shadows, sea ice   \n",
       " 12     259  2014-11-02  LC82320152014306LGN00     NaN  Shadows, sea ice   \n",
       " 13     259  2015-05-27  LC80010152015147LGN00     NaN           Sea ice   \n",
       " 14     259  2015-05-27  LC80010152015147LGN00     NaN           Sea ice   \n",
       " 15     259  2015-06-14  LC82320152015165LGN00     NaN           Sea ice   \n",
       " 16     259  2015-07-07  LC82330152015188LGN00     NaN           Sea ice   \n",
       " 17     259  2015-07-16  LC82320152015197LGN00     NaN           Sea ice   \n",
       " 18     259  2015-07-30  LC80010152015211LGN00     NaN           Sea ice   \n",
       " 19     259  2015-08-01  LC82320152015213LGN00     NaN           Sea ice   \n",
       " 20     259  2015-08-08  LC82330152015220LGN00     NaN           Sea ice   \n",
       " 21     259  2016-04-11  LC80010152016102LGN00     NaN           Sea ice   \n",
       " 22     259  2016-04-29  LC82320152016120LGN00     NaN           Sea ice   \n",
       " 23     259  2016-04-29  LC82320152016120LGN00     NaN           Sea ice   \n",
       " 24     259  2016-05-06  LC82330152016127LGN00     NaN           Sea ice   \n",
       " 25     259  2016-05-13  LC80010152016134LGN00     NaN           Sea ice   \n",
       " 26     259  2016-07-02  LC82320152016184LGN00     NaN           Sea ice   \n",
       " 27     259  2016-07-09  LC82330152016191LGN00     NaN           Sea ice   \n",
       " 28     259  2016-07-18  LC82320152016200LGN00     NaN           Sea ice   \n",
       " 29     259  2016-09-02  LC80010152016246LGN00     NaN           Sea ice   \n",
       " 30     259  2016-09-11  LC82330152016255LGN00     NaN           Sea ice   \n",
       " 31     259  2016-09-11  LC82330152016255LGN00     NaN           Sea ice   \n",
       " 32     259  2016-09-27  LC82330152016271LGN00     NaN           Sea ice   \n",
       " 33     259  2016-10-13  LC82330152016287LGN00     NaN   Cloudy, sea ice   \n",
       " 34     259  2016-10-13  LC82330152016287LGN00     NaN   Cloudy, sea ice   \n",
       " 35     259  2016-11-07  LC82320152016312LGN00     NaN   Shadow, sea ice   \n",
       " 36     259  2016-11-07  LC82320152016312LGN00     NaN   Shadow, sea ice   \n",
       " \n",
       "    BoxID_y                                   Scene_y   tpos_y  diff  \n",
       " 0      259  LC08_L1TP_233015_20130818_20180426_01_T1  1211.25   NaN  \n",
       " 1      259  LC08_L1TP_001015_20130926_20170501_01_T1  1976.25   NaN  \n",
       " 2      259  LC08_L1TP_232015_20130928_20180528_01_T1  1841.25   NaN  \n",
       " 3      259  LC08_L1TP_233015_20131005_20170429_01_T1   656.25   NaN  \n",
       " 4      259  LC08_L1TP_233015_20131021_20170429_01_T1   746.25   NaN  \n",
       " 5      259  LC08_L1TP_232015_20140510_20170422_01_T1    86.25   NaN  \n",
       " 6      259  LC08_L1TP_232015_20140526_20170422_01_T1   626.25   NaN  \n",
       " 7      259  LC08_L1TP_001015_20140711_20170421_01_T1   446.25   NaN  \n",
       " 8      259  LC08_L1TP_001015_20140812_20170420_01_T1   881.25   NaN  \n",
       " 9      259  LC08_L1TP_233015_20140906_20170419_01_T1  1271.25   NaN  \n",
       " 10     259  LC08_L1TP_001015_20141015_20170418_01_T1  1586.25   NaN  \n",
       " 11     259  LC08_L1TP_232015_20141102_20180528_01_T1   116.25   NaN  \n",
       " 12     259  LC08_L1TP_232015_20141102_20180528_01_T1   116.25   NaN  \n",
       " 13     259  LC08_L1TP_001015_20150527_20170408_01_T1   431.25   NaN  \n",
       " 14     259  LC08_L1TP_001015_20150527_20170408_01_T1   431.25   NaN  \n",
       " 15     259  LC08_L1TP_232015_20150614_20170407_01_T1   881.25   NaN  \n",
       " 16     259  LC08_L1TP_233015_20150707_20170407_01_T1  1961.25   NaN  \n",
       " 17     259  LC08_L1TP_232015_20150716_20170407_01_T1  3521.25   NaN  \n",
       " 18     259  LC08_L1TP_001015_20150730_20170406_01_T1  1091.25   NaN  \n",
       " 19     259  LC08_L1TP_232015_20150801_20170406_01_T1   656.25   NaN  \n",
       " 20     259  LC08_L1TP_233015_20150808_20180528_01_T1   656.25   NaN  \n",
       " 21     259  LC08_L1TP_001015_20160411_20170326_01_T1   521.25   NaN  \n",
       " 22     259  LC08_L1TP_232015_20160429_20170326_01_T1  1151.25   NaN  \n",
       " 23     259  LC08_L1TP_232015_20160429_20170326_01_T1  1151.25   NaN  \n",
       " 24     259  LC08_L1TP_233015_20160506_20170325_01_T1  2036.25   NaN  \n",
       " 25     259  LC08_L1TP_001015_20160513_20170324_01_T1   881.25   NaN  \n",
       " 26     259  LC08_L1TP_232015_20160702_20170323_01_T1   116.25   NaN  \n",
       " 27     259  LC08_L1TP_233015_20160709_20170323_01_T1   596.25   NaN  \n",
       " 28     259  LC08_L1TP_232015_20160718_20170323_01_T1   641.25   NaN  \n",
       " 29     259  LC08_L1TP_001015_20160902_20170321_01_T1   671.25   NaN  \n",
       " 30     259  LC08_L1TP_233015_20160911_20170321_01_T1   656.25   NaN  \n",
       " 31     259  LC08_L1TP_233015_20160911_20170321_01_T1   656.25   NaN  \n",
       " 32     259  LC08_L1TP_233015_20160927_20170320_01_T1   656.25   NaN  \n",
       " 33     259  LC08_L1TP_233015_20161013_20170319_01_T1   656.25   NaN  \n",
       " 34     259  LC08_L1TP_233015_20161013_20170319_01_T1   656.25   NaN  \n",
       " 35     259  LC08_L1TP_232015_20161107_20170318_01_T1  1976.25   NaN  \n",
       " 36     259  LC08_L1TP_232015_20161107_20170318_01_T1  1976.25   NaN  ,\n",
       "    BoxID_x   datetimes                Scene_x      tpos_x         Condition  \\\n",
       " 0      259  2013-08-25  LC80010152013237LGN00  929.410095           Sea ice   \n",
       " 1      259  2013-10-05  LC82330152013278LGN00  671.344267           Sea ice   \n",
       " 2      259  2013-10-21  LC82330152013294LGN00  596.356123           Sea ice   \n",
       " 3      259  2014-05-10  LC82320152014130LGN00  422.303949           Sea ice   \n",
       " 4      259  2014-06-09  LC80010152014160LGN00  488.278225           Sea ice   \n",
       " 5      259  2014-07-04  LC82330152014185LGN00  563.274467           Sea ice   \n",
       " 6      259  2014-08-21  LC82330152014233LGN00  218.467675           Sea ice   \n",
       " 7      259  2014-10-15  LC80010152014288LGN00  209.262096           Sea ice   \n",
       " 8      259  2014-11-02  LC82320152014306LGN00   90.156115  Shadows, sea ice   \n",
       " 9      259  2014-11-02  LC82320152014306LGN00  101.319421  Shadows, sea ice   \n",
       " 10     259  2015-05-27  LC80010152015147LGN00  164.744120           Sea ice   \n",
       " 11     259  2015-05-27  LC80010152015147LGN00  143.581423           Sea ice   \n",
       " 12     259  2015-06-14  LC82320152015165LGN00  188.323193           Sea ice   \n",
       " 13     259  2015-07-16  LC82320152015197LGN00  326.271551           Sea ice   \n",
       " 14     259  2015-08-01  LC82320152015213LGN00  164.744120           Sea ice   \n",
       " 15     259  2015-08-15  LC80010152015227LGN00   80.253505           Sea ice   \n",
       " 16     259  2015-09-02  LC82320152015245LGN00  122.436208           Sea ice   \n",
       " 17     259  2016-04-11  LC80010152016102LGN00  326.271551           Sea ice   \n",
       " 18     259  2016-04-29  LC82320152016120LGN00  392.336749           Sea ice   \n",
       " 19     259  2016-04-29  LC82320152016120LGN00  380.250740           Sea ice   \n",
       " 20     259  2016-05-06  LC82330152016127LGN00  422.303949           Sea ice   \n",
       " 21     259  2016-05-13  LC80010152016134LGN00  422.303949           Sea ice   \n",
       " 22     259  2016-07-16  LC80010152016198LGN00  692.299159           Sea ice   \n",
       " 23     259  2016-09-02  LC80010152016246LGN00  983.458502           Sea ice   \n",
       " 24     259  2016-09-11  LC82330152016255LGN00  284.258905           Sea ice   \n",
       " 25     259  2016-09-11  LC82330152016255LGN00  293.412040           Sea ice   \n",
       " 26     259  2016-10-20  LC80010152016294LGN00  111.369318   Shadow, sea ice   \n",
       " 27     259  2016-11-07  LC82320152016312LGN00   38.608613   Shadow, sea ice   \n",
       " 28     259  2016-11-07  LC82320152016312LGN00   19.121323   Shadow, sea ice   \n",
       " \n",
       "    BoxID_y                                   Scene_y   tpos_y         diff  \n",
       " 0      259  LC08_L1TP_001015_20130825_20170502_01_T1  2943.75  2014.339905  \n",
       " 1      259  LC08_L1TP_233015_20131005_20170429_01_T1  2538.75  1867.405733  \n",
       " 2      259  LC08_L1TP_233015_20131021_20170429_01_T1   573.75    22.606123  \n",
       " 3      259  LC08_L1TP_232015_20140510_20170422_01_T1   303.75   118.553949  \n",
       " 4      259  LC08_L1TP_001015_20140609_20170422_01_T1   453.75    34.528225  \n",
       " 5      259  LC08_L1TP_233015_20140704_20170421_01_T1   528.75    34.524467  \n",
       " 6      259  LC08_L1TP_233015_20140821_20170420_01_T1  1998.75  1780.282325  \n",
       " 7      259  LC08_L1TP_001015_20141015_20170418_01_T1  1323.75  1114.487904  \n",
       " 8      259  LC08_L1TP_232015_20141102_20180528_01_T1   228.75   138.593885  \n",
       " 9      259  LC08_L1TP_232015_20141102_20180528_01_T1   228.75   127.430579  \n",
       " 10     259  LC08_L1TP_001015_20150527_20170408_01_T1   498.75   334.005880  \n",
       " 11     259  LC08_L1TP_001015_20150527_20170408_01_T1   498.75   355.168577  \n",
       " 12     259  LC08_L1TP_232015_20150614_20170407_01_T1   228.75    40.426807  \n",
       " 13     259  LC08_L1TP_232015_20150716_20170407_01_T1  3528.75  3202.478449  \n",
       " 14     259  LC08_L1TP_232015_20150801_20170406_01_T1  1398.75  1234.005880  \n",
       " 15     259  LC08_L1TP_001015_20150815_20170406_01_T1   333.75   253.496495  \n",
       " 16     259  LC08_L1TP_232015_20150902_20170404_01_T1   138.75    16.313792  \n",
       " 17     259  LC08_L1TP_001015_20160411_20170326_01_T1  1743.75  1417.478449  \n",
       " 18     259  LC08_L1TP_232015_20160429_20170326_01_T1  1008.75   616.413251  \n",
       " 19     259  LC08_L1TP_232015_20160429_20170326_01_T1  1008.75   628.499260  \n",
       " 20     259  LC08_L1TP_233015_20160506_20170325_01_T1   333.75    88.553949  \n",
       " 21     259  LC08_L1TP_001015_20160513_20170324_01_T1   438.75    16.446051  \n",
       " 22     259  LC08_L1TP_001015_20160716_20180129_01_T1   843.75   151.450841  \n",
       " 23     259  LC08_L1TP_001015_20160902_20170321_01_T1   753.75   229.708502  \n",
       " 24     259  LC08_L1TP_233015_20160911_20170321_01_T1   228.75    55.508905  \n",
       " 25     259  LC08_L1TP_233015_20160911_20170321_01_T1   228.75    64.662040  \n",
       " 26     259  LC08_L1TP_001015_20161020_20170319_01_T1   573.75   462.380682  \n",
       " 27     259  LC08_L1TP_232015_20161107_20170318_01_T1  2898.75  2860.141387  \n",
       " 28     259  LC08_L1TP_232015_20161107_20170318_01_T1  2898.75  2879.628677  ]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box120\n",
      "Theta values: 108.03952155560408\n",
      "Box174\n",
      "Theta values: 253.09239737745898\n",
      "Box002\n",
      "Theta values: 344.2793399204759\n",
      "Box259\n",
      "Theta values: 354.46125446329745\n",
      "Box001\n",
      "Theta values: 244.018222814385\n"
     ]
    }
   ],
   "source": [
    "theta1s = []; theta2s = []; compare_dfs = []\n",
    "#FOR EACH GLACIER BOXID:\n",
    "BoxIDs = list(set(manual_df.BoxID))\n",
    "for BoxID in BoxIDs:\n",
    "    print(\"Box\"+BoxID)\n",
    "    #grab automated tpos\n",
    "    auto50 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline50_filtered.csv', dtype=str,sep=',')\n",
    "    auto25 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline25_filtered.csv', dtype=str,sep=',')\n",
    "    auto75 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline75_filtered.csv', dtype=str,sep=',')\n",
    "    autodfs = [auto50, auto25, auto75]\n",
    "    #grab manual tpos that corresponds to just boxID\n",
    "    manual50_df = manual50[manual50.BoxID == BoxID].copy()\n",
    "    manual25_df = manual25[manual25.BoxID == BoxID].copy()\n",
    "    manual75_df = manual75[manual75.BoxID == BoxID].copy()\n",
    "    manualdfs = [manual50_df, manual25_df, manual75_df]\n",
    "    #calculate difference in terminus positions along the three flowlines\n",
    "    lists3 = []; lists3_norm = []\n",
    "    for i in range(0, len(manualdfs)):\n",
    "        man = manualdfs[i]; auto = autodfs[i]; # sigma = sigmas[i]\n",
    "        compare_df = man.merge(auto, how='inner', on=['datetimes'])\n",
    "        #cast terminus positions into float values\n",
    "        compare_df = compare_df.astype({'tpos_x': 'float', 'tpos_y': 'float'})\n",
    "        #subtract the absolute value of the difference and put into df as a column named \"diff\"\n",
    "        compare_df['diff'] = abs(np.array(compare_df.tpos_x) - np.array(compare_df.tpos_y))\n",
    "#         compare_df['diff/sigma'] = abs(np.array(compare_df.tpos_x) - np.array(compare_df.tpos_y))/sigma\n",
    "        lists3.append(list(compare_df['diff']))  \n",
    "#         lists3_norm.append(list(compare_df['diff/sigma']))\n",
    "    diff_all = lists3[0]+lists3[1]+lists3[2] #list of all the differences between manual and auto\n",
    "#     normalizeddiff_all = lists3_norm[0]+lists3_norm[1]+lists3_norm[2] #list of all the normalized differences\n",
    "    \n",
    "    N = len(diff_all) #number of total intersections\n",
    "    \n",
    "    #CALCULATE THETA:\n",
    "#     theta1 = (1.0/N)*np.sum(normalizeddiff_all) #sum of normalized differences along flowlines\n",
    "    theta2 = (1.0/N)*(np.nansum(diff_all)) #sum of differences normalized by average sigma\n",
    "#     theta1s.append(theta1); \n",
    "    theta2s.append(theta2)\n",
    "    print(\"Theta values:\",theta2)\n",
    "    \n",
    "    compare_dfs.append(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE OVERALL THETA and write results to csv\n",
    "theta1_all = np.average(theta1s)\n",
    "theta2_all = np.average(theta2s)\n",
    "\n",
    "#organize data\n",
    "columns = ['Theta_avg']+BoxIDs\n",
    "theta1_for_df = [theta1_all]+theta1s\n",
    "theta2_for_df = [theta2_all]+theta2s\n",
    "#write to csv\n",
    "pd.DataFrame(list(zip(columns, theta1_for_df, theta2_for_df)), \n",
    "             columns=['ID', 'theta1', 'theta2']).to_csv(manual_path+'thetas.csv', sep=',') \n",
    "\n",
    "#ADJUST FILENAME TO INCLUDE PARAMETERS OR SOMETHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT INTO 3 DATAFRAMES FOR 3 FLOWLINES:\n",
    "manual50 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                      'tpos50']].copy().reset_index(drop=True).rename(columns={\"tpos50\": \"tpos\"})\n",
    "manual25 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                      'tpos25']].copy().reset_index(drop=True).rename(columns={\"tpos25\": \"tpos\"})\n",
    "manual75 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y',\n",
    "                                      'tpos75']].copy().reset_index(drop=True).rename(columns={\"tpos75\": \"tpos\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_theta(manual_df):\n",
    "    #SPLIT INTO 3 DATAFRAMES FOR 3 FLOWLINES:\n",
    "    manual50 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                          'tpos50']].copy().reset_index(drop=True).rename(columns={\"tpos50\": \"tpos\"})\n",
    "    manual25 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y', \n",
    "                                          'tpos25']].copy().reset_index(drop=True).rename(columns={\"tpos25\": \"tpos\"})\n",
    "    manual75 = manual_df[['BoxID','datetimes', 'intersect_x', 'intersect_y',\n",
    "                                          'tpos75']].copy().reset_index(drop=True).rename(columns={\"tpos75\": \"tpos\"})\n",
    "    thetas = []\n",
    "    #FOR EACH GLACIER BOXID:\n",
    "    BoxIDs = list(set(manual_df.BoxID))\n",
    "    for BoxID in BoxIDs:\n",
    "        print(\"Box\"+BoxID)\n",
    "        #grab automated tpos\n",
    "        auto50 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline50_filtered.csv', dtype=str,sep=',')\n",
    "        auto25 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline25_filtered.csv', dtype=str,sep=',')\n",
    "        auto75 = pd.read_csv(auto_path+'Tpos_Box'+BoxID+'_flowline75_filtered.csv', dtype=str,sep=',')\n",
    "        autodfs = [auto50, auto25, auto75]\n",
    "        #grab manual tpos that corresponds to just boxID\n",
    "        manual50_df = manual50[manual50.BoxID == BoxID].copy()\n",
    "        manual25_df = manual25[manual25.BoxID == BoxID].copy()\n",
    "        manual75_df = manual75[manual75.BoxID == BoxID].copy()\n",
    "        manualdfs = [manual50_df, manual25_df, manual75_df]\n",
    "        #calculate difference in terminus positions along the three flowlines\n",
    "        lists3 = []; lists3_norm = []\n",
    "        for i in range(0, len(manualdfs)):\n",
    "            man = manualdfs[i]; auto = autodfs[i]; # sigma = sigmas[i]\n",
    "            compare_df = man.merge(auto, how='inner', on=['datetimes'])\n",
    "            #cast terminus positions into float values\n",
    "            compare_df = compare_df.astype({'tpos_x': 'float', 'tpos_y': 'float'})\n",
    "            #subtract the absolute value of the difference and put into df as a column named \"diff\"\n",
    "            compare_df['diff'] = abs(np.array(compare_df.tpos_x) - np.array(compare_df.tpos_y))  \n",
    "            lists3.append(list(compare_df['diff']))  \n",
    "        diff_all = lists3[0]+lists3[1]+lists3[2] #list of all the differences between manual and auto\n",
    "    #     normalizeddiff_all = lists3_norm[0]+lists3_norm[1]+lists3_norm[2] #list of all the normalized differences\n",
    "\n",
    "        N = len(diff_all) #number of total intersections\n",
    "\n",
    "        #CALCULATE THETA:\n",
    "        theta = (1.0/N)*(np.nansum(diff_all)) #sum of differences normalized by average sigma\n",
    "        thetas.append(theta)\n",
    "        print(\"Theta values:\",theta)\n",
    "        \n",
    "                \n",
    "    #CALCULATE OVERALL THETA\n",
    "    theta_all = np.nanmean(thetas)\n",
    "    #organize data in dataframe\n",
    "    column_titles = ['Theta_avg']+BoxIDs\n",
    "    theta_for_df = [theta_all]+thetas\n",
    "    #write to csv\n",
    "    theta_df = pd.DataFrame(list(zip(column_titles, theta_for_df)), \n",
    "                 columns=['ID', 'theta'])\n",
    "    \n",
    "    return theta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box120\n",
      "Theta values: 108.03952155560408\n",
      "Box174\n",
      "Theta values: 253.09239737745898\n",
      "Box002\n",
      "Theta values: 344.2793399204759\n",
      "Box259\n",
      "Theta values: 354.46125446329745\n",
      "Box001\n",
      "Theta values: 244.018222814385\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>theta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Theta_avg</td>\n",
       "      <td>260.778147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>108.039522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>253.092397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002</td>\n",
       "      <td>344.279340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>354.461254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001</td>\n",
       "      <td>244.018223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID       theta\n",
       "0  Theta_avg  260.778147\n",
       "1        120  108.039522\n",
       "2        174  253.092397\n",
       "3        002  344.279340\n",
       "4        259  354.461254\n",
       "5        001  244.018223"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_theta(manual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
