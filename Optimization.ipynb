{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold optimization \n",
    "\n",
    "Jukes Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pylab as pl\n",
    "import numpy.ma as ma\n",
    "import datetime\n",
    "import math\n",
    "import scipy.optimize\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/media/jukes/jukes1/'\n",
    "sg_path = '/home/jukes/Documents/Sample_glaciers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in analysis dates for manual and automated delinations, convert to datetime objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #read in manual image dates\n",
    "# manual_df= pd.read_csv(basepath+'Manual/manual_tpos.csv', sep=',', dtype=str, header=0)\n",
    "# manual_df = manual_df.dropna()\n",
    "# manual_df.drop_duplicates(subset=['BoxID','datetimes'])\n",
    "# print(manual_df.shape)\n",
    "# manual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Read in datetags csv as datetime_df\n",
    "# automated_df = pd.read_csv(sg_path+'imgdates.csv', sep=',', dtype=str, header=0, names=['Scene', 'datetimes'])\n",
    "# print(automated_df.shape)\n",
    "# automated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find overlaps and select 90% for training, 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overlap_df = manual_df.merge(automated_df, how='inner', on=['datetimes'])\n",
    "# overlap_df = overlap_df.drop(['Line_x', 'Line_y'], axis=1)\n",
    "# overlap_df = overlap_df.drop_duplicates(['BoxID','datetimes'])\n",
    "# overlap_df = overlap_df.sort_values(by=['BoxID','datetimes'], ascending=True)\n",
    "# overlap_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = []\n",
    "# for idx, row in overlap_df.iterrows():\n",
    "#     dateID = str(row['BoxID'])+','+str(row['datetimes']+','+str(row['Scene']))\n",
    "#     dates.append(dateID)\n",
    "# # print(date_IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 90% for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N = len(dates); print(N)\n",
    "\n",
    "# #pick a random sample of dates for training\n",
    "# train_dates = random.sample(dates, int(N*0.9))\n",
    "# print(len(train_dates))\n",
    "# # print(train_dates)\n",
    "\n",
    "# #grab remaining for testing\n",
    "# test_dates = []\n",
    "# for date in dates:\n",
    "#     if date not in train_dates:\n",
    "#         test_dates.append(date)\n",
    "# print(len(test_dates))\n",
    "# # print(test_dates)\n",
    "\n",
    "# #Check that they don't overlap, should return empty\n",
    "# print(len(train_dates)+len(test_dates)); print(set(train_dates).intersection(test_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# boxes = []; imgdates = []; scenes = []\n",
    "\n",
    "# for td in train_dates:\n",
    "#     BoxID, imgdate, scene = td.split(',')\n",
    "#     boxes.append(BoxID); imgdates.append(imgdate); scenes.append(scene)\n",
    "\n",
    "# train_df = pd.DataFrame(list(zip(boxes, imgdates, scenes)), columns=['BoxID', 'datetime', 'Scene'])\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export to csv and text\n",
    "# train_df.to_csv(basepath+'/Manual/train.csv', sep=',', index=False, header=False)\n",
    "# train_df.to_csv(basepath+'/Manual/train.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab test dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes = []; imgdates = []; scenes = []\n",
    "\n",
    "# for td in test_dates:\n",
    "#     BoxID, imgdate, scene = td.split(',')\n",
    "#     boxes.append(BoxID); imgdates.append(imgdate); scenes.append(scene)\n",
    "\n",
    "# test_df = pd.DataFrame(list(zip(boxes, imgdates, scenes)), columns=['BoxID', 'datetime', 'Scene'])\n",
    "# # test_df.head()\n",
    "# #export to csv and text\n",
    "# test_df.to_csv(basepath+'/Manual/test.csv', sep=',', index=False, header=False)\n",
    "# test_df.to_csv(basepath+'/Manual/test.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective function\n",
    "\n",
    "I'm using a modified version of the L1-norm. Imported from the automated_terminus_functions.py script. The objective funciton will be 1/N * (|Xa-Xm|i) where i=3 (for each centerline 50, 25, 75) and N equals the number of delineations generated (the more the better). The goal then is to minimize the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOA = '2020_01_20'\n",
    "\n",
    "# def calc_theta(size_thresh, mod_thresh):\n",
    "#     #Calculate automated tpos\n",
    "#     #run terminus_pick.tcl using each of the thresholds\n",
    "#     terminus_pick = '/home/akhalil/src/xsmurf-2.7/main/xsmurf -nodisplay /home/jukes/Documents/Scripts/terminus_pick.tcl '+str(size_thresh)+' '+str(mod_thresh)\n",
    "#     print(terminus_pick)\n",
    "#     subprocess.call(terminus_pick, shell=True)\n",
    "    \n",
    "#     #pull automated terminus position from the output\n",
    "#     #grab each output file\n",
    "#     differences = []\n",
    "    \n",
    "#     for file in os.listdir(sg_path):\n",
    "#         if DOA in file and file.endswith('csv'):\n",
    "#             if len(file)>28:\n",
    "#                 print(file)\n",
    "\n",
    "#                 #read the output file in and calculate terminus position for each image\n",
    "#                 #pull automated terminus delineations\n",
    "#                 auto_tpos = \n",
    "\n",
    "#                 #pull in manual tpos \n",
    "#                 man_tpos = \n",
    "\n",
    "#                 diff = abs(auto_tpos - man_tpos)\n",
    "#                 differences.append(diff)\n",
    "    \n",
    "# #     #return objective function = distance between the two\n",
    "# #     return np.average(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimize(size_guess, mod_guess):\n",
    "#     minimum = scipy.optimize.fmin(center_dist, [size_guess, mod_guess], args=(size_guess, mod_guess),full_output=True)\n",
    "#     xopt = minimum[0][0]\n",
    "#     funcval = minimum[1]\n",
    "#     return xopt, funcval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the optimization\n",
    "\n",
    "    scipy.optimize.fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)[source]\n",
    "\n",
    "Minimize a function using the downhill simplex algorithm.\n",
    "This algorithm only uses function values, not derivatives or second derivatives.\n",
    "\n",
    "Parameters\n",
    "   - funccallable func(x,*args)\n",
    "The objective function to be minimized.\n",
    "\n",
    "   - x0ndarray\n",
    "Initial guess.\n",
    "\n",
    "Returns\n",
    "   - xoptndarray\n",
    "Parameter that minimizes function.\n",
    "\n",
    "   - foptfloat\n",
    "Value of function at minimum: fopt = func(xopt).\n",
    "\n",
    "   - iterint\n",
    "Number of iterations performed.\n",
    "\n",
    "   - funcallsint\n",
    "Number of function calls made.\n",
    "\n",
    "   - warnflagint\n",
    "1 : Maximum number of function evaluations made. 2 : Maximum number of iterations reached.\n",
    "\n",
    "    -allvecslist\n",
    "Solution at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import objective function\n",
    "os.chdir('/home/jukes/automated-glacier-terminus')\n",
    "from automated_terminus_functions import objective_func, calc_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.6 0.7 0.8]\n",
      "[0.5 0.6 0.7 0.8]\n",
      "[0.1 0.2 0.3 0.4]\n"
     ]
    }
   ],
   "source": [
    "#create the thresholds to input into the objective function\n",
    "base_size_thresh = 0.7; base_mod_thresh = 0.7; base_arg_thresh = 0.3;\n",
    "thresh_range = 0.2; step=0.1;\n",
    "size_guesses = np.arange(base_size_thresh-thresh_range, base_size_thresh+thresh_range, step)\n",
    "mod_guesses = np.arange(base_mod_thresh-thresh_range, base_mod_thresh+thresh_range, step)\n",
    "arg_guesses = np.arange(base_arg_thresh-thresh_range, base_arg_thresh+thresh_range, step)\n",
    "# display the threshold guesses\n",
    "print(size_guesses); print(mod_guesses); print(arg_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 iterations\n",
      "149.33333333333334 max hours to run\n"
     ]
    }
   ],
   "source": [
    "# create list of thresholding orders to test\n",
    "orders = ['_AMS', '_ASM', '_MAS', '_MSA', '_MSA', '_SAM', '_SMA']\n",
    "n_iter = len(orders)*len(size_guesses)*len(mod_guesses)*len(arg_guesses)\n",
    "print(n_iter, 'iterations'); print(n_iter*20/60, \"max hours to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\" \".join(['001', '002', '120', '174', '259']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS 0.5 0.5 0.1\n",
      "26.20983595742514\n",
      "Iteration run time: 212.5241198539734 seconds \n",
      "AMS 0.5 0.5 0.2\n",
      "26.387647557647163\n",
      "Iteration run time: 210.99532747268677 seconds \n",
      "AMS 0.5 0.5 0.3\n",
      "26.387647557647163\n",
      "Iteration run time: 190.0870406627655 seconds \n",
      "AMS 0.5 0.5 0.4\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd; import numpy as np\n",
    "import time\n",
    "\n",
    "# BoxIDs = \" \".join(['001', '002', '120', '174', '259']); print(IDs)\n",
    "start_time = time.time() #start recording time\n",
    "\n",
    "slist = []; mlist = []; alist = []; orderlist = []; thetalist = []; runtimes = []; #store run results\n",
    "\n",
    "for order in orders:\n",
    "    for s in size_guesses:\n",
    "        for m in mod_guesses:\n",
    "            for a in arg_guesses:\n",
    "                t0 = time.time() # start recording time for each run\n",
    "                # make sure precision and format is correct for the thresholds\n",
    "                s = float(\"{0:.3f}\".format(s)); m = float(\"{0:.3f}\".format(m)); a = float(\"{0:.3f}\".format(a));\n",
    "                print(order[1:4], s, m, a)\n",
    "                #append the thresholds and order to lists\n",
    "                slist.append(s); mlist.append(m); alist.append(a); orderlist.append(order[1:4]) \n",
    "                \n",
    "                #run objective function calculation\n",
    "                theta = objective_func(s, m, a, order); print(theta); thetalist.append(theta)\n",
    "\n",
    "                runtime = time.time() - t0; runtimes.append(runtime) # calculate run time and store\n",
    "                print(\"Iteration run time: %s seconds \" % runtime) #print run time for each iteration\n",
    "        \n",
    "print(\"Total time elapsed: --- %s seconds ---\" % (time.time() - start_time)) #Print total time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimization_df = pd.DataFrame(list(zip(thetalist, orderlist, slist, mlist, alist, runtimes)), \n",
    "                               columns=['Theta', 'Order', 'Size_thresh', 'Mod_thresh', 'Arg_thresh','run_time'])\n",
    "optimization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization_df[optimization_df['Theta'] == np.min(optimization_df['Theta'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization_df.to_csv(sg_path+'optimizationresults_4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize optimization results\n",
    "\n",
    "Results from calculating mistfits between automated and manual delineations based on different thresholds used for filtering the WTMM lines. The two thresholds shown here are the size (length) threshold and the mod (gradient value) threshold, which are percentages of the maximum size/mod. So ideally, for 1 parameter (one threshold), the results should look like a parabola. For two 2 parameters like used here, one can visualize it in 3D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as mtri\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "\n",
    "#activate interactive widgets for plots:\n",
    "%matplotlib notebook\n",
    "\n",
    "path = '/Users/julialiu/Documents/BSU/EGG/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(sg_path+'optimizationresults_3.csv', sep=',')\n",
    "# df2 = pd.read_csv(sg_path+'optimizationresults_2.csv', sep=',')\n",
    "# df3 = pd.read_csv(sg_path+'optimizationresults_1.csv', sep=',')\n",
    "# df4 = pd.read_csv(sg_path+'optimizationresults_4.csv', sep=',')\n",
    "# optimization_df = pd.concat([df1, df2, df3, df4])\n",
    "optimization_df = pd.read_csv(path+'optimizationresults_all4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization_df.to_csv(sg_path+'optimizationresults_all4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = optimization_df['Size_thresh']\n",
    "y = optimization_df['mod_thresh']\n",
    "z = optimization_df['Theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triang = mtri.Triangulation(x, y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.triplot(triang, c=\"#D3D3D3\", marker='.', markerfacecolor=\"r\", markeredgecolor=\"black\", markersize=10)\n",
    "ax.set_xlabel('Size threshold')\n",
    "ax.set_ylabel('Mod threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "ax.plot_trisurf(triang, z, cmap='hot') # plot the surface over the mesh\n",
    "ax.scatter(x,y,z, marker='.', s=10, c=\"k\", alpha=0.5) # plot the points\n",
    "ax.view_init(elev=20, azim=45) # set initial view angle\n",
    "\n",
    "ax.set_xlabel('Size threshold')\n",
    "ax.set_ylabel('Mod threshold')\n",
    "ax.set_zlabel('Misfit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
